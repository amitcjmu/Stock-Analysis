#!/usr/bin/env python3
"""
Cache Monitoring and Security System for AI Force Migration Platform
Generated by CC DevSecOps Engineer

This script provides comprehensive monitoring for Redis cache operations:
- Performance metrics with Prometheus integration
- Security violation detection and alerting
- Cache health monitoring
- Multi-tenant access validation
- Real-time alerting for anomalies
"""

import asyncio
import time
import logging
import json
import re
from typing import Dict, Any, List, Optional, Set
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from pathlib import Path
import sys

# Prometheus metrics (optional - install with: pip install prometheus-client)
try:
    from prometheus_client import Counter, Histogram, Gauge, Info, start_http_server
    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False
    print("‚ö†Ô∏è  Prometheus client not available. Install with: pip install prometheus-client")

# Redis client (optional - for direct monitoring)
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False
    print("‚ö†Ô∏è  Redis client not available. Install with: pip install redis")


@dataclass
class CacheMetrics:
    """Cache operation metrics"""
    timestamp: datetime
    operation: str
    cache_type: str
    key_pattern: str
    latency_ms: float
    success: bool
    tenant_id: Optional[str] = None
    user_id: Optional[str] = None
    data_size_bytes: int = 0
    ttl_seconds: Optional[int] = None


@dataclass
class SecurityViolation:
    """Security violation details"""
    timestamp: datetime
    violation_type: str
    severity: str
    description: str
    cache_key: str
    source_ip: Optional[str] = None
    user_id: Optional[str] = None
    tenant_id: Optional[str] = None


class PrometheusMetrics:
    """Prometheus metrics for cache monitoring"""

    def __init__(self):
        if not PROMETHEUS_AVAILABLE:
            return

        # Cache operation metrics
        self.cache_operations = Counter(
            'redis_cache_operations_total',
            'Total cache operations',
            ['operation', 'status', 'cache_type', 'tenant']
        )

        self.cache_latency = Histogram(
            'redis_cache_latency_seconds',
            'Cache operation latency in seconds',
            ['operation', 'cache_type'],
            buckets=[0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0]
        )

        self.cache_size = Gauge(
            'redis_cache_size_bytes',
            'Current cache size in bytes',
            ['cache_type', 'tenant']
        )

        self.cache_hit_rate = Gauge(
            'redis_cache_hit_rate',
            'Cache hit rate percentage',
            ['endpoint', 'cache_type', 'tenant']
        )

        # Security metrics
        self.security_violations = Counter(
            'redis_cache_security_violations_total',
            'Security violations detected',
            ['violation_type', 'severity', 'tenant']
        )

        self.failed_authentications = Counter(
            'redis_cache_auth_failures_total',
            'Failed authentication attempts',
            ['source', 'tenant']
        )

        # System health metrics
        self.redis_connections = Gauge(
            'redis_connections_active',
            'Active Redis connections'
        )

        self.redis_memory_usage = Gauge(
            'redis_memory_usage_bytes',
            'Redis memory usage in bytes'
        )

        self.cache_key_count = Gauge(
            'redis_cache_keys_total',
            'Total number of cache keys',
            ['pattern', 'tenant']
        )

    def record_operation(self, metrics: CacheMetrics):
        """Record cache operation metrics"""
        if not PROMETHEUS_AVAILABLE:
            return

        status = 'success' if metrics.success else 'failure'
        tenant = metrics.tenant_id or 'unknown'

        self.cache_operations.labels(
            operation=metrics.operation,
            status=status,
            cache_type=metrics.cache_type,
            tenant=tenant
        ).inc()

        self.cache_latency.labels(
            operation=metrics.operation,
            cache_type=metrics.cache_type
        ).observe(metrics.latency_ms / 1000.0)

    def record_security_violation(self, violation: SecurityViolation):
        """Record security violation"""
        if not PROMETHEUS_AVAILABLE:
            return

        tenant = violation.tenant_id or 'unknown'

        self.security_violations.labels(
            violation_type=violation.violation_type,
            severity=violation.severity,
            tenant=tenant
        ).inc()


class CacheSecurityMonitor:
    """Monitor cache operations for security violations"""

    def __init__(self):
        self.violations: List[SecurityViolation] = []
        self.metrics = PrometheusMetrics() if PROMETHEUS_AVAILABLE else None

        # Security validation rules
        self.key_validation_rules = {
            'version_prefix': r'^v\d+:',
            'tenant_isolation': r'tenant:|client:|account:',
            'no_dangerous_chars': r'^[a-zA-Z0-9:_\-\.]+$'
        }

        # Sensitive data patterns to watch for
        self.sensitive_patterns = {
            'credentials': ['password', 'token', 'secret', 'key', 'auth'],
            'pii': ['ssn', 'social_security', 'email', 'phone', 'address'],
            'financial': ['credit_card', 'bank_account', 'payment'],
            'healthcare': ['medical_record', 'patient_id', 'diagnosis']
        }

        # Rate limiting thresholds
        self.rate_limits = {
            'operations_per_minute': 1000,
            'failed_operations_per_minute': 50,
            'unique_keys_per_minute': 100
        }

        # Tracking for rate limiting
        self.operation_counts = {}
        self.last_reset = time.time()

    def validate_cache_operation(self, operation: str, cache_key: str,
                                data: Any = None, **kwargs) -> List[SecurityViolation]:
        """Validate cache operation for security issues"""
        violations = []
        timestamp = datetime.now()

        # Validate cache key format
        key_violations = self._validate_cache_key(cache_key, timestamp, **kwargs)
        violations.extend(key_violations)

        # Check for sensitive data
        if data is not None:
            data_violations = self._check_sensitive_data(cache_key, data, timestamp, **kwargs)
            violations.extend(data_violations)

        # Check rate limits
        rate_violations = self._check_rate_limits(operation, cache_key, timestamp, **kwargs)
        violations.extend(rate_violations)

        # Record violations
        for violation in violations:
            self.violations.append(violation)
            if self.metrics:
                self.metrics.record_security_violation(violation)

        return violations

    def _validate_cache_key(self, cache_key: str, timestamp: datetime, **kwargs) -> List[SecurityViolation]:
        """Validate cache key format and security"""
        violations = []
        tenant_id = kwargs.get('tenant_id')
        user_id = kwargs.get('user_id')

        # Check version prefix
        if not re.match(self.key_validation_rules['version_prefix'], cache_key):
            violations.append(SecurityViolation(
                timestamp=timestamp,
                violation_type='missing_version_prefix',
                severity='high',
                description=f'Cache key missing version prefix: {cache_key}',
                cache_key=cache_key,
                tenant_id=tenant_id,
                user_id=user_id
            ))

        # Check tenant isolation for user data
        if any(user_term in cache_key.lower() for user_term in ['user:', 'client:', 'tenant:']):
            if not re.search(self.key_validation_rules['tenant_isolation'], cache_key):
                violations.append(SecurityViolation(
                    timestamp=timestamp,
                    violation_type='missing_tenant_isolation',
                    severity='critical',
                    description=f'User data cache key missing tenant isolation: {cache_key}',
                    cache_key=cache_key,
                    tenant_id=tenant_id,
                    user_id=user_id
                ))

        # Check for dangerous characters
        if not re.match(self.key_validation_rules['no_dangerous_chars'], cache_key):
            violations.append(SecurityViolation(
                timestamp=timestamp,
                violation_type='dangerous_key_characters',
                severity='medium',
                description=f'Cache key contains dangerous characters: {cache_key}',
                cache_key=cache_key,
                tenant_id=tenant_id,
                user_id=user_id
            ))

        return violations

    def _check_sensitive_data(self, cache_key: str, data: Any,
                            timestamp: datetime, **kwargs) -> List[SecurityViolation]:
        """Check for unencrypted sensitive data"""
        violations = []
        tenant_id = kwargs.get('tenant_id')
        user_id = kwargs.get('user_id')

        # Convert data to string for analysis
        data_str = str(data).lower()

        # Check for sensitive patterns
        for category, patterns in self.sensitive_patterns.items():
            for pattern in patterns:
                if pattern in data_str:
                    # Check if data appears encrypted
                    if not self._appears_encrypted(data):
                        severity = 'critical' if category in ['credentials', 'financial'] else 'high'
                        violations.append(SecurityViolation(
                            timestamp=timestamp,
                            violation_type=f'unencrypted_{category}_data',
                            severity=severity,
                            description=f'Unencrypted {category} data detected in cache: {pattern}',
                            cache_key=cache_key,
                            tenant_id=tenant_id,
                            user_id=user_id
                        ))

        return violations

    def _check_rate_limits(self, operation: str, cache_key: str,
                          timestamp: datetime, **kwargs) -> List[SecurityViolation]:
        """Check for rate limit violations"""
        violations = []
        current_time = time.time()

        # Reset counters every minute
        if current_time - self.last_reset > 60:
            self.operation_counts = {}
            self.last_reset = current_time

        # Count operations
        minute_key = int(current_time // 60)
        if minute_key not in self.operation_counts:
            self.operation_counts[minute_key] = {'total': 0, 'failed': 0, 'unique_keys': set()}

        self.operation_counts[minute_key]['total'] += 1
        self.operation_counts[minute_key]['unique_keys'].add(cache_key)

        # Check thresholds
        counts = self.operation_counts[minute_key]
        tenant_id = kwargs.get('tenant_id')
        user_id = kwargs.get('user_id')

        if counts['total'] > self.rate_limits['operations_per_minute']:
            violations.append(SecurityViolation(
                timestamp=timestamp,
                violation_type='rate_limit_exceeded',
                severity='medium',
                description=f'Cache operations rate limit exceeded: {counts["total"]}/min',
                cache_key=cache_key,
                tenant_id=tenant_id,
                user_id=user_id
            ))

        if len(counts['unique_keys']) > self.rate_limits['unique_keys_per_minute']:
            violations.append(SecurityViolation(
                timestamp=timestamp,
                violation_type='unique_keys_rate_limit',
                severity='medium',
                description=f'Unique cache keys rate limit exceeded: {len(counts["unique_keys"])}/min',
                cache_key=cache_key,
                tenant_id=tenant_id,
                user_id=user_id
            ))

        return violations

    def _appears_encrypted(self, data: Any) -> bool:
        """Check if data appears to be encrypted"""
        if isinstance(data, dict):
            # Check for encryption indicators
            return any(key in data for key in ['encrypted', 'cipher', 'hash', 'encoded'])

        if isinstance(data, str):
            # Basic heuristics for encrypted data
            if len(data) > 20 and data.isalnum():
                # Long alphanumeric string might be encrypted
                return True
            if data.startswith(('enc:', 'cipher:', 'hash:')):
                return True

        return False


class CacheHealthMonitor:
    """Monitor Redis cache health and performance"""

    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_client = None
        self.metrics = PrometheusMetrics() if PROMETHEUS_AVAILABLE else None

        if REDIS_AVAILABLE:
            try:
                self.redis_client = redis.from_url(redis_url)
                self.redis_client.ping()  # Test connection
                print("‚úÖ Connected to Redis for health monitoring")
            except Exception as e:
                print(f"‚ö†Ô∏è  Could not connect to Redis: {e}")
                self.redis_client = None

    async def monitor_health(self) -> Dict[str, Any]:
        """Monitor Redis health metrics"""
        health_data = {
            'timestamp': datetime.now().isoformat(),
            'status': 'unknown',
            'metrics': {}
        }

        if not self.redis_client:
            health_data['status'] = 'disconnected'
            return health_data

        try:
            # Basic connectivity test
            self.redis_client.ping()
            health_data['status'] = 'healthy'

            # Get Redis info
            info = self.redis_client.info()
            health_data['metrics'] = {
                'connected_clients': info.get('connected_clients', 0),
                'used_memory': info.get('used_memory', 0),
                'used_memory_human': info.get('used_memory_human', '0B'),
                'keyspace_hits': info.get('keyspace_hits', 0),
                'keyspace_misses': info.get('keyspace_misses', 0),
                'expired_keys': info.get('expired_keys', 0),
                'evicted_keys': info.get('evicted_keys', 0),
                'total_commands_processed': info.get('total_commands_processed', 0)
            }

            # Calculate hit rate
            hits = health_data['metrics']['keyspace_hits']
            misses = health_data['metrics']['keyspace_misses']
            total = hits + misses
            hit_rate = (hits / total * 100) if total > 0 else 0
            health_data['metrics']['hit_rate_percent'] = round(hit_rate, 2)

            # Update Prometheus metrics
            if self.metrics:
                self.metrics.redis_connections.set(health_data['metrics']['connected_clients'])
                self.metrics.redis_memory_usage.set(health_data['metrics']['used_memory'])
                self.metrics.cache_hit_rate.labels(
                    endpoint='global',
                    cache_type='redis',
                    tenant='system'
                ).set(hit_rate)

        except Exception as e:
            health_data['status'] = 'error'
            health_data['error'] = str(e)

        return health_data

    async def monitor_keys_by_pattern(self) -> Dict[str, int]:
        """Monitor cache keys by pattern for tenant isolation validation"""
        if not self.redis_client:
            return {}

        patterns = {}
        try:
            # Get all keys (use SCAN in production to avoid blocking)
            keys = self.redis_client.keys('*')

            for key in keys:
                key_str = key.decode('utf-8') if isinstance(key, bytes) else str(key)

                # Extract pattern (version:context)
                parts = key_str.split(':')
                if len(parts) >= 2:
                    pattern = f"{parts[0]}:{parts[1]}"
                    patterns[pattern] = patterns.get(pattern, 0) + 1

        except Exception as e:
            print(f"Error monitoring keys: {e}")

        return patterns


class CacheMonitorOrchestrator:
    """Main orchestrator for cache monitoring"""

    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.security_monitor = CacheSecurityMonitor()
        self.health_monitor = CacheHealthMonitor(
            self.config.get('redis_url', 'redis://localhost:6379')
        )
        self.running = False

        # Setup logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('cache_monitor.log'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)

    async def start_monitoring(self):
        """Start the monitoring system"""
        self.running = True
        self.logger.info("üöÄ Starting Cache Monitoring System")

        # Start Prometheus metrics server if available
        if PROMETHEUS_AVAILABLE and self.config.get('prometheus_port'):
            try:
                start_http_server(self.config['prometheus_port'])
                self.logger.info(f"üìä Prometheus metrics server started on port {self.config['prometheus_port']}")
            except Exception as e:
                self.logger.error(f"Failed to start Prometheus server: {e}")

        # Start monitoring tasks
        tasks = [
            self._health_monitoring_loop(),
            self._security_monitoring_loop(),
            self._alert_processing_loop()
        ]

        await asyncio.gather(*tasks)

    async def _health_monitoring_loop(self):
        """Main health monitoring loop"""
        while self.running:
            try:
                health_data = await self.health_monitor.monitor_health()

                if health_data['status'] != 'healthy':
                    self.logger.warning(f"Cache health issue: {health_data}")

                # Monitor key patterns
                key_patterns = await self.health_monitor.monitor_keys_by_pattern()
                if key_patterns:
                    self.logger.info(f"Cache key patterns: {key_patterns}")

                await asyncio.sleep(30)  # Check every 30 seconds

            except Exception as e:
                self.logger.error(f"Health monitoring error: {e}")
                await asyncio.sleep(60)  # Wait longer on error

    async def _security_monitoring_loop(self):
        """Security monitoring loop"""
        while self.running:
            try:
                # Process recent violations
                recent_violations = [
                    v for v in self.security_monitor.violations
                    if v.timestamp > datetime.now() - timedelta(minutes=5)
                ]

                if recent_violations:
                    self.logger.warning(f"Recent security violations: {len(recent_violations)}")
                    for violation in recent_violations[-5:]:  # Log last 5
                        self.logger.warning(f"Security violation: {violation.violation_type} - {violation.description}")

                await asyncio.sleep(60)  # Check every minute

            except Exception as e:
                self.logger.error(f"Security monitoring error: {e}")
                await asyncio.sleep(60)

    async def _alert_processing_loop(self):
        """Process and send alerts for critical issues"""
        while self.running:
            try:
                # Check for critical violations
                critical_violations = [
                    v for v in self.security_monitor.violations
                    if v.severity == 'critical' and v.timestamp > datetime.now() - timedelta(minutes=1)
                ]

                for violation in critical_violations:
                    await self._send_alert(violation)

                await asyncio.sleep(30)  # Check every 30 seconds

            except Exception as e:
                self.logger.error(f"Alert processing error: {e}")
                await asyncio.sleep(60)

    async def _send_alert(self, violation: SecurityViolation):
        """Send alert for critical security violation"""
        alert_message = f"""
üö® CRITICAL CACHE SECURITY VIOLATION

Type: {violation.violation_type}
Severity: {violation.severity}
Description: {violation.description}
Cache Key: {violation.cache_key}
Tenant: {violation.tenant_id or 'Unknown'}
User: {violation.user_id or 'Unknown'}
Timestamp: {violation.timestamp}

Action Required: Investigate and remediate immediately.
        """

        self.logger.critical(alert_message)

        # Here you would integrate with your alerting system
        # (Slack, PagerDuty, email, etc.)

    def stop_monitoring(self):
        """Stop the monitoring system"""
        self.running = False
        self.logger.info("üõë Stopping Cache Monitoring System")


# Example usage and testing
async def main():
    """Main function for testing the monitoring system"""
    config = {
        'redis_url': 'redis://localhost:6379',
        'prometheus_port': 9090
    }

    monitor = CacheMonitorOrchestrator(config)

    # Test security monitoring
    violations = monitor.security_monitor.validate_cache_operation(
        operation='set',
        cache_key='user_data:password',  # Should trigger violations
        data={'password': 'secret123'},
        tenant_id='test_tenant',
        user_id='test_user'
    )

    print(f"Found {len(violations)} security violations:")
    for violation in violations:
        print(f"  - {violation.violation_type}: {violation.description}")

    # Start monitoring (would run indefinitely in production)
    # await monitor.start_monitoring()


if __name__ == "__main__":
    print("üîç Cache Security Monitor - AI Force Migration Platform")
    print("Generated by CC DevSecOps Engineer")
    print()

    # Run the test
    asyncio.run(main())
