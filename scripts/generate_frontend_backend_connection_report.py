import csv
import os
from collections import defaultdict

# Manually compiled from the grep search.
# In a real-world scenario, this could be auto-generated by a script
# that parses the FastAPI app, but this is a good starting point.
BACKEND_ROUTES = {
    'admin/session_comparison.py': [
        "/health", "/engagement/{engagement_id}/sessions", "/compare",
        "/history", "/comparisons/{comparison_id}/details"
    ],
    'admin/llm_usage.py': [
        "/usage/report", "/usage/summary/daily", "/usage/costs/breakdown",
        "/pricing/models", "/usage/real-time", "/pricing/initialize", "/health"
    ],
    'discovery/app_server_mappings.py': [
        "/app-server-mappings", "/app-server-mappings/{app_id}/add-server"
    ],
    'discovery/feedback_fallback.py': [
        "/feedback/fallback", "/feedback/fallback/status"
    ],
    'discovery/discovery_flow.py': [
        "/run", "/health", "/status/{flow_id}", "/active", "/metrics",
        "/validate-data", "/cleanup", "/config", "/capabilities"
    ],
    'discovery/database_test.py': [
        "/database/test", "/database/health", "/test-chat-simple"
    ],
    'discovery/cmdb_analysis.py': [
        "/analyze-cmdb", "/process-cmdb", "/cmdb-feedback", "/cmdb-templates"
    ],
    'discovery/chat_interface.py': [
        "/chat-test", "/ws/{client_id}"
    ],
    'discovery/testing_endpoints.py': [
        "/test-field-mapping", "/test-asset-classification", "/test-json-parsing",
        "/ai-parsing-analytics"
    ],
    'discovery/asset_management_modular.py': [
        "/health_check", "/", "/bulk", "/{asset_id}", "/reprocess",
        "/applications", "/unlinked", "/discovery-metrics",
        "/application-landscape", "/infrastructure-landscape", "/duplicates",
        "/cleanup-duplicates", "/data-issues", "/validate-data",
        "/data-issues/{issue_id}/approve", "/data-issues/{issue_id}/reject",
        "/legacy/summary"
    ],
    'auth/rbac.py': ["/system/info"],
    'endpoints/websocket.py': ["/stats", "/broadcast", "/health", "/sixr/{analysis_id}"],
    'auth/rbac_original.py': [
        "/login", "/change-password", "/register", "/registration-status/{user_id}",
        "/pending-approvals", "/approve-user", "/reject-user", "/validate-access",
        "/grant-client-access", "/user-profile/{user_id}", "/admin/dashboard-stats",
        "/active-users", "/admin/access-logs", "/health", "/demo/create-admin-user",
        "/admin/create-user", "/deactivate-user", "/activate-user"
    ]
}

def analyze_frontend_connections(frontend_path, output_csv_file):
    """
    Analyzes frontend source code to count connections to backend endpoints.

    Args:
        frontend_path (str): Path to the frontend 'src' directory.
        output_csv_file (str): Path to the output CSV file.
    """
    endpoint_counts = defaultdict(lambda: {'module': 'unknown', 'count': 0})

    # Initialize counts
    for module, endpoints in BACKEND_ROUTES.items():
        for endpoint in endpoints:
            endpoint_counts[endpoint]['module'] = module
            endpoint_counts[endpoint]['count'] = 0

    # Walk through the frontend source code
    for root, _, files in os.walk(frontend_path):
        for file in files:
            if file.endswith(('.ts', '.tsx', '.js', '.jsx')):
                filepath = os.path.join(root, file)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                        for endpoint in endpoint_counts.keys():
                            # This is a simple string search. A more advanced
                            # approach would use an AST parser for the frontend code.
                            count = content.count(endpoint)
                            if count > 0:
                                endpoint_counts[endpoint]['count'] += count
                except Exception as e:
                    print(f"Could not read file {filepath}: {e}")

    # Prepare data for CSV
    report_data = []
    for endpoint, data in endpoint_counts.items():
        report_data.append({
            'module': data['module'],
            'endpoint': endpoint,
            'frontend_connections': data['count']
        })

    # Sort by module, then by endpoint
    report_data.sort(key=lambda x: (x['module'], x['endpoint']))

    # Write to CSV
    try:
        with open(output_csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=['module', 'endpoint', 'frontend_connections'])
            writer.writeheader()
            writer.writerows(report_data)
        print(f"Successfully generated frontend connection report: {output_csv_file}")
    except IOError:
        print(f"Error: Could not write to file {output_csv_file}")

if __name__ == "__main__":
    analyze_frontend_connections('src', 'frontend_backend_connections.csv')
