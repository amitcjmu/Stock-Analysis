"""
Assessment Flow Integration Tests

This module provides comprehensive integration tests for Assessment Flow fixes:
- Assessment flow initialization and loading issues
- Backend integration components  
- Database schema compatibility
- Application loading and filtering
- Flow transition scenarios
- Multi-tenant isolation and security

Generated by CC for ADCS Backend Testing Framework
"""

import asyncio
import uuid
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional
from unittest.mock import AsyncMock, Mock, patch

import pytest
from fastapi.testclient import TestClient
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.context import RequestContext


class TestAssessmentFlowInitialization:
    """Test assessment flow initialization and loading fixes"""

    @pytest.fixture
    def client(self):
        """Create test client"""
        from main import app
        return TestClient(app)

    @pytest.fixture
    def mock_context(self):
        """Create mock request context"""
        return RequestContext(
            client_account_id=uuid.uuid4(),
            engagement_id=uuid.uuid4(),
            user_id=str(uuid.uuid4()),
        )

    @pytest.fixture
    def auth_headers(self, mock_context):
        """Create authentication headers"""
        return {
            "Authorization": "Bearer test_token",
            "Content-Type": "application/json",
            "X-Client-Account-ID": str(mock_context.client_account_id),
            "X-Engagement-ID": str(mock_context.engagement_id),
            "X-User-ID": mock_context.user_id,
        }

    @pytest.fixture
    def sample_assessment_data(self):
        """Create sample assessment data"""
        return {
            "assessment_id": str(uuid.uuid4()),
            "engagement_id": str(uuid.uuid4()),
            "client_account_id": str(uuid.uuid4()),
            "selected_applications": [
                {
                    "application_id": str(uuid.uuid4()),
                    "application_name": "Customer Portal",
                    "business_criticality": "High",
                    "technical_complexity": "Medium",
                    "environment": "Production"
                },
                {
                    "application_id": str(uuid.uuid4()),
                    "application_name": "Payment System", 
                    "business_criticality": "Critical",
                    "technical_complexity": "High",
                    "environment": "Production"
                }
            ],
            "assessment_scope": {
                "include_architecture_review": True,
                "include_dependency_analysis": True,
                "include_risk_assessment": True,
                "include_migration_strategy": True
            },
            "configuration": {
                "assessment_depth": "comprehensive",
                "timeline_weeks": 8,
                "resource_allocation": "standard"
            }
        }

    @pytest.mark.asyncio
    async def test_assessment_flow_initialization_success(self, client, auth_headers, sample_assessment_data):
        """Test successful assessment flow initialization"""
        with patch('app.core.auth.get_current_context') as mock_get_context:
            mock_get_context.return_value = RequestContext(
                client_account_id=uuid.UUID(sample_assessment_data["client_account_id"]),
                engagement_id=uuid.UUID(sample_assessment_data["engagement_id"]),
                user_id=str(uuid.uuid4()),
            )
            
            response = client.post(
                "/api/v1/assessment/flow/initialize",
                headers=auth_headers,
                json=sample_assessment_data
            )
            
            # Should handle initialization gracefully
            assert response.status_code in [200, 201, 404, 405]
            
            if response.status_code in [200, 201]:
                init_data = response.json()
                assert "assessment_id" in init_data or "flow_id" in init_data
                assert "status" in init_data

    @pytest.mark.asyncio
    async def test_assessment_flow_initialization_with_empty_applications(self, client, auth_headers):
        """Test assessment flow initialization with empty application list"""
        empty_assessment_data = {
            "assessment_id": str(uuid.uuid4()),
            "engagement_id": str(uuid.uuid4()),
            "client_account_id": str(uuid.uuid4()),
            "selected_applications": [],  # Empty applications list
            "assessment_scope": {
                "include_architecture_review": True
            }
        }
        
        with patch('app.core.auth.get_current_context') as mock_get_context:
            mock_get_context.return_value = RequestContext(
                client_account_id=uuid.UUID(empty_assessment_data["client_account_id"]),
                engagement_id=uuid.UUID(empty_assessment_data["engagement_id"]),
                user_id=str(uuid.uuid4()),
            )
            
            response = client.post(
                "/api/v1/assessment/flow/initialize",
                headers=auth_headers,
                json=empty_assessment_data
            )
            
            # Should handle empty applications gracefully
            assert response.status_code in [400, 404, 405, 422]
            
            if response.status_code == 400:
                error_data = response.json()
                assert "application" in str(error_data).lower() or "empty" in str(error_data).lower()

    @pytest.mark.asyncio
    async def test_assessment_flow_initialization_with_invalid_data(self, client, auth_headers):
        """Test assessment flow initialization with invalid data"""
        invalid_assessment_data = {
            "assessment_id": "invalid-uuid",  # Invalid UUID format
            "engagement_id": str(uuid.uuid4()),
            "selected_applications": "not-a-list",  # Invalid data type
        }
        
        with patch('app.core.auth.get_current_context') as mock_get_context:
            mock_get_context.return_value = RequestContext(
                client_account_id=uuid.uuid4(),
                engagement_id=uuid.uuid4(),
                user_id=str(uuid.uuid4()),
            )
            
            response = client.post(
                "/api/v1/assessment/flow/initialize",
                headers=auth_headers,
                json=invalid_assessment_data
            )
            
            # Should handle invalid data gracefully
            assert response.status_code in [400, 404, 405, 422]


class TestAssessmentFlowBackendIntegration:
    """Test assessment flow backend integration components"""

    @pytest.fixture
    def mock_db_session(self):
        """Create mock database session"""
        session = AsyncMock(spec=AsyncSession)
        return session

    @pytest.fixture
    def mock_context(self):
        """Create mock request context"""
        return RequestContext(
            client_account_id=uuid.uuid4(),
            engagement_id=uuid.uuid4(),
            user_id=str(uuid.uuid4()),
        )

    @pytest.mark.asyncio
    async def test_assessment_flow_service_integration(self, mock_db_session, mock_context):
        """Test assessment flow service integration"""
        try:
            from app.services.crewai_flows.unified_assessment_flow import UnifiedAssessmentFlow
            
            # Test service initialization
            mock_crewai_service = Mock()
            assessment_flow = UnifiedAssessmentFlow(mock_crewai_service, mock_context)
            
            # Verify service is properly initialized
            assert assessment_flow is not None
            assert assessment_flow.context == mock_context
            assert assessment_flow.crewai_service == mock_crewai_service
            
        except ImportError:
            # Service may not exist yet, which is expected
            pytest.skip("UnifiedAssessmentFlow not yet implemented")

    @pytest.mark.asyncio
    async def test_assessment_repository_integration(self, mock_db_session, mock_context):
        """Test assessment repository integration"""
        try:
            from app.repositories.assessment_flow_repository import AssessmentFlowRepository
            
            # Test repository initialization
            repository = AssessmentFlowRepository(mock_db_session)
            
            # Verify repository is properly initialized
            assert repository is not None
            assert repository.db == mock_db_session
            
        except ImportError:
            # Repository may not exist yet, which is expected
            pytest.skip("AssessmentFlowRepository not yet implemented")

    @pytest.mark.asyncio
    async def test_assessment_model_integration(self):
        """Test assessment model integration"""
        try:
            from app.models.assessment_flow import AssessmentFlow, AssessmentPhase
            
            # Test model can be instantiated
            assessment = AssessmentFlow(
                id=uuid.uuid4(),
                assessment_id=uuid.uuid4(),
                client_account_id=uuid.uuid4(),
                engagement_id=uuid.uuid4(),
                status="initialized",
                current_phase=AssessmentPhase.ARCHITECTURE_MINIMUMS
            )
            
            # Verify model is properly structured
            assert assessment is not None
            assert assessment.assessment_id is not None
            assert assessment.status == "initialized"
            
        except ImportError:
            # Models may not exist yet, which is expected
            pytest.skip("Assessment models not yet implemented")

    @pytest.mark.asyncio
    async def test_crewai_assessment_crew_integration(self):
        """Test CrewAI assessment crew integration"""
        try:
            from app.services.crews.assessment_crews import (
                ArchitectureAnalysisCrew,
                TechnicalDebtCrew,
                SixRStrategyCrew
            )
            
            # Test crews can be instantiated
            arch_crew = ArchitectureAnalysisCrew()
            debt_crew = TechnicalDebtCrew()
            sixr_crew = SixRStrategyCrew()
            
            # Verify crews are properly structured
            assert arch_crew is not None
            assert debt_crew is not None
            assert sixr_crew is not None
            
        except ImportError:
            # Crews may not exist yet, which is expected
            pytest.skip("Assessment crews not yet implemented")


class TestAssessmentFlowDatabaseCompatibility:
    """Test assessment flow database schema compatibility"""

    @pytest.fixture
    async def test_session(self):
        """Create test database session"""
        from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
        from sqlalchemy.orm import sessionmaker
        from app.models.base import Base
        
        engine = create_async_engine(
            "sqlite+aiosqlite:///:memory:",
            echo=False,
            future=True
        )
        
        async with engine.begin() as conn:
            await conn.run_sync(Base.metadata.create_all)
        
        session_factory = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)
        
        async with session_factory() as session:
            yield session
            await session.rollback()
        
        await engine.dispose()

    @pytest.fixture
    def mock_context(self):
        """Create mock request context"""
        return RequestContext(
            client_account_id=uuid.uuid4(),
            engagement_id=uuid.uuid4(),
            user_id=str(uuid.uuid4()),
        )

    @pytest.mark.asyncio
    async def test_assessment_flow_table_creation(self, test_session):
        """Test assessment flow table can be created"""
        try:
            # Try to query the assessment flow table
            from sqlalchemy import select, text
            from app.models.assessment_flow import AssessmentFlow
            
            stmt = select(AssessmentFlow).limit(1)
            result = await test_session.execute(stmt)
            
            # Should not raise an exception
            assert result is not None
            
        except ImportError:
            # Table may not exist yet, which is expected
            pytest.skip("Assessment flow table not yet implemented")
        except Exception as e:
            # Other database errors indicate schema issues
            if "no such table" in str(e).lower():
                pytest.skip("Assessment flow table not yet created")
            else:
                raise

    @pytest.mark.asyncio
    async def test_assessment_application_relationship(self, test_session, mock_context):
        """Test assessment to application relationship"""
        try:
            from app.models.assessment_flow import AssessmentFlow
            from app.models.asset import Asset
            
            # Create test application
            application = Asset(
                id=uuid.uuid4(),
                engagement_id=mock_context.engagement_id,
                name="Test Application",
                type="application",
                status="active"
            )
            test_session.add(application)
            
            # Create assessment referencing the application
            assessment = AssessmentFlow(
                id=uuid.uuid4(),
                assessment_id=uuid.uuid4(),
                client_account_id=mock_context.client_account_id,
                engagement_id=mock_context.engagement_id,
                status="initialized"
            )
            test_session.add(assessment)
            
            await test_session.commit()
            
            # Verify relationship works
            assert assessment.engagement_id == application.engagement_id
            
        except ImportError:
            pytest.skip("Assessment models not yet implemented")

    @pytest.mark.asyncio
    async def test_assessment_state_persistence(self, test_session, mock_context):
        """Test assessment state can be persisted"""
        try:
            from app.models.assessment_flow import AssessmentFlow
            
            # Create assessment with state data
            state_data = {
                "current_phase": "architecture_review",
                "progress": 25.0,
                "completed_phases": ["initialization"],
                "metadata": {
                    "architecture_standards": [
                        {"standard": "java_versions", "required": "11+"}
                    ]
                }
            }
            
            assessment = AssessmentFlow(
                id=uuid.uuid4(),
                assessment_id=uuid.uuid4(),
                client_account_id=mock_context.client_account_id,
                engagement_id=mock_context.engagement_id,
                status="in_progress",
                state_data=state_data
            )
            
            test_session.add(assessment)
            await test_session.commit()
            await test_session.refresh(assessment)
            
            # Verify state was persisted
            assert assessment.state_data == state_data
            assert assessment.state_data["progress"] == 25.0
            
        except ImportError:
            pytest.skip("Assessment models not yet implemented")


class TestAssessmentFlowApplicationLoading:
    """Test assessment flow application loading and filtering"""

    @pytest.fixture
    def mock_db_session(self):
        """Create mock database session"""
        session = AsyncMock(spec=AsyncSession)
        return session

    @pytest.fixture
    def mock_context(self):
        """Create mock request context"""
        return RequestContext(
            client_account_id=uuid.uuid4(),
            engagement_id=uuid.uuid4(),
            user_id=str(uuid.uuid4()),
        )

    @pytest.fixture
    def sample_applications(self):
        """Create sample application data"""
        return [
            {
                "id": uuid.uuid4(),
                "name": "Customer Portal",
                "business_criticality": "High",
                "technical_complexity": "Medium",
                "migration_readiness": "Ready"
            },
            {
                "id": uuid.uuid4(),
                "name": "Legacy Mainframe App",
                "business_criticality": "Critical", 
                "technical_complexity": "Very High",
                "migration_readiness": "Complex"
            },
            {
                "id": uuid.uuid4(),
                "name": "Analytics Dashboard",
                "business_criticality": "Medium",
                "technical_complexity": "Low", 
                "migration_readiness": "Ready"
            }
        ]

    @pytest.mark.asyncio
    async def test_application_selection_for_assessment(self, mock_db_session, mock_context, sample_applications):
        """Test application selection for assessment"""
        try:
            from app.services.crewai_flows.unified_assessment_flow import UnifiedAssessmentFlow
            
            mock_crewai_service = Mock()
            assessment_flow = UnifiedAssessmentFlow(mock_crewai_service, mock_context)
            
            # Mock application selection
            with patch.object(assessment_flow, '_select_applications_for_assessment') as mock_select:
                mock_select.return_value = sample_applications[:2]  # Select first 2 apps
                
                selected = await assessment_flow._select_applications_for_assessment()
                
                assert len(selected) == 2
                assert selected[0]["name"] == "Customer Portal"
                assert selected[1]["name"] == "Legacy Mainframe App"
                
        except (ImportError, AttributeError):
            pytest.skip("Assessment flow service not yet implemented")

    @pytest.mark.asyncio
    async def test_application_filtering_by_criteria(self, sample_applications):
        """Test application filtering by various criteria"""
        try:
            from app.services.assessment.application_selector import ApplicationSelector
            
            selector = ApplicationSelector()
            
            # Test filtering by business criticality
            critical_apps = selector.filter_by_criticality(
                sample_applications, 
                min_criticality="High"
            )
            
            assert len(critical_apps) >= 1
            
            # Test filtering by migration readiness
            ready_apps = selector.filter_by_readiness(
                sample_applications,
                readiness_level="Ready"
            )
            
            assert len(ready_apps) >= 1
            
        except ImportError:
            pytest.skip("Application selector not yet implemented")

    @pytest.mark.asyncio
    async def test_application_metadata_loading(self, mock_db_session, mock_context, sample_applications):
        """Test loading application metadata for assessment"""
        try:
            from app.services.crewai_flows.unified_assessment_flow import UnifiedAssessmentFlow
            
            mock_crewai_service = Mock()
            assessment_flow = UnifiedAssessmentFlow(mock_crewai_service, mock_context)
            
            # Mock metadata loading
            with patch.object(assessment_flow, '_get_application_metadata') as mock_metadata:
                mock_metadata.return_value = {
                    "technologies": ["Java", "Spring Boot", "PostgreSQL"],
                    "dependencies": ["Redis", "Elasticsearch"],
                    "deployment_model": "containerized"
                }
                
                metadata = await assessment_flow._get_application_metadata(sample_applications[0]["id"])
                
                assert "technologies" in metadata
                assert "dependencies" in metadata
                assert "deployment_model" in metadata
                
        except (ImportError, AttributeError):
            pytest.skip("Assessment flow service not yet implemented")


class TestAssessmentFlowPhaseTransitions:
    """Test assessment flow phase transitions"""

    @pytest.fixture
    def mock_context(self):
        """Create mock request context"""
        return RequestContext(
            client_account_id=uuid.uuid4(),
            engagement_id=uuid.uuid4(),
            user_id=str(uuid.uuid4()),
        )

    @pytest.fixture
    def sample_flow_state(self):
        """Create sample flow state"""
        return {
            "assessment_id": str(uuid.uuid4()),
            "status": "initialized",
            "current_phase": "architecture_minimums",
            "progress": 10,
            "selected_application_ids": [str(uuid.uuid4()), str(uuid.uuid4())],
            "engagement_architecture_standards": []
        }

    @pytest.mark.asyncio
    async def test_phase_transition_validation(self, mock_context, sample_flow_state):
        """Test phase transition validation"""
        try:
            from app.services.crewai_flows.unified_assessment_flow import UnifiedAssessmentFlow
            from app.models.assessment_flow import AssessmentFlowState, AssessmentPhase
            
            mock_crewai_service = Mock()
            assessment_flow = UnifiedAssessmentFlow(mock_crewai_service, mock_context)
            
            flow_state = AssessmentFlowState(**sample_flow_state)
            
            # Test invalid phase transition
            flow_state.current_phase = AssessmentPhase.ARCHITECTURE_MINIMUMS
            
            with pytest.raises(ValueError, match="Invalid phase transition|Prerequisites not met"):
                # Try to jump to 6R strategies without completing tech debt analysis
                await assessment_flow.determine_component_sixr_strategies(flow_state)
                
        except ImportError:
            pytest.skip("Assessment flow components not yet implemented")

    @pytest.mark.asyncio
    async def test_phase_completion_tracking(self, mock_context, sample_flow_state):
        """Test phase completion tracking"""
        try:
            from app.services.crewai_flows.unified_assessment_flow import UnifiedAssessmentFlow
            from app.models.assessment_flow import AssessmentFlowState
            
            mock_crewai_service = Mock()
            assessment_flow = UnifiedAssessmentFlow(mock_crewai_service, mock_context)
            
            flow_state = AssessmentFlowState(**sample_flow_state)
            
            # Mock repository
            assessment_flow.repository = Mock()
            assessment_flow.repository.save_flow_state = AsyncMock()
            
            # Test phase completion
            with patch.object(assessment_flow, '_load_engagement_standards') as mock_load:
                mock_load.return_value = []
                
                with patch.object(assessment_flow, '_initialize_default_standards') as mock_init:
                    mock_init.return_value = []
                    
                    result = await assessment_flow.capture_architecture_minimums(flow_state)
                    
                    assert result.current_phase == "architecture_minimums"
                    assert result.progress > sample_flow_state["progress"]
                    
        except ImportError:
            pytest.skip("Assessment flow components not yet implemented")


class TestAssessmentFlowErrorHandling:
    """Test assessment flow error handling"""

    @pytest.fixture
    def mock_context(self):
        """Create mock request context"""
        return RequestContext(
            client_account_id=uuid.uuid4(),
            engagement_id=uuid.uuid4(),
            user_id=str(uuid.uuid4()),
        )

    @pytest.mark.asyncio
    async def test_crew_execution_error_handling(self, mock_context):
        """Test error handling when CrewAI execution fails"""
        try:
            from app.services.crewai_flows.unified_assessment_flow import UnifiedAssessmentFlow
            from app.models.assessment_flow import AssessmentFlowState
            
            mock_crewai_service = Mock()
            mock_crewai_service.run_crew.side_effect = Exception("Crew execution failed")
            
            assessment_flow = UnifiedAssessmentFlow(mock_crewai_service, mock_context)
            
            flow_state = AssessmentFlowState(
                assessment_id=str(uuid.uuid4()),
                status="initialized",
                current_phase="tech_debt_analysis",
                progress=25,
                selected_application_ids=[str(uuid.uuid4())]
            )
            
            with pytest.raises(Exception, match="Crew execution failed"):
                await assessment_flow.analyze_technical_debt(flow_state)
                
        except ImportError:
            pytest.skip("Assessment flow components not yet implemented")

    @pytest.mark.asyncio
    async def test_timeout_error_handling(self, mock_context):
        """Test handling of crew execution timeouts"""
        try:
            from app.services.crewai_flows.unified_assessment_flow import UnifiedAssessmentFlow
            from app.models.assessment_flow import AssessmentFlowState
            
            mock_crewai_service = Mock()
            mock_crewai_service.run_crew.side_effect = asyncio.TimeoutError("Crew execution timed out")
            
            assessment_flow = UnifiedAssessmentFlow(mock_crewai_service, mock_context)
            
            flow_state = AssessmentFlowState(
                assessment_id=str(uuid.uuid4()),
                status="initialized", 
                current_phase="tech_debt_analysis",
                progress=25,
                selected_application_ids=[str(uuid.uuid4())]
            )
            
            with pytest.raises(asyncio.TimeoutError):
                await assessment_flow.analyze_technical_debt(flow_state)
                
        except ImportError:
            pytest.skip("Assessment flow components not yet implemented")

    @pytest.mark.asyncio
    async def test_invalid_user_input_handling(self, mock_context):
        """Test handling of invalid user input"""
        try:
            from app.services.crewai_flows.unified_assessment_flow import UnifiedAssessmentFlow
            from app.models.assessment_flow import AssessmentPhase
            
            mock_crewai_service = Mock()
            assessment_flow = UnifiedAssessmentFlow(mock_crewai_service, mock_context)
            
            invalid_user_input = {"invalid_field": "invalid_value"}
            
            with pytest.raises(ValueError, match="Invalid user input|Required fields missing"):
                await assessment_flow.resume_from_phase(
                    AssessmentPhase.ARCHITECTURE_MINIMUMS, 
                    invalid_user_input
                )
                
        except ImportError:
            pytest.skip("Assessment flow components not yet implemented")


class TestAssessmentFlowMultiTenantIsolation:
    """Test assessment flow multi-tenant isolation"""

    @pytest.mark.asyncio
    async def test_multi_tenant_flow_isolation(self):
        """Test that assessment flows maintain multi-tenant isolation"""
        try:
            from app.services.crewai_flows.unified_assessment_flow import UnifiedAssessmentFlow
            
            # Create flows for different clients
            context_1 = RequestContext(
                client_account_id=uuid.uuid4(),
                engagement_id=uuid.uuid4(),
                user_id=str(uuid.uuid4()),
            )
            
            context_2 = RequestContext(
                client_account_id=uuid.uuid4(),
                engagement_id=uuid.uuid4(),
                user_id=str(uuid.uuid4()),
            )
            
            mock_crewai = Mock()
            
            flow_1 = UnifiedAssessmentFlow(mock_crewai, context_1)
            flow_2 = UnifiedAssessmentFlow(mock_crewai, context_2)
            
            # Mock repositories with different client contexts
            flow_1.repository = Mock()
            flow_1.repository.client_account_id = context_1.client_account_id
            
            flow_2.repository = Mock()
            flow_2.repository.client_account_id = context_2.client_account_id
            
            # Verify different client contexts
            assert flow_1.context.client_account_id != flow_2.context.client_account_id
            assert flow_1.repository.client_account_id != flow_2.repository.client_account_id
            
        except ImportError:
            pytest.skip("Assessment flow components not yet implemented")

    @pytest.fixture
    def client(self):
        """Create test client"""
        from main import app
        return TestClient(app)

    def test_api_multi_tenant_isolation(self, client):
        """Test API endpoints maintain multi-tenant isolation"""
        # Create headers for different clients
        client_1_headers = {
            "Authorization": "Bearer client_1_token",
            "Content-Type": "application/json",
            "X-Client-Account-ID": str(uuid.uuid4()),
        }
        
        client_2_headers = {
            "Authorization": "Bearer client_2_token", 
            "Content-Type": "application/json",
            "X-Client-Account-ID": str(uuid.uuid4()),
        }
        
        assessment_id = str(uuid.uuid4())
        
        with patch('app.core.auth.get_current_context') as mock_context:
            # Client 1 creates assessment
            mock_context.return_value = RequestContext(
                client_account_id=uuid.UUID(client_1_headers["X-Client-Account-ID"]),
                engagement_id=uuid.uuid4(),
                user_id=str(uuid.uuid4()),
            )
            
            response_1 = client.post(
                "/api/v1/assessment/flow/initialize",
                headers=client_1_headers,
                json={"assessment_id": assessment_id}
            )
            
            # Client 2 tries to access client 1's assessment
            mock_context.return_value = RequestContext(
                client_account_id=uuid.UUID(client_2_headers["X-Client-Account-ID"]),
                engagement_id=uuid.uuid4(), 
                user_id=str(uuid.uuid4()),
            )
            
            response_2 = client.get(
                f"/api/v1/assessment/flow/{assessment_id}/status",
                headers=client_2_headers
            )
            
            # Should maintain isolation
            if response_1.status_code in [200, 201]:
                assert response_2.status_code in [403, 404]
            else:
                # If endpoint doesn't exist yet, that's expected
                assert response_1.status_code in [404, 405]
                assert response_2.status_code in [404, 405]


class TestAssessmentFlowPerformance:
    """Test assessment flow performance characteristics"""

    @pytest.fixture
    def mock_context(self):
        """Create mock request context"""
        return RequestContext(
            client_account_id=uuid.uuid4(),
            engagement_id=uuid.uuid4(),
            user_id=str(uuid.uuid4()),
        )

    @pytest.mark.asyncio
    async def test_assessment_initialization_performance(self, mock_context):
        """Test assessment initialization performance"""
        try:
            from app.services.crewai_flows.unified_assessment_flow import UnifiedAssessmentFlow
            
            mock_crewai_service = Mock()
            
            start_time = datetime.utcnow()
            
            assessment_flow = UnifiedAssessmentFlow(mock_crewai_service, mock_context)
            
            # Mock repository and dependencies
            assessment_flow.repository = Mock()
            assessment_flow.repository.create_assessment_flow = AsyncMock(return_value=str(uuid.uuid4()))
            assessment_flow.repository.save_flow_state = AsyncMock()
            
            with patch.object(assessment_flow, '_select_applications_for_assessment') as mock_select:
                mock_select.return_value = [{"id": uuid.uuid4(), "name": "Test App"}]
                
                result = await assessment_flow.initialize_assessment()
                
            end_time = datetime.utcnow()
            execution_time = (end_time - start_time).total_seconds()
            
            # Should complete within reasonable time
            assert execution_time < 5.0
            assert result is not None
            
        except ImportError:
            pytest.skip("Assessment flow components not yet implemented")

    @pytest.mark.asyncio
    async def test_bulk_application_processing_performance(self, mock_context):
        """Test performance with bulk application processing"""
        try:
            from app.services.crewai_flows.unified_assessment_flow import UnifiedAssessmentFlow
            from app.models.assessment_flow import AssessmentFlowState
            
            mock_crewai_service = Mock()
            mock_crewai_service.run_crew.return_value = {"status": "completed", "results": {}}
            
            assessment_flow = UnifiedAssessmentFlow(mock_crewai_service, mock_context)
            assessment_flow.repository = Mock()
            assessment_flow.repository.save_flow_state = AsyncMock()
            
            # Create flow state with many applications
            flow_state = AssessmentFlowState(
                assessment_id=str(uuid.uuid4()),
                status="initialized",
                current_phase="tech_debt_analysis",
                progress=25,
                selected_application_ids=[str(uuid.uuid4()) for _ in range(10)]  # 10 applications
            )
            
            start_time = datetime.utcnow()
            
            with patch.object(assessment_flow, '_get_application_metadata') as mock_metadata:
                mock_metadata.return_value = {"app_type": "web_application"}
                
                result = await assessment_flow.analyze_technical_debt(flow_state)
                
            end_time = datetime.utcnow()
            execution_time = (end_time - start_time).total_seconds()
            
            # Should handle bulk processing within reasonable time
            assert execution_time < 10.0
            assert result is not None
            
        except ImportError:
            pytest.skip("Assessment flow components not yet implemented")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])