# Phase 4: Discovery Flow Data Integrity Test Suite

This directory contains the comprehensive Phase 4 test suite for validating discovery flow data integrity in the AI Modernize Migration Platform. The test suite provides end-to-end validation of data relationships, constraints, API consistency, performance, and deployment readiness.

## ğŸ¯ Test Suite Overview

Phase 4 testing focuses on validating the complete data integrity of the discovery flow system after implementing proper foreign key relationships and master flow orchestration. The test suite ensures that:

1. **Data relationships are properly established** between master flows, discovery flows, data imports, raw records, and assets
2. **Database constraints are enforced** to prevent orphaned records and maintain referential integrity
3. **API endpoints return consistent data** with proper foreign key relationships
4. **System performance is optimized** and constraints don't impact query speed
5. **Production deployment is validated** with proper monitoring and alerting

## ğŸ“ Test Structure

```
tests/integration/
â”œâ”€â”€ test_discovery_flow_data_integrity.py    # Main end-to-end integration tests
â”œâ”€â”€ test_api_consistency.py                  # API consistency and relationship tests
â””â”€â”€ README_Phase4_Tests.md                   # This documentation

scripts/deployment/
â”œâ”€â”€ validate_data_integrity.py               # Production validation script
â”œâ”€â”€ performance_monitoring.py                # Performance monitoring and alerting
â””â”€â”€ run_phase4_tests.py                     # Test orchestration script
```

## ğŸ§ª Test Categories

### 1. End-to-End Integration Tests (`test_discovery_flow_data_integrity.py`)

**Purpose:** Validate complete data flow from initial import through to final asset creation.

**Test Coverage:**
- Complete data import â†’ master flow â†’ discovery flow â†’ assets pipeline
- Foreign key relationship establishment
- Cascade deletion operations
- Orphaned record prevention
- Query performance with relationships
- Constraint enforcement impact

**Key Test Methods:**
- `test_complete_data_import_to_discovery_flow_pipeline()` - Full end-to-end flow validation
- `test_foreign_key_constraints_prevent_invalid_data()` - Constraint enforcement validation
- `test_cascade_deletion_operations()` - Cascade deletion validation
- `test_orphaned_records_prevention()` - Orphan prevention validation
- `test_query_performance_with_relationships()` - Performance impact validation

### 2. API Consistency Tests (`test_api_consistency.py`)

**Purpose:** Validate that API endpoints return properly linked data with correct relationships.

**Test Coverage:**
- Master Flow Orchestrator API consistency
- Discovery Flow API data relationships
- Data Import API linkage validation
- Asset API relationship consistency
- Cross-API data integrity validation
- API response format standardization
- Error handling consistency
- Multi-tenant isolation consistency

**Key Test Methods:**
- `test_master_flow_orchestrator_api_consistency()` - Orchestrator API validation
- `test_discovery_flow_api_data_relationships()` - Discovery flow API validation
- `test_cross_api_data_integrity_validation()` - Cross-API consistency validation
- `test_api_tenant_isolation_consistency()` - Multi-tenant isolation validation

### 3. Database Constraint Tests

**Purpose:** Validate that database constraints properly enforce data integrity.

**Test Coverage:**
- Foreign key constraint validation
- Cascade deletion rule verification
- Constraint performance impact
- Orphaned record detection
- Data consistency validation

### 4. Performance Validation Tests

**Purpose:** Ensure that foreign key relationships don't negatively impact system performance.

**Test Coverage:**
- Query execution time benchmarking
- Join query performance validation
- Batch operation performance
- Concurrent operation handling
- Index optimization verification

### 5. Deployment Validation Tests

**Purpose:** Validate that the system is ready for production deployment.

**Test Coverage:**
- Production schema validation
- Index existence verification
- Constraint configuration validation
- Data consistency checks
- Monitoring query generation

## ğŸš€ Running the Tests

### Quick Start

Run the complete Phase 4 test suite:

```bash
# From the backend directory
python scripts/deployment/run_phase4_tests.py --all
```

### Individual Test Suites

Run specific test categories:

```bash
# Integration tests only
python scripts/deployment/run_phase4_tests.py --integration

# API consistency tests only
python scripts/deployment/run_phase4_tests.py --api

# Performance tests only
python scripts/deployment/run_phase4_tests.py --performance

# Deployment validation only
python scripts/deployment/run_phase4_tests.py --deployment
```

### Using pytest directly

Run specific test files:

```bash
# Run main integration tests
pytest tests/integration/test_discovery_flow_data_integrity.py -v

# Run API consistency tests
pytest tests/integration/test_api_consistency.py -v

# Run with detailed output and JSON report
pytest tests/integration/test_discovery_flow_data_integrity.py -v --tb=short --json-report --json-report-file=test_results.json
```

### Production Validation

Run production validation script:

```bash
# Validate production data integrity
python scripts/deployment/validate_data_integrity.py

# Validate specific client or engagement
python scripts/deployment/validate_data_integrity.py --client-id <uuid>
python scripts/deployment/validate_data_integrity.py --engagement-id <uuid>

# Generate monitoring queries
python scripts/deployment/validate_data_integrity.py --monitoring-queries
```

### Performance Monitoring

Run performance monitoring:

```bash
# Run performance benchmark
python scripts/deployment/performance_monitoring.py --benchmark

# Run real-time monitoring
python scripts/deployment/performance_monitoring.py --monitor --interval 60 --duration 30
```

## ğŸ“Š Test Results and Reporting

### Test Output Files

The test runner generates comprehensive reports:

```
test_results/
â”œâ”€â”€ phase4_test_results.json           # Complete test results (JSON)
â”œâ”€â”€ phase4_test_report.html            # HTML test report
â”œâ”€â”€ phase4_test_summary.txt            # Text summary report
â”œâ”€â”€ integration_test_results.json      # Integration test details
â”œâ”€â”€ api_consistency_test_results.json  # API consistency test details
â”œâ”€â”€ performance_benchmark_results.json # Performance benchmark results
â”œâ”€â”€ deployment_validation_results.json # Deployment validation results
â””â”€â”€ deployment_validation_results_monitoring.sql # Monitoring queries
```

### Understanding Test Results

**Overall Status:**
- `passed` - All tests passed, system ready for production
- `partial` - Some tests failed, review issues before deployment
- `failed` - Critical failures detected, do not deploy

**Test Suite Status:**
- `passed` - Test suite completed successfully
- `failed` - Test suite failed with errors
- `timeout` - Test suite timed out
- `error` - Test suite encountered an error

## ğŸ” Troubleshooting Common Issues

### Foreign Key Constraint Failures

**Symptoms:** Tests fail with `IntegrityError` exceptions

**Solutions:**
1. Verify foreign key relationships are properly configured
2. Check that parent records exist before creating child records
3. Ensure proper cascade deletion rules are set

```sql
-- Check foreign key constraints
SELECT
    tc.table_name,
    kcu.column_name,
    ccu.table_name AS foreign_table_name,
    ccu.column_name AS foreign_column_name
FROM information_schema.table_constraints AS tc
JOIN information_schema.key_column_usage AS kcu
    ON tc.constraint_name = kcu.constraint_name
JOIN information_schema.constraint_column_usage AS ccu
    ON ccu.constraint_name = tc.constraint_name
WHERE tc.constraint_type = 'FOREIGN KEY'
AND tc.table_name IN ('discovery_flows', 'data_imports', 'assets');
```

### Performance Issues

**Symptoms:** Tests timeout or performance benchmarks fail

**Solutions:**
1. Check database indexes on foreign key columns
2. Optimize query patterns for joins
3. Review batch operation sizes

```sql
-- Check indexes on foreign key columns
SELECT
    tablename,
    indexname,
    indexdef
FROM pg_indexes
WHERE tablename IN ('discovery_flows', 'data_imports', 'assets')
AND indexdef LIKE '%client_account_id%';
```

### Orphaned Records

**Symptoms:** Data integrity validation finds orphaned records

**Solutions:**
1. Run data cleanup scripts to remove orphaned records
2. Ensure proper parent-child relationship creation order
3. Verify cascade deletion rules are working

```sql
-- Find orphaned discovery flows
SELECT df.flow_id, df.master_flow_id
FROM discovery_flows df
LEFT JOIN crewai_flow_state_extensions mf ON df.master_flow_id = mf.flow_id
WHERE mf.flow_id IS NULL AND df.master_flow_id IS NOT NULL;
```

### API Consistency Issues

**Symptoms:** API tests fail with missing or incorrect relationship data

**Solutions:**
1. Verify repository queries include proper joins
2. Check API response serialization includes all required fields
3. Ensure multi-tenant filtering is properly applied

## ğŸ“ˆ Performance Benchmarks

### Expected Performance Thresholds

The test suite validates against these performance thresholds:

- **Simple queries:** < 500ms
- **Join queries:** < 1000ms
- **Constraint checks:** < 100ms
- **Batch operations:** < 2000ms
- **Complex aggregations:** < 300ms

### Performance Optimization

If performance tests fail, consider:

1. **Index Optimization:**
   ```sql
   CREATE INDEX IF NOT EXISTS idx_discovery_flows_master_flow_id
   ON discovery_flows(master_flow_id);

   CREATE INDEX IF NOT EXISTS idx_data_imports_master_flow_id
   ON data_imports(master_flow_id);

   CREATE INDEX IF NOT EXISTS idx_assets_discovery_flow_id
   ON assets(discovery_flow_id);
   ```

2. **Query Optimization:**
   - Use proper SELECT field lists instead of SELECT *
   - Implement query result caching for frequently accessed data
   - Consider materialized views for complex aggregations

3. **Batch Operation Optimization:**
   - Use bulk insert/update operations
   - Process data in smaller batches
   - Implement batch size tuning based on system capacity

## ğŸ”§ Configuration and Customization

### Custom Alert Thresholds

Create a custom alert thresholds file:

```json
{
  "query_execution_time_ms": 300,
  "constraint_check_time_ms": 50,
  "join_query_time_ms": 800,
  "batch_operation_time_ms": 1500,
  "active_connections": 50
}
```

Use with performance monitoring:

```bash
python scripts/deployment/performance_monitoring.py --benchmark --alert-thresholds custom_thresholds.json
```

### Environment-Specific Testing

Test against specific environments:

```bash
# Test specific client account
python scripts/deployment/validate_data_integrity.py --client-id "12345678-1234-1234-1234-123456789012"

# Test specific engagement
python scripts/deployment/validate_data_integrity.py --engagement-id "87654321-4321-4321-4321-210987654321"
```

## ğŸ“‹ Continuous Integration

### CI/CD Integration

Add Phase 4 tests to your CI/CD pipeline:

```yaml
# GitHub Actions example
- name: Run Phase 4 Data Integrity Tests
  run: |
    cd backend
    python scripts/deployment/run_phase4_tests.py --all --output-dir ci_test_results

- name: Upload Test Results
  uses: actions/upload-artifact@v3
  if: always()
  with:
    name: phase4-test-results
    path: backend/ci_test_results/
```

### Pre-deployment Validation

Before production deployment:

```bash
# Run complete validation suite
python scripts/deployment/run_phase4_tests.py --all

# Generate monitoring queries for production
python scripts/deployment/validate_data_integrity.py --monitoring-queries

# Set up performance monitoring
python scripts/deployment/performance_monitoring.py --monitor --interval 300 --duration 60
```

## ğŸ“š Additional Resources

- **Platform Documentation:** `docs/development/PLATFORM_EVOLUTION_AND_CURRENT_STATE.md`
- **Master Flow Architecture:** `docs/planning/discovery-flow/master-flow-orchestration-analysis.md`
- **Database Schema:** `backend/app/models/`
- **API Documentation:** `backend/app/api/v1/`

## ğŸ¤ Contributing

When adding new tests to the Phase 4 suite:

1. **Follow naming conventions:** `test_<category>_<specific_feature>()`
2. **Include comprehensive documentation:** Describe test purpose and validation
3. **Add proper cleanup:** Ensure test data is properly cleaned up
4. **Update this README:** Document new test methods and their purpose
5. **Consider performance impact:** Ensure new tests complete within reasonable time

## ğŸš¨ Support and Issues

If you encounter issues with the Phase 4 test suite:

1. **Check logs:** Review test output and error messages
2. **Verify database state:** Ensure database schema is up to date
3. **Check dependencies:** Verify all required packages are installed
4. **Review configuration:** Ensure database connection and environment variables are correct
5. **Run individual tests:** Isolate failing tests for detailed debugging

For additional support, review the platform documentation or contact the development team.
