"""
Flow Test Fixtures

Shared fixtures for testing Discovery and Assessment flow fixes:
- Common test data and mock objects
- Database session management
- Request context creation
- Sample flow states and transitions
- Error simulation utilities

Generated by CC for ADCS Backend Testing Framework
"""

import asyncio
import uuid
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional
from unittest.mock import AsyncMock, Mock

import pytest
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker

from app.core.context import RequestContext
from app.models.base import Base
from app.models.discovery_flow import DiscoveryFlow


@pytest.fixture(scope="session")
def event_loop():
    """Create an event loop for the test session"""
    loop = asyncio.new_event_loop()
    yield loop
    loop.close()


@pytest.fixture(scope="session")
async def test_engine():
    """Create test database engine with in-memory SQLite"""
    engine = create_async_engine(
        "sqlite+aiosqlite:///:memory:",
        echo=False,
        future=True,
        pool_pre_ping=True,
    )

    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)

    yield engine
    await engine.dispose()


@pytest.fixture(scope="session")
def test_session_factory(test_engine):
    """Create test session factory"""
    return sessionmaker(
        test_engine, 
        class_=AsyncSession, 
        expire_on_commit=False
    )


@pytest.fixture
async def test_db_session(test_session_factory):
    """Create test database session with automatic cleanup"""
    async with test_session_factory() as session:
        yield session
        await session.rollback()


@pytest.fixture
def mock_db_session():
    """Create mock database session for unit tests"""
    session = AsyncMock(spec=AsyncSession)
    session.add = Mock()
    session.commit = AsyncMock()
    session.refresh = AsyncMock()
    session.execute = AsyncMock()
    session.rollback = AsyncMock()
    session.close = AsyncMock()
    return session


@pytest.fixture
def test_context():
    """Create test request context with valid UUIDs"""
    return RequestContext(
        client_account_id=uuid.uuid4(),
        engagement_id=uuid.uuid4(),
        user_id=str(uuid.uuid4()),
    )


@pytest.fixture
def test_context_dict(test_context):
    """Create test context as dictionary for API calls"""
    return {
        "client_account_id": str(test_context.client_account_id),
        "engagement_id": str(test_context.engagement_id),
        "user_id": test_context.user_id,
    }


@pytest.fixture
def auth_headers(test_context):
    """Create authentication headers for API tests"""
    return {
        "Authorization": "Bearer test_jwt_token",
        "Content-Type": "application/json",
        "X-Client-Account-ID": str(test_context.client_account_id),
        "X-Engagement-ID": str(test_context.engagement_id),
        "X-User-ID": test_context.user_id,
    }


@pytest.fixture
def valid_flow_id():
    """Generate valid flow ID as UUID string"""
    return str(uuid.uuid4())


@pytest.fixture
def invalid_flow_ids():
    """Common invalid flow ID formats for testing"""
    return [
        None,
        "",
        "   ",
        "not-a-uuid",
        "12345",
        "invalid-format",
        123,
        [],
        {},
    ]


@pytest.fixture
def sample_discovery_flow_data(test_context, valid_flow_id):
    """Create sample discovery flow data"""
    return {
        "flow_id": valid_flow_id,
        "client_account_id": str(test_context.client_account_id),
        "engagement_id": str(test_context.engagement_id),
        "user_id": test_context.user_id,
        "flow_type": "discovery",
        "description": "Test Discovery Flow",
        "initial_state_data": {
            "status": "initialized",
            "current_phase": "initialization",
            "progress_percentage": 0.0,
            "metadata": {"test": True}
        },
        "raw_data": [
            {
                "hostname": "web-server-01",
                "ip_address": "192.168.1.10",
                "os_type": "Linux",
                "application": "Web Application",
                "environment": "Production",
                "business_criticality": "High"
            },
            {
                "hostname": "db-server-01",
                "ip_address": "192.168.1.20", 
                "os_type": "Windows",
                "application": "Database Server",
                "environment": "Production",
                "business_criticality": "Critical"
            },
            {
                "hostname": "app-server-01",
                "ip_address": "192.168.1.30",
                "os_type": "Linux", 
                "application": "Application Server",
                "environment": "Staging",
                "business_criticality": "Medium"
            }
        ],
        "configuration": {
            "enable_field_mapping": True,
            "enable_data_cleansing": True,
            "enable_dependency_analysis": True,
            "parallel_execution": False,
            "confidence_threshold": 0.8
        }
    }


@pytest.fixture
def sample_assessment_flow_data(test_context):
    """Create sample assessment flow data"""
    return {
        "assessment_id": str(uuid.uuid4()),
        "client_account_id": str(test_context.client_account_id),
        "engagement_id": str(test_context.engagement_id),
        "user_id": test_context.user_id,
        "selected_applications": [
            {
                "application_id": str(uuid.uuid4()),
                "application_name": "Customer Portal",
                "business_criticality": "High",
                "technical_complexity": "Medium",
                "environment": "Production",
                "migration_readiness": "Ready"
            },
            {
                "application_id": str(uuid.uuid4()),
                "application_name": "Payment System",
                "business_criticality": "Critical",
                "technical_complexity": "High", 
                "environment": "Production",
                "migration_readiness": "Complex"
            },
            {
                "application_id": str(uuid.uuid4()),
                "application_name": "Analytics Dashboard",
                "business_criticality": "Medium",
                "technical_complexity": "Low",
                "environment": "Staging",
                "migration_readiness": "Ready"
            }
        ],
        "assessment_scope": {
            "include_architecture_review": True,
            "include_dependency_analysis": True,
            "include_risk_assessment": True,
            "include_migration_strategy": True,
            "include_cost_analysis": True
        },
        "configuration": {
            "assessment_depth": "comprehensive",
            "timeline_weeks": 8,
            "resource_allocation": "standard",
            "include_user_training": True
        }
    }


@pytest.fixture
def discovery_flow_phases():
    """Define discovery flow phases and their expected progression"""
    return [
        {
            "name": "initialization",
            "display_name": "Initialization",
            "progress": 0.0,
            "duration_estimate": "5 minutes"
        },
        {
            "name": "data_validation", 
            "display_name": "Data Validation",
            "progress": 10.0,
            "duration_estimate": "15 minutes"
        },
        {
            "name": "field_mapping",
            "display_name": "Field Mapping", 
            "progress": 30.0,
            "duration_estimate": "20 minutes"
        },
        {
            "name": "data_cleansing",
            "display_name": "Data Cleansing",
            "progress": 50.0,
            "duration_estimate": "25 minutes"
        },
        {
            "name": "asset_inventory",
            "display_name": "Asset Inventory",
            "progress": 70.0,
            "duration_estimate": "30 minutes"
        },
        {
            "name": "dependency_analysis",
            "display_name": "Dependency Analysis",
            "progress": 90.0,
            "duration_estimate": "35 minutes"
        },
        {
            "name": "tech_debt_assessment", 
            "display_name": "Technical Debt Assessment",
            "progress": 100.0,
            "duration_estimate": "40 minutes"
        }
    ]


@pytest.fixture
def assessment_flow_phases():
    """Define assessment flow phases and their expected progression"""
    return [
        {
            "name": "architecture_minimums",
            "display_name": "Architecture Minimums",
            "progress": 20,
            "requires_user_input": True
        },
        {
            "name": "tech_debt_analysis",
            "display_name": "Technical Debt Analysis", 
            "progress": 50,
            "requires_user_input": True
        },
        {
            "name": "component_sixr_strategies",
            "display_name": "6R Strategy Determination",
            "progress": 75,
            "requires_user_input": True
        },
        {
            "name": "app_on_page_generation",
            "display_name": "App-on-Page Generation",
            "progress": 100,
            "requires_user_input": False
        }
    ]


@pytest.fixture
async def test_discovery_flow(test_db_session, test_context, valid_flow_id):
    """Create test discovery flow in database"""
    flow = DiscoveryFlow(
        id=uuid.uuid4(),
        flow_id=uuid.UUID(valid_flow_id),
        client_account_id=test_context.client_account_id,
        engagement_id=test_context.engagement_id,
        user_id=test_context.user_id,
        flow_name="Test Discovery Flow",
        status="initialized",
        current_phase="initialization",
        progress_percentage=0.0,
        crewai_state_data={"test": True},
        created_at=datetime.utcnow(),
        updated_at=datetime.utcnow(),
    )
    
    test_db_session.add(flow)
    await test_db_session.commit()
    await test_db_session.refresh(flow)
    
    return flow


@pytest.fixture
async def test_discovery_flow_with_null_id(test_db_session, test_context):
    """Create test discovery flow with null flow_id (for testing filters)"""
    flow = DiscoveryFlow(
        id=uuid.uuid4(),
        flow_id=None,  # This should be filtered out by queries
        client_account_id=test_context.client_account_id,
        engagement_id=test_context.engagement_id,
        user_id=test_context.user_id,
        flow_name="Invalid Flow",
        status="active",
        current_phase="field_mapping"
    )
    
    test_db_session.add(flow)
    await test_db_session.commit()
    await test_db_session.refresh(flow)
    
    return flow


@pytest.fixture
def mock_crewai_service():
    """Create mock CrewAI service for testing"""
    service = Mock()
    service.run_crew = AsyncMock(return_value={
        "status": "completed",
        "results": {"test": "data"},
        "execution_time": 1.5,
        "agent_insights": []
    })
    return service


@pytest.fixture
def mock_successful_crew_execution():
    """Mock successful crew execution response"""
    return {
        "status": "completed",
        "data": {
            "records_processed": 100,
            "records_valid": 95,
            "records_failed": 5,
            "field_mappings": [
                {
                    "source_field": "hostname",
                    "target_field": "server_name",
                    "confidence_score": 0.95,
                    "match_type": "exact"
                },
                {
                    "source_field": "ip_address",
                    "target_field": "ip",
                    "confidence_score": 0.98,
                    "match_type": "exact"
                }
            ]
        },
        "agent_insights": [
            {
                "agent": "field_mapping_agent",
                "insight": "High confidence field mappings identified",
                "recommendation": "Proceed with automatic mapping",
                "confidence": 0.95
            }
        ],
        "execution_time": 2.3
    }


@pytest.fixture
def mock_failed_crew_execution():
    """Mock failed crew execution response"""
    def create_failure(error_message="Crew execution failed"):
        return Exception(error_message)
    return create_failure


@pytest.fixture
def mock_timeout_crew_execution():
    """Mock crew execution timeout"""
    def create_timeout(timeout_message="Crew execution timed out"):
        return asyncio.TimeoutError(timeout_message)
    return create_timeout


@pytest.fixture
def error_scenarios():
    """Common error scenarios for testing"""
    return {
        "database_connection_lost": {
            "error": "Connection to database lost",
            "expected_status": 500,
            "recovery_action": "retry_with_backoff"
        },
        "crew_timeout": {
            "error": "Crew execution timed out after 30 seconds",
            "expected_status": 202,
            "recovery_action": "extend_timeout"
        },
        "invalid_data": {
            "error": "Invalid input data format",
            "expected_status": 422,
            "recovery_action": "validate_and_retry"
        },
        "flow_not_found": {
            "error": "Flow not found or has been deleted",
            "expected_status": 404,
            "recovery_action": "redirect_to_flows_list"
        },
        "unauthorized_access": {
            "error": "Insufficient permissions to access flow",
            "expected_status": 403,
            "recovery_action": "request_permissions"
        }
    }


@pytest.fixture
def performance_thresholds():
    """Performance thresholds for testing"""
    return {
        "api_response_time": 2.0,  # seconds
        "database_query_time": 1.0,  # seconds
        "crew_execution_time": 30.0,  # seconds
        "flow_initialization_time": 5.0,  # seconds
        "memory_usage_limit": 100 * 1024 * 1024,  # 100MB
        "concurrent_flows_limit": 10
    }


@pytest.fixture
def multi_tenant_contexts():
    """Create multiple tenant contexts for isolation testing"""
    return {
        "tenant_1": RequestContext(
            client_account_id=uuid.uuid4(),
            engagement_id=uuid.uuid4(),
            user_id=str(uuid.uuid4()),
        ),
        "tenant_2": RequestContext(
            client_account_id=uuid.uuid4(),
            engagement_id=uuid.uuid4(),
            user_id=str(uuid.uuid4()),
        ),
        "tenant_3": RequestContext(
            client_account_id=uuid.uuid4(),
            engagement_id=uuid.uuid4(),
            user_id=str(uuid.uuid4()),
        )
    }


@pytest.fixture
def large_dataset():
    """Generate large dataset for performance testing"""
    def generate_data(size: int = 100) -> List[Dict[str, Any]]:
        return [
            {
                "hostname": f"server-{i:03d}",
                "ip_address": f"192.168.{i // 255 + 1}.{i % 255 + 1}",
                "os_type": "Linux" if i % 2 == 0 else "Windows",
                "application": f"App-{i % 10}",
                "environment": ["Production", "Staging", "Development"][i % 3],
                "business_criticality": ["Low", "Medium", "High", "Critical"][i % 4],
                "cpu_cores": 4 + (i % 8),
                "memory_gb": 8 + (i % 16),
                "storage_gb": 100 + (i % 500)
            }
            for i in range(size)
        ]
    return generate_data


class FlowTestHelpers:
    """Helper methods for flow testing"""
    
    @staticmethod
    def assert_flow_state_valid(flow_state):
        """Assert that flow state is valid"""
        assert flow_state is not None
        assert flow_state.flow_id is not None
        assert flow_state.status is not None
        assert flow_state.current_phase is not None
        assert isinstance(flow_state.progress_percentage, (int, float))
        assert 0.0 <= flow_state.progress_percentage <= 100.0
        
    @staticmethod
    def assert_flow_response_format(response_data):
        """Assert that flow response has correct format"""
        required_fields = [
            'flow_id', 'status', 'type', 'current_phase',
            'progress_percentage', 'client_account_id', 'engagement_id'
        ]
        
        for field in required_fields:
            assert field in response_data, f"Missing required field: {field}"
            
        # Validate data types
        assert isinstance(response_data['progress_percentage'], (int, float))
        assert response_data['type'] == 'discovery' or response_data['type'] == 'assessment'
        
    @staticmethod
    def assert_error_response_format(error_response):
        """Assert that error response has correct format"""
        assert 'error' in error_response or 'detail' in error_response
        assert 'status' in error_response or 'status_code' in error_response
        
    @staticmethod
    async def simulate_phase_progression(flow, phases):
        """Simulate flow progressing through phases"""
        current_progress = 0.0
        
        for phase in phases:
            # Simulate phase execution
            await asyncio.sleep(0.01)  # Small delay to simulate work
            
            # Verify progress increases
            assert phase['progress'] >= current_progress
            current_progress = phase['progress']
            
        return current_progress


@pytest.fixture
def flow_test_helpers():
    """Provide FlowTestHelpers instance"""
    return FlowTestHelpers()


@pytest.fixture(autouse=True)
def setup_test_logging():
    """Setup test logging configuration"""
    import logging
    
    # Configure logging for tests
    logging.getLogger("app").setLevel(logging.WARNING)
    logging.getLogger("sqlalchemy").setLevel(logging.WARNING)
    
    # Only show errors and critical messages during tests
    logging.basicConfig(level=logging.ERROR)


@pytest.fixture
def mock_redis_cache():
    """Mock Redis cache for testing"""
    cache = Mock()
    cache.enabled = True
    cache.get = AsyncMock(return_value=None)
    cache.set = AsyncMock(return_value=True)
    cache.delete = AsyncMock(return_value=True)
    cache.exists = AsyncMock(return_value=False)
    return cache


@pytest.fixture
def test_timeout_config():
    """Test timeout configuration"""
    return {
        "api_timeout": 5.0,
        "database_timeout": 3.0,
        "crew_timeout": 10.0,
        "test_timeout": 30.0
    }