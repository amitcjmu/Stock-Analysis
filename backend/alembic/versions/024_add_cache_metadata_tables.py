"""Add cache metadata tables for Redis cache management

Revision ID: 024_add_cache_metadata_tables
Revises: 023_fix_data_imports_foreign_key_constraint
Create Date: 2025-07-31

This migration adds comprehensive cache metadata tracking tables to support
the Redis caching infrastructure with proper multi-tenant isolation,
performance monitoring, and coherence management.

Generated by CC (Claude Code)
"""

import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
from alembic import op

# revision identifiers, used by Alembic.
revision = "024_add_cache_metadata_tables"
down_revision = "023_fix_data_imports_foreign_key_constraint"
branch_labels = None
depends_on = None


def table_exists(table_name):
    """Check if a table exists in the migration schema"""
    bind = op.get_bind()
    try:
        result = bind.execute(
            sa.text(
                """
                SELECT EXISTS (
                    SELECT FROM information_schema.tables
                    WHERE table_schema = 'migration'
                    AND table_name = :table_name
                )
            """
            ).bindparams(table_name=table_name)
        ).scalar()
        return result
    except Exception as e:
        print(f"Error checking if table {table_name} exists: {e}")
        # Return False when we can't check - this will cause table creation to proceed
        # which is safer than assuming table exists and skipping creation
        return False


def create_table_if_not_exists(table_name, *columns, **kwargs):
    """Create a table only if it doesn't already exist"""
    if not table_exists(table_name):
        op.create_table(table_name, *columns, **kwargs)
        print(f"Created table: {table_name}")
    else:
        print(f"Table {table_name} already exists, skipping creation")


def index_exists(index_name, table_name):
    """Check if an index exists on a table in the migration schema"""
    bind = op.get_bind()
    try:
        result = bind.execute(
            sa.text(
                """
                SELECT EXISTS (
                    SELECT FROM pg_indexes
                    WHERE schemaname = 'migration'
                    AND tablename = :table_name
                    AND indexname = :index_name
                )
            """
            ).bindparams(table_name=table_name, index_name=index_name)
        ).scalar()
        return result
    except Exception as e:
        print(f"Error checking if index {index_name} exists on table {table_name}: {e}")
        return True


def create_index_if_not_exists(index_name, table_name, columns, **kwargs):
    """Create an index only if it doesn't already exist"""
    if table_exists(table_name) and not index_exists(index_name, table_name):
        op.create_index(index_name, table_name, columns, **kwargs)
        print(f"Created index: {index_name}")
    else:
        print(
            f"Index {index_name} already exists or table doesn't exist, skipping creation"
        )


def constraint_exists(constraint_name, table_name):
    """Check if a constraint exists on a table in the migration schema"""
    bind = op.get_bind()
    try:
        result = bind.execute(
            sa.text(
                """
                SELECT EXISTS (
                    SELECT FROM information_schema.table_constraints
                    WHERE table_schema = 'migration'
                    AND table_name = :table_name
                    AND constraint_name = :constraint_name
                )
            """
            ).bindparams(table_name=table_name, constraint_name=constraint_name)
        ).scalar()
        return result
    except Exception as e:
        print(
            f"Error checking if constraint {constraint_name} exists on table {table_name}: {e}"
        )
        return True


def create_constraint_if_not_exists(constraint_name, table_name, constraint_def):
    """Create a constraint only if it doesn't already exist"""
    if table_exists(table_name) and not constraint_exists(constraint_name, table_name):
        try:
            bind = op.get_bind()
            bind.execute(
                sa.text(
                    f"ALTER TABLE migration.{table_name} ADD CONSTRAINT {constraint_name} {constraint_def}"
                )
            )
            print(f"Created constraint: {constraint_name}")
        except Exception as e:
            print(f"Error creating constraint {constraint_name}: {e}")
    else:
        print(
            f"Constraint {constraint_name} already exists or table doesn't exist, skipping creation"
        )


def upgrade():
    """Add cache metadata tables with comprehensive indexing and constraints"""

    print("Starting cache metadata tables migration...")

    # 1. Cache Metadata Table - Primary cache tracking
    create_table_if_not_exists(
        "cache_metadata",
        sa.Column(
            "id",
            sa.Integer(),
            nullable=False,
            comment="Unique identifier for the cache metadata record",
        ),
        sa.Column(
            "cache_key",
            sa.String(length=500),
            nullable=False,
            comment="The full Redis cache key including version and tenant information",
        ),
        sa.Column(
            "cache_key_type",
            sa.String(length=50),
            nullable=False,
            comment="Type of cache key for monitoring and metrics (from CacheKeyType enum)",
        ),
        sa.Column(
            "entity_type",
            sa.String(length=50),
            nullable=False,
            comment="Type of entity being cached (user, flow, import, etc.)",
        ),
        sa.Column(
            "entity_id",
            sa.String(length=100),
            nullable=False,
            comment="ID of the primary entity being cached",
        ),
        sa.Column(
            "client_account_id",
            postgresql.UUID(as_uuid=True),
            nullable=False,
            comment="Client account this cache entry belongs to for tenant isolation",
        ),
        sa.Column(
            "engagement_id",
            postgresql.UUID(as_uuid=True),
            nullable=True,
            comment="Optional engagement context for engagement-scoped cache entries",
        ),
        sa.Column(
            "ttl_seconds",
            sa.Integer(),
            nullable=False,
            comment="Time-to-live in seconds for this cache entry",
        ),
        sa.Column(
            "expires_at",
            sa.DateTime(timezone=True),
            nullable=False,
            comment="Calculated expiration timestamp for cache cleanup",
        ),
        sa.Column(
            "is_active",
            sa.Boolean(),
            nullable=False,
            comment="Whether this cache entry is currently active",
        ),
        sa.Column(
            "access_count",
            sa.Integer(),
            nullable=False,
            comment="Number of times this cache key has been accessed",
        ),
        sa.Column(
            "hit_count",
            sa.Integer(),
            nullable=False,
            comment="Number of successful cache hits",
        ),
        sa.Column(
            "miss_count",
            sa.Integer(),
            nullable=False,
            comment="Number of cache misses (key not found or expired)",
        ),
        sa.Column(
            "last_accessed",
            sa.DateTime(timezone=True),
            nullable=True,
            comment="Timestamp of last cache access",
        ),
        sa.Column(
            "avg_access_time_ms",
            sa.Float(),
            nullable=True,
            comment="Average access time in milliseconds for performance monitoring",
        ),
        sa.Column(
            "data_size_bytes",
            sa.Integer(),
            nullable=True,
            comment="Size of cached data in bytes for memory usage tracking",
        ),
        sa.Column(
            "compression_ratio",
            sa.Float(),
            nullable=True,
            comment="Compression ratio if data is compressed (0.0-1.0)",
        ),
        sa.Column(
            "parent_cache_keys",
            postgresql.JSON(astext_type=sa.Text()),
            nullable=True,
            comment="List of parent cache keys that trigger invalidation of this entry",
        ),
        sa.Column(
            "child_cache_keys",
            postgresql.JSON(astext_type=sa.Text()),
            nullable=True,
            comment="List of child cache keys that should be invalidated when this entry changes",
        ),
        sa.Column(
            "invalidation_triggers",
            postgresql.JSON(astext_type=sa.Text()),
            nullable=True,
            comment="List of events/operations that should invalidate this cache entry",
        ),
        sa.Column(
            "cache_tags",
            postgresql.JSON(astext_type=sa.Text()),
            nullable=True,
            comment="Tags for cache organization and batch operations",
        ),
        sa.Column(
            "metadata",
            postgresql.JSON(astext_type=sa.Text()),
            nullable=True,
            comment="Additional metadata about the cached content",
        ),
        sa.Column(
            "created_by_user_id",
            postgresql.UUID(as_uuid=True),
            nullable=True,
            comment="User who initiated the cache operation",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(),
            nullable=False,
            comment="Timestamp of when the cache metadata was created",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(),
            nullable=False,
            comment="Timestamp of when the cache metadata was last updated",
        ),
        sa.ForeignKeyConstraint(
            ["client_account_id"], ["migration.client_accounts.id"], ondelete="CASCADE"
        ),
        sa.ForeignKeyConstraint(
            ["engagement_id"], ["migration.engagements.id"], ondelete="CASCADE"
        ),
        sa.ForeignKeyConstraint(
            ["created_by_user_id"], ["migration.users.id"], ondelete="SET NULL"
        ),
        sa.PrimaryKeyConstraint("id"),
        schema="migration",
    )

    # 2. Cache Performance Log Table - Detailed performance tracking
    create_table_if_not_exists(
        "cache_performance_logs",
        sa.Column(
            "id",
            sa.Integer(),
            nullable=False,
            comment="Unique identifier for the performance log entry",
        ),
        sa.Column(
            "cache_metadata_id",
            sa.Integer(),
            nullable=False,
            comment="Reference to the cache metadata entry",
        ),
        sa.Column(
            "client_account_id",
            postgresql.UUID(as_uuid=True),
            nullable=False,
            comment="Client account for tenant isolation",
        ),
        sa.Column(
            "operation_type",
            sa.String(length=20),
            nullable=False,
            comment="Type of cache operation (GET, SET, DELETE, INVALIDATE)",
        ),
        sa.Column(
            "was_hit",
            sa.Boolean(),
            nullable=False,
            comment="Whether the cache operation resulted in a hit",
        ),
        sa.Column(
            "response_time_ms",
            sa.Float(),
            nullable=False,
            comment="Response time for the cache operation in milliseconds",
        ),
        sa.Column(
            "data_size_bytes",
            sa.Integer(),
            nullable=True,
            comment="Size of data involved in the operation",
        ),
        sa.Column(
            "endpoint",
            sa.String(length=200),
            nullable=True,
            comment="API endpoint that triggered the cache operation",
        ),
        sa.Column(
            "user_id",
            postgresql.UUID(as_uuid=True),
            nullable=True,
            comment="User who initiated the operation",
        ),
        sa.Column(
            "session_id",
            sa.String(length=100),
            nullable=True,
            comment="Session identifier for request correlation",
        ),
        sa.Column(
            "error_message",
            sa.Text(),
            nullable=True,
            comment="Error message if operation failed",
        ),
        sa.Column(
            "error_code",
            sa.String(length=50),
            nullable=True,
            comment="Error code for categorization",
        ),
        sa.Column(
            "redis_memory_usage_mb",
            sa.Float(),
            nullable=True,
            comment="Redis memory usage at time of operation",
        ),
        sa.Column(
            "concurrent_operations",
            sa.Integer(),
            nullable=True,
            comment="Number of concurrent cache operations",
        ),
        sa.Column(
            "metadata",
            postgresql.JSON(astext_type=sa.Text()),
            nullable=True,
            comment="Additional operation metadata",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(),
            nullable=False,
            comment="Timestamp of when the performance log was created",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(),
            nullable=False,
            comment="Timestamp of when the performance log was last updated",
        ),
        sa.ForeignKeyConstraint(
            ["cache_metadata_id"], ["migration.cache_metadata.id"], ondelete="CASCADE"
        ),
        sa.ForeignKeyConstraint(
            ["client_account_id"], ["migration.client_accounts.id"], ondelete="CASCADE"
        ),
        sa.ForeignKeyConstraint(
            ["user_id"], ["migration.users.id"], ondelete="SET NULL"
        ),
        sa.PrimaryKeyConstraint("id"),
        schema="migration",
    )

    # 3. Cache Invalidation Log Table - Audit trail for invalidations
    create_table_if_not_exists(
        "cache_invalidation_logs",
        sa.Column(
            "id",
            sa.Integer(),
            nullable=False,
            comment="Unique identifier for the invalidation log entry",
        ),
        sa.Column(
            "cache_metadata_id",
            sa.Integer(),
            nullable=True,
            comment="Reference to the cache metadata entry that was invalidated",
        ),
        sa.Column(
            "client_account_id",
            postgresql.UUID(as_uuid=True),
            nullable=False,
            comment="Client account for tenant isolation",
        ),
        sa.Column(
            "invalidation_type",
            sa.String(length=30),
            nullable=False,
            comment="Type of invalidation (EXPLICIT, CASCADE, EXPIRATION, MANUAL)",
        ),
        sa.Column(
            "cache_key_pattern",
            sa.String(length=500),
            nullable=False,
            comment="The cache key or pattern that was invalidated",
        ),
        sa.Column(
            "keys_invalidated_count",
            sa.Integer(),
            nullable=False,
            comment="Number of cache keys invalidated by this operation",
        ),
        sa.Column(
            "trigger_entity_type",
            sa.String(length=50),
            nullable=True,
            comment="Type of entity that triggered the invalidation",
        ),
        sa.Column(
            "trigger_entity_id",
            sa.String(length=100),
            nullable=True,
            comment="ID of entity that triggered the invalidation",
        ),
        sa.Column(
            "trigger_operation",
            sa.String(length=50),
            nullable=True,
            comment="Operation that triggered invalidation (CREATE, UPDATE, DELETE)",
        ),
        sa.Column(
            "user_id",
            postgresql.UUID(as_uuid=True),
            nullable=True,
            comment="User who initiated the operation that caused invalidation",
        ),
        sa.Column(
            "endpoint",
            sa.String(length=200),
            nullable=True,
            comment="API endpoint that triggered the invalidation",
        ),
        sa.Column(
            "invalidation_time_ms",
            sa.Float(),
            nullable=True,
            comment="Time taken to complete the invalidation in milliseconds",
        ),
        sa.Column(
            "cascade_depth",
            sa.Integer(),
            nullable=False,
            comment="Depth of cascade invalidation (0 for direct invalidation)",
        ),
        sa.Column(
            "reason",
            sa.Text(),
            nullable=True,
            comment="Human-readable reason for the invalidation",
        ),
        sa.Column(
            "metadata",
            postgresql.JSON(astext_type=sa.Text()),
            nullable=True,
            comment="Additional invalidation metadata",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(),
            nullable=False,
            comment="Timestamp of when the invalidation log was created",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(),
            nullable=False,
            comment="Timestamp of when the invalidation log was last updated",
        ),
        sa.ForeignKeyConstraint(
            ["cache_metadata_id"], ["migration.cache_metadata.id"], ondelete="CASCADE"
        ),
        sa.ForeignKeyConstraint(
            ["client_account_id"], ["migration.client_accounts.id"], ondelete="CASCADE"
        ),
        sa.ForeignKeyConstraint(
            ["user_id"], ["migration.users.id"], ondelete="SET NULL"
        ),
        sa.PrimaryKeyConstraint("id"),
        schema="migration",
    )

    # 4. Cache Configuration Table - Per-client cache settings
    create_table_if_not_exists(
        "cache_configurations",
        sa.Column(
            "id",
            sa.Integer(),
            nullable=False,
            comment="Unique identifier for the cache configuration",
        ),
        sa.Column(
            "client_account_id",
            postgresql.UUID(as_uuid=True),
            nullable=False,
            comment="Client account this configuration applies to",
        ),
        sa.Column(
            "cache_enabled",
            sa.Boolean(),
            nullable=False,
            comment="Whether caching is enabled for this client",
        ),
        sa.Column(
            "distributed_cache_enabled",
            sa.Boolean(),
            nullable=False,
            comment="Whether distributed caching features are enabled",
        ),
        sa.Column(
            "cache_analytics_enabled",
            sa.Boolean(),
            nullable=False,
            comment="Whether cache analytics and logging are enabled",
        ),
        sa.Column(
            "user_context_ttl",
            sa.Integer(),
            nullable=True,
            comment="Custom TTL for user context cache entries",
        ),
        sa.Column(
            "field_mapping_ttl",
            sa.Integer(),
            nullable=True,
            comment="Custom TTL for field mapping cache entries",
        ),
        sa.Column(
            "flow_state_ttl",
            sa.Integer(),
            nullable=True,
            comment="Custom TTL for flow state cache entries",
        ),
        sa.Column(
            "agent_results_ttl",
            sa.Integer(),
            nullable=True,
            comment="Custom TTL for agent execution results",
        ),
        sa.Column(
            "max_cache_size_mb",
            sa.Integer(),
            nullable=True,
            comment="Maximum cache size for this client in MB",
        ),
        sa.Column(
            "eviction_policy",
            sa.String(length=20),
            nullable=False,
            comment="Cache eviction policy (LRU, LFU, TTL)",
        ),
        sa.Column(
            "encryption_enabled",
            sa.Boolean(),
            nullable=False,
            comment="Whether sensitive cache data should be encrypted",
        ),
        sa.Column(
            "pii_cache_enabled",
            sa.Boolean(),
            nullable=False,
            comment="Whether PII data can be cached",
        ),
        sa.Column(
            "custom_settings",
            postgresql.JSON(astext_type=sa.Text()),
            nullable=True,
            comment="Client-specific custom cache settings",
        ),
        sa.Column(
            "alert_hit_ratio_threshold",
            sa.Float(),
            nullable=True,
            comment="Alert when hit ratio falls below this threshold",
        ),
        sa.Column(
            "alert_response_time_threshold_ms",
            sa.Float(),
            nullable=True,
            comment="Alert when average response time exceeds this threshold",
        ),
        sa.Column(
            "created_at",
            sa.DateTime(),
            nullable=False,
            comment="Timestamp of when the cache configuration was created",
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(),
            nullable=False,
            comment="Timestamp of when the cache configuration was last updated",
        ),
        sa.ForeignKeyConstraint(
            ["client_account_id"], ["migration.client_accounts.id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("id"),
        schema="migration",
    )

    # Create Indexes for Performance Optimization
    print("Creating cache metadata indexes...")

    # Cache Metadata Indexes
    create_index_if_not_exists(
        "ix_cache_metadata_cache_key", "cache_metadata", ["cache_key"]
    )
    create_index_if_not_exists(
        "ix_cache_metadata_client_account_id", "cache_metadata", ["client_account_id"]
    )
    create_index_if_not_exists(
        "ix_cache_metadata_entity_lookup",
        "cache_metadata",
        ["entity_type", "entity_id", "client_account_id"],
    )
    create_index_if_not_exists(
        "ix_cache_metadata_expiration", "cache_metadata", ["expires_at"]
    )
    create_index_if_not_exists(
        "ix_cache_metadata_access_patterns",
        "cache_metadata",
        ["last_accessed", "access_count"],
    )
    create_index_if_not_exists(
        "ix_cache_metadata_tenant_active",
        "cache_metadata",
        ["client_account_id", "is_active"],
    )

    # Performance Log Indexes
    create_index_if_not_exists(
        "ix_cache_perf_time_series",
        "cache_performance_logs",
        ["created_at", "operation_type"],
    )
    create_index_if_not_exists(
        "ix_cache_perf_metadata_lookup",
        "cache_performance_logs",
        ["cache_metadata_id", "created_at"],
    )
    create_index_if_not_exists(
        "ix_cache_perf_tenant_analytics",
        "cache_performance_logs",
        ["client_account_id", "created_at"],
    )

    # Invalidation Log Indexes
    create_index_if_not_exists(
        "ix_cache_invalidation_time_series",
        "cache_invalidation_logs",
        ["created_at", "invalidation_type"],
    )
    create_index_if_not_exists(
        "ix_cache_invalidation_metadata",
        "cache_invalidation_logs",
        ["cache_metadata_id", "created_at"],
    )
    create_index_if_not_exists(
        "ix_cache_invalidation_trigger",
        "cache_invalidation_logs",
        ["trigger_entity_type", "trigger_entity_id"],
    )

    # Create Unique Constraints
    print("Creating unique constraints...")

    # Unique cache key per tenant
    create_constraint_if_not_exists(
        "uq_cache_metadata_key_tenant",
        "cache_metadata",
        "UNIQUE (cache_key, client_account_id)",
    )

    # One cache configuration per client
    create_constraint_if_not_exists(
        "uq_cache_config_client", "cache_configurations", "UNIQUE (client_account_id)"
    )

    # Create Check Constraints for Data Integrity
    print("Creating check constraints...")

    # Cache metadata constraints
    create_constraint_if_not_exists(
        "ck_cache_metadata_positive_ttl", "cache_metadata", "CHECK (ttl_seconds > 0)"
    )

    create_constraint_if_not_exists(
        "ck_cache_metadata_positive_access_count",
        "cache_metadata",
        "CHECK (access_count >= 0)",
    )

    create_constraint_if_not_exists(
        "ck_cache_metadata_positive_hit_count",
        "cache_metadata",
        "CHECK (hit_count >= 0)",
    )

    create_constraint_if_not_exists(
        "ck_cache_metadata_positive_miss_count",
        "cache_metadata",
        "CHECK (miss_count >= 0)",
    )

    create_constraint_if_not_exists(
        "ck_cache_metadata_valid_compression_ratio",
        "cache_metadata",
        "CHECK (compression_ratio IS NULL OR (compression_ratio >= 0.0 AND compression_ratio <= 1.0))",
    )

    # Performance log constraints
    create_constraint_if_not_exists(
        "ck_cache_perf_positive_response_time",
        "cache_performance_logs",
        "CHECK (response_time_ms >= 0)",
    )

    create_constraint_if_not_exists(
        "ck_cache_perf_valid_operation_type",
        "cache_performance_logs",
        "CHECK (operation_type IN ('GET', 'SET', 'DELETE', 'INVALIDATE', 'EXISTS'))",
    )

    # Invalidation log constraints
    create_constraint_if_not_exists(
        "ck_cache_invalidation_positive_count",
        "cache_invalidation_logs",
        "CHECK (keys_invalidated_count > 0)",
    )

    create_constraint_if_not_exists(
        "ck_cache_invalidation_positive_cascade_depth",
        "cache_invalidation_logs",
        "CHECK (cascade_depth >= 0)",
    )

    create_constraint_if_not_exists(
        "ck_cache_invalidation_valid_type",
        "cache_invalidation_logs",
        "CHECK (invalidation_type IN ('EXPLICIT', 'CASCADE', 'EXPIRATION', 'MANUAL', 'BATCH'))",
    )

    # Configuration constraints
    create_constraint_if_not_exists(
        "ck_cache_config_positive_max_size",
        "cache_configurations",
        "CHECK (max_cache_size_mb IS NULL OR max_cache_size_mb > 0)",
    )

    create_constraint_if_not_exists(
        "ck_cache_config_valid_eviction_policy",
        "cache_configurations",
        "CHECK (eviction_policy IN ('LRU', 'LFU', 'TTL', 'RANDOM'))",
    )

    create_constraint_if_not_exists(
        "ck_cache_config_valid_hit_ratio_threshold",
        "cache_configurations",
        "CHECK (alert_hit_ratio_threshold IS NULL OR "
        "(alert_hit_ratio_threshold >= 0.0 AND alert_hit_ratio_threshold <= 1.0))",
    )

    # Create partial index for active cache entries (most common queries)
    if table_exists("cache_metadata"):
        try:
            bind = op.get_bind()
            if not index_exists("ix_cache_metadata_active_keys", "cache_metadata"):
                bind.execute(
                    sa.text(
                        """
                    CREATE INDEX ix_cache_metadata_active_keys
                    ON migration.cache_metadata (cache_key, client_account_id)
                    WHERE is_active = true
                    """
                    )
                )
                print("Created partial index: ix_cache_metadata_active_keys")
        except Exception as e:
            print(f"Error creating partial index: {e}")

    # Set default values for existing configurations if any
    try:
        bind = op.get_bind()
        bind.execute(
            sa.text(
                """
            UPDATE migration.cache_metadata
            SET
                is_active = COALESCE(is_active, true),
                access_count = COALESCE(access_count, 0),
                hit_count = COALESCE(hit_count, 0),
                miss_count = COALESCE(miss_count, 0),
                parent_cache_keys = COALESCE(parent_cache_keys, '[]'::json),
                child_cache_keys = COALESCE(child_cache_keys, '[]'::json),
                invalidation_triggers = COALESCE(invalidation_triggers, '[]'::json),
                cache_tags = COALESCE(cache_tags, '[]'::json),
                metadata = COALESCE(metadata, '{}'::json)
            WHERE
                is_active IS NULL OR
                access_count IS NULL OR
                hit_count IS NULL OR
                miss_count IS NULL OR
                parent_cache_keys IS NULL OR
                child_cache_keys IS NULL OR
                invalidation_triggers IS NULL OR
                cache_tags IS NULL OR
                metadata IS NULL
            """
            )
        )

        bind.execute(
            sa.text(
                """
            UPDATE migration.cache_configurations
            SET
                cache_enabled = COALESCE(cache_enabled, true),
                distributed_cache_enabled = COALESCE(distributed_cache_enabled, true),
                cache_analytics_enabled = COALESCE(cache_analytics_enabled, true),
                eviction_policy = COALESCE(eviction_policy, 'LRU'),
                encryption_enabled = COALESCE(encryption_enabled, false),
                pii_cache_enabled = COALESCE(pii_cache_enabled, false),
                custom_settings = COALESCE(custom_settings, '{}'::json),
                alert_hit_ratio_threshold = COALESCE(alert_hit_ratio_threshold, 0.7),
                alert_response_time_threshold_ms = COALESCE(alert_response_time_threshold_ms, 100.0)
            WHERE
                cache_enabled IS NULL OR
                distributed_cache_enabled IS NULL OR
                cache_analytics_enabled IS NULL OR
                eviction_policy IS NULL OR
                encryption_enabled IS NULL OR
                pii_cache_enabled IS NULL OR
                custom_settings IS NULL OR
                alert_hit_ratio_threshold IS NULL OR
                alert_response_time_threshold_ms IS NULL
            """
            )
        )

        bind.execute(
            sa.text(
                """
            UPDATE migration.cache_invalidation_logs
            SET
                keys_invalidated_count = COALESCE(keys_invalidated_count, 1),
                cascade_depth = COALESCE(cascade_depth, 0),
                metadata = COALESCE(metadata, '{}'::json)
            WHERE
                keys_invalidated_count IS NULL OR
                cascade_depth IS NULL OR
                metadata IS NULL
            """
            )
        )

        bind.execute(
            sa.text(
                """
            UPDATE migration.cache_performance_logs
            SET
                metadata = COALESCE(metadata, '{}'::json)
            WHERE
                metadata IS NULL
            """
            )
        )

    except Exception as e:
        print(f"Error setting default values: {e}")

    print("Cache metadata tables migration completed successfully!")


def downgrade():
    """Remove cache metadata tables and related objects"""

    print("Starting cache metadata tables downgrade...")

    # Drop indexes first (if they exist)
    indexes_to_drop = [
        "ix_cache_metadata_active_keys",
        "ix_cache_invalidation_trigger",
        "ix_cache_invalidation_metadata",
        "ix_cache_invalidation_time_series",
        "ix_cache_perf_tenant_analytics",
        "ix_cache_perf_metadata_lookup",
        "ix_cache_perf_time_series",
        "ix_cache_metadata_tenant_active",
        "ix_cache_metadata_access_patterns",
        "ix_cache_metadata_expiration",
        "ix_cache_metadata_entity_lookup",
        "ix_cache_metadata_client_account_id",
        "ix_cache_metadata_cache_key",
    ]

    for index_name in indexes_to_drop:
        try:
            if (
                index_exists(index_name, "cache_metadata")
                or index_exists(index_name, "cache_performance_logs")
                or index_exists(index_name, "cache_invalidation_logs")
            ):
                op.drop_index(index_name, schema="migration")
                print(f"Dropped index: {index_name}")
        except Exception as e:
            print(f"Error dropping index {index_name}: {e}")

    # Drop tables in reverse dependency order
    tables_to_drop = [
        "cache_configurations",
        "cache_invalidation_logs",
        "cache_performance_logs",
        "cache_metadata",
    ]

    for table_name in tables_to_drop:
        if table_exists(table_name):
            try:
                op.drop_table(table_name, schema="migration")
                print(f"Dropped table: {table_name}")
            except Exception as e:
                print(f"Error dropping table {table_name}: {e}")
        else:
            print(f"Table {table_name} doesn't exist, skipping")

    print("Cache metadata tables downgrade completed!")
