"""
Vulnerability Enrichment Agent - Enriches assets with security vulnerabilities (CVE tracking).

**ADR COMPLIANCE**:
- ADR-015: Uses TenantScopedAgentPool for persistent agents
- ADR-024: Uses TenantMemoryManager for learning (memory=False)
- LLM Tracking: Uses multi_model_service.generate_response()

**Target Table**: asset_vulnerabilities
**Fields Populated**:
- vulnerability_id: CVE identifier (e.g., CVE-2024-12345)
- severity: 'critical', 'high', 'medium', 'low'
- cvss_score: CVSS score (0.0-10.0)
- remediation_status: 'open', 'patched', 'mitigated', 'accepted'
"""

import logging
from typing import Any, Dict, List
from uuid import UUID

from sqlalchemy.ext.asyncio import AsyncSession

from app.models.asset import Asset
from app.services.crewai_flows.memory.tenant_memory_manager import (
    LearningScope,
    TenantMemoryManager,
)
from app.services.enrichment.constants import get_db_pattern_type
from app.services.multi_model_service import TaskComplexity, multi_model_service
from app.services.persistent_agents.tenant_scoped_agent_pool import (
    TenantScopedAgentPool,
)
from app.utils.json_sanitization import safe_parse_llm_json

logger = logging.getLogger(__name__)


class VulnerabilityEnrichmentAgent:
    """
    Enriches assets with security vulnerabilities using CVE database knowledge.

    **Enrichment Target**: asset_vulnerabilities table

    **Fields Populated**:
    - vulnerability_id: CVE identifier
    - severity: Vulnerability severity level
    - cvss_score: CVSS score
    - remediation_status: Current remediation status

    **Agent Strategy**:
    1. Analyze technology stack and versions
    2. Retrieve known vulnerabilities for the software stack
    3. Use LLM to identify applicable CVEs
    4. Store vulnerability data
    5. Update asset risk profile
    """

    def __init__(
        self,
        db: AsyncSession,
        agent_pool: TenantScopedAgentPool,
        memory_manager: TenantMemoryManager,
        client_account_id: UUID,
        engagement_id: UUID,
    ):
        self.db = db
        self.agent_pool = agent_pool
        self.memory_manager = memory_manager
        self.client_account_id = client_account_id
        self.engagement_id = engagement_id

    async def enrich_assets(self, assets: List[Asset]) -> int:
        """
        Enrich multiple assets with vulnerability information.

        Args:
            assets: List of Asset objects to enrich

        Returns:
            Count of successfully enriched assets
        """
        enriched_count = 0

        for asset in assets:
            try:
                # Step 1: Retrieve similar vulnerability patterns (scope is implicit via engagement_id)
                patterns = await self.memory_manager.retrieve_similar_patterns(
                    client_account_id=self.client_account_id,
                    engagement_id=self.engagement_id,
                    pattern_type=get_db_pattern_type("vulnerability_analysis").value,
                    query_context={
                        "asset_type": asset.asset_type,
                        "technology_stack": asset.technology_stack or [],
                        "operating_system": asset.operating_system,
                    },
                )

                # Step 2: Build analysis prompt
                prompt = self._build_vulnerability_prompt(asset, patterns)

                # Step 3: Call LLM (automatic tracking)
                response = await multi_model_service.generate_response(
                    prompt=prompt,
                    task_type="vulnerability_analysis",
                    complexity=TaskComplexity.AGENTIC,
                )

                # Step 4: Parse response
                vulnerability_data = self._parse_vulnerability_response(
                    response["response"]
                )

                # Step 5: Store enrichment data
                if asset.custom_attributes is None:
                    asset.custom_attributes = {}

                asset.custom_attributes["vulnerability_enrichment"] = {
                    "vulnerabilities": vulnerability_data.get("vulnerabilities", []),
                    "critical_count": vulnerability_data.get("critical_count", 0),
                    "high_count": vulnerability_data.get("high_count", 0),
                    "total_cvss_score": vulnerability_data.get("total_cvss_score", 0.0),
                    "confidence": vulnerability_data.get("confidence", 0.6),
                    "enriched_at": "now",
                }

                await self.db.flush()

                # Step 6: Store learned pattern
                await self.memory_manager.store_learning(
                    client_account_id=self.client_account_id,
                    engagement_id=self.engagement_id,
                    scope=LearningScope.ENGAGEMENT,
                    pattern_type=get_db_pattern_type("vulnerability_analysis").value,
                    pattern_data={
                        "asset_type": asset.asset_type,
                        "technology_stack": asset.technology_stack or [],
                        "operating_system": asset.operating_system,
                        "vulnerability_count": len(
                            vulnerability_data.get("vulnerabilities", [])
                        ),
                        "critical_count": vulnerability_data.get("critical_count", 0),
                        "confidence": vulnerability_data.get("confidence", 0.6),
                    },
                )

                enriched_count += 1
                vuln_count = len(vulnerability_data.get("vulnerabilities", []))
                logger.info(
                    f"Enriched asset {asset.id} with {vuln_count} vulnerabilities "
                    f"(critical: {vulnerability_data.get('critical_count', 0)})"
                )

            except Exception as e:
                logger.error(f"Failed to enrich asset {asset.id}: {e}", exc_info=True)
                continue

        return enriched_count

    def _build_vulnerability_prompt(self, asset: Asset, patterns: List[Dict]) -> str:
        """Build LLM prompt for vulnerability analysis"""
        pattern_context = ""
        if patterns:
            pattern_context = "\n\nKnown vulnerabilities for similar assets:\n"
            for pattern in patterns[:3]:
                pattern_data = pattern.get("pattern_data", {})
                os = pattern_data.get("operating_system")
                vuln_count = pattern_data.get("vulnerability_count")
                crit_count = pattern_data.get("critical_count")
                pattern_context += (
                    f"- {os}: {vuln_count} vulns ({crit_count} critical)\n"
                )

        return f"""
Analyze the following asset to identify known security vulnerabilities:

Asset Name: {asset.asset_name or 'Unknown'}
Asset Type: {asset.asset_type}
Technology Stack: {asset.technology_stack or 'Not specified'}
Operating System: {asset.operating_system or 'Unknown'}
OS Version: {asset.os_version or 'Unknown'}

{pattern_context}

Based on this information, identify:

1. **Known CVEs**: List of CVE identifiers applicable to this technology stack
   - For each CVE, provide:
     - vulnerability_id (e.g., "CVE-2024-12345")
     - severity: "critical", "high", "medium", or "low"
     - cvss_score: CVSS 3.x score (0.0-10.0)
     - description: Brief description of the vulnerability
     - remediation_status: "open" (assume unpatched unless stated otherwise)

2. **Risk Summary**:
   - Total number of vulnerabilities
   - Count by severity (critical, high, medium, low)
   - Highest CVSS score

Return analysis in JSON format:
{{
    "vulnerabilities": [
        {{
            "vulnerability_id": "CVE-2024-12345",
            "severity": "critical",
            "cvss_score": 9.8,
            "description": "Brief description",
            "remediation_status": "open"
        }}
    ],
    "critical_count": 1,
    "high_count": 2,
    "medium_count": 3,
    "low_count": 1,
    "total_cvss_score": 9.8,
    "confidence": 0.7,
    "reasoning": "Explanation of analysis"
}}

IMPORTANT:
- Only include known CVEs from public databases
- If technology/version is unknown, list common vulnerabilities for that type
- If no vulnerabilities found, return empty array
- Provide confidence score based on data completeness
- Focus on critical and high severity vulnerabilities
"""

    def _parse_vulnerability_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response into vulnerability data.

        Uses safe_parse_llm_json per ADR-029 to handle LLM output quirks:
        - Markdown code blocks (```json...```)
        - Trailing commas, single quotes
        - JSON embedded in text
        """
        # ADR-029: Use safe_parse_llm_json instead of json.loads
        data = safe_parse_llm_json(response)

        if data is None:
            logger.warning(
                f"Failed to parse vulnerability response: {response[:100]}..."
            )
            return {
                "vulnerabilities": [],
                "critical_count": 0,
                "high_count": 0,
                "medium_count": 0,
                "low_count": 0,
                "total_cvss_score": 0.0,
                "confidence": 0.2,
                "reasoning": "Failed to parse LLM response",
            }

        try:
            # Validate vulnerability structure
            vulnerabilities = data.get("vulnerabilities", [])
            if not isinstance(vulnerabilities, list):
                vulnerabilities = []

            normalized_vulns = []
            for vuln in vulnerabilities:
                try:
                    normalized_vuln = {
                        "vulnerability_id": vuln.get("vulnerability_id", "CVE-UNKNOWN"),
                        "severity": vuln.get("severity", "medium"),
                        "cvss_score": float(vuln.get("cvss_score", 0.0)),
                        "description": vuln.get("description", ""),
                        "remediation_status": vuln.get("remediation_status", "open"),
                    }
                    normalized_vulns.append(normalized_vuln)
                except (ValueError, TypeError):
                    continue

            normalized = {
                "vulnerabilities": normalized_vulns,
                "critical_count": int(data.get("critical_count", 0)),
                "high_count": int(data.get("high_count", 0)),
                "medium_count": int(data.get("medium_count", 0)),
                "low_count": int(data.get("low_count", 0)),
                "total_cvss_score": float(data.get("total_cvss_score", 0.0)),
                "confidence": float(data.get("confidence", 0.5)),
                "reasoning": data.get("reasoning", ""),
            }

            return normalized

        except (ValueError, TypeError) as e:
            logger.warning(f"Failed to normalize vulnerability data: {e}")
            return {
                "vulnerabilities": [],
                "critical_count": 0,
                "high_count": 0,
                "medium_count": 0,
                "low_count": 0,
                "total_cvss_score": 0.0,
                "confidence": 0.2,
                "reasoning": "Failed to normalize LLM response data",
            }
