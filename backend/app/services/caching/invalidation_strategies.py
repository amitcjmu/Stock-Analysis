"""
Cache Invalidation Strategies for Auth Performance Optimization

This module implements comprehensive cache invalidation strategies including:
- Time-based expiration with strategic refresh windows
- Event-driven invalidation for real-time updates
- Write-through and write-behind patterns
- Cache versioning and data integrity checks
- Adaptive TTL management

These strategies ensure cache consistency while maintaining high performance
and handling complex data dependencies in the auth performance system.

ðŸ”’ Security: Multi-tenant isolation and secure invalidation
âš¡ Performance: Optimized batch operations and minimal overhead
ðŸŽ¯ Coherence: Cascade invalidation with cycle detection
ðŸ“Š Analytics: Comprehensive invalidation tracking and metrics

Generated by CC (Claude Code)
"""

import time
from abc import ABC, abstractmethod
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Set

from app.constants.cache_keys import CacheKeys
from app.core.logging import get_logger
from app.services.caching.redis_cache import RedisCache

logger = get_logger(__name__)


class InvalidationTrigger(Enum):
    """Types of events that can trigger cache invalidation"""

    USER_LOGIN = "user_login"
    USER_LOGOUT = "user_logout"
    USER_CONTEXT_CHANGE = "user_context_change"
    USER_ROLE_CHANGE = "user_role_change"
    USER_PERMISSION_CHANGE = "user_permission_change"
    CLIENT_ACCESS_CHANGE = "client_access_change"
    ENGAGEMENT_CHANGE = "engagement_change"
    DATA_MODIFICATION = "data_modification"
    SYSTEM_EVENT = "system_event"
    SECURITY_EVENT = "security_event"
    MANUAL_INVALIDATION = "manual_invalidation"
    TTL_EXPIRATION = "ttl_expiration"
    BACKGROUND_REFRESH = "background_refresh"


class InvalidationPriority(Enum):
    """Priority levels for invalidation operations"""

    CRITICAL = 1  # Security events, login/logout
    HIGH = 2  # Context changes, role changes
    MEDIUM = 3  # Data modifications, engagement changes
    LOW = 4  # Analytics, background refresh
    BACKGROUND = 5  # Cleanup, optimization


@dataclass
class InvalidationEvent:
    """Represents a cache invalidation event"""

    trigger: InvalidationTrigger
    priority: InvalidationPriority
    entity_type: str
    entity_id: str
    client_account_id: str
    user_id: Optional[str] = None
    engagement_id: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.utcnow)
    correlation_id: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization"""
        return {
            "trigger": self.trigger.value,
            "priority": self.priority.value,
            "entity_type": self.entity_type,
            "entity_id": self.entity_id,
            "client_account_id": str(self.client_account_id),
            "user_id": str(self.user_id) if self.user_id else None,
            "engagement_id": str(self.engagement_id) if self.engagement_id else None,
            "metadata": self.metadata,
            "timestamp": self.timestamp.isoformat(),
            "correlation_id": self.correlation_id,
        }


@dataclass
class InvalidationResult:
    """Result of an invalidation operation"""

    success: bool
    keys_invalidated: int
    execution_time_ms: float
    strategy_used: str
    errors: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def __post_init__(self):
        if not self.metadata:
            self.metadata = {}


class BaseInvalidationStrategy(ABC):
    """Base class for all cache invalidation strategies"""

    def __init__(self, redis_cache: RedisCache):
        self.redis_cache = redis_cache
        self.stats = {
            "total_invalidations": 0,
            "successful_invalidations": 0,
            "failed_invalidations": 0,
            "keys_invalidated": 0,
            "total_execution_time_ms": 0,
        }

    @abstractmethod
    async def invalidate(self, event: InvalidationEvent) -> InvalidationResult:
        """Execute invalidation strategy for the given event"""
        pass

    @abstractmethod
    def can_handle(self, event: InvalidationEvent) -> bool:
        """Check if this strategy can handle the given event"""
        pass

    def get_strategy_name(self) -> str:
        """Get the name of this strategy"""
        return self.__class__.__name__

    def update_stats(self, result: InvalidationResult) -> None:
        """Update strategy statistics"""
        self.stats["total_invalidations"] += 1
        if result.success:
            self.stats["successful_invalidations"] += 1
        else:
            self.stats["failed_invalidations"] += 1
        self.stats["keys_invalidated"] += result.keys_invalidated
        self.stats["total_execution_time_ms"] += result.execution_time_ms


class TimeBasedInvalidationStrategy(BaseInvalidationStrategy):
    """
    Handles time-based cache invalidation including:
    - TTL expiration
    - Strategic refresh windows
    - Sliding expiration for frequently accessed data
    - Predictive refresh based on usage patterns
    """

    def __init__(self, redis_cache: RedisCache):
        super().__init__(redis_cache)
        self.refresh_window_ratio = 0.1  # Refresh when 10% of TTL remains
        self.access_pattern_cache: Dict[str, List[datetime]] = defaultdict(list)
        self.max_access_history = 100  # Keep last 100 access times

    def can_handle(self, event: InvalidationEvent) -> bool:
        """Handle TTL expiration and background refresh events"""
        return event.trigger in [
            InvalidationTrigger.TTL_EXPIRATION,
            InvalidationTrigger.BACKGROUND_REFRESH,
        ]

    async def invalidate(self, event: InvalidationEvent) -> InvalidationResult:
        """Execute time-based invalidation"""
        start_time = time.time()

        try:
            if event.trigger == InvalidationTrigger.TTL_EXPIRATION:
                return await self._handle_ttl_expiration(event)
            elif event.trigger == InvalidationTrigger.BACKGROUND_REFRESH:
                return await self._handle_background_refresh(event)
            else:
                return InvalidationResult(
                    success=False,
                    keys_invalidated=0,
                    execution_time_ms=(time.time() - start_time) * 1000,
                    strategy_used=self.get_strategy_name(),
                    errors=[f"Unsupported trigger: {event.trigger}"],
                )

        except Exception as e:
            logger.error(f"Time-based invalidation failed: {e}")
            return InvalidationResult(
                success=False,
                keys_invalidated=0,
                execution_time_ms=(time.time() - start_time) * 1000,
                strategy_used=self.get_strategy_name(),
                errors=[str(e)],
            )

    async def _handle_ttl_expiration(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle TTL-based expiration"""
        start_time = time.time()
        keys_invalidated = 0

        # Generate cache keys for the entity
        cache_keys = self._generate_entity_cache_keys(event)

        # Remove expired keys from Redis and update statistics
        for cache_key in cache_keys:
            try:
                deleted = await self.redis_cache.delete(cache_key)
                if deleted:
                    keys_invalidated += 1
                    logger.debug(f"Expired cache key: {cache_key}")
            except Exception as e:
                logger.error(f"Failed to delete expired key {cache_key}: {e}")

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=keys_invalidated,
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={"trigger": "ttl_expiration", "entity_type": event.entity_type},
        )

    async def _handle_background_refresh(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle background refresh operations"""
        start_time = time.time()
        keys_invalidated = 0

        # Check if keys are within refresh window
        cache_keys = self._generate_entity_cache_keys(event)

        for cache_key in cache_keys:
            try:
                # Check if key exists and get TTL
                if await self.redis_cache.exists(cache_key):
                    # For background refresh, we mark keys for refresh but don't delete them
                    # This allows the application to refresh data proactively
                    await self._mark_for_refresh(cache_key, event)
                    keys_invalidated += 1
            except Exception as e:
                logger.error(
                    f"Failed to process background refresh for {cache_key}: {e}"
                )

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=keys_invalidated,
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={
                "trigger": "background_refresh",
                "entity_type": event.entity_type,
            },
        )

    async def _mark_for_refresh(self, cache_key: str, event: InvalidationEvent) -> None:
        """Mark cache key for background refresh"""
        refresh_key = f"{cache_key}:refresh_pending"
        await self.redis_cache.set(
            refresh_key,
            {
                "marked_at": datetime.utcnow().isoformat(),
                "event": event.to_dict(),
            },
            ttl=300,
        )  # 5 minute refresh window

    def _generate_entity_cache_keys(self, event: InvalidationEvent) -> List[str]:
        """Generate relevant cache keys for an entity"""
        cache_keys = []

        if event.entity_type == "user":
            cache_keys.extend(
                [
                    CacheKeys.user_context(event.entity_id),
                    CacheKeys.user_clients(event.entity_id),
                    CacheKeys.user_defaults(event.entity_id),
                ]
            )

            if event.client_account_id:
                cache_keys.append(
                    CacheKeys.user_engagements(event.entity_id, event.client_account_id)
                )

        elif event.entity_type == "client":
            cache_keys.extend(
                [
                    CacheKeys.client_engagements(event.entity_id),
                    CacheKeys.client_users(event.entity_id),
                    CacheKeys.client_settings(event.entity_id),
                ]
            )

        elif event.entity_type == "engagement" and event.client_account_id:
            cache_keys.extend(
                [
                    CacheKeys.client_flows(event.client_account_id, event.entity_id),
                ]
            )

        return cache_keys

    def track_access_pattern(self, cache_key: str) -> None:
        """Track access patterns for predictive refresh"""
        now = datetime.utcnow()
        access_history = self.access_pattern_cache[cache_key]

        # Add current access time
        access_history.append(now)

        # Keep only recent access times
        if len(access_history) > self.max_access_history:
            access_history.pop(0)

        # Clean up old access times (older than 24 hours)
        cutoff_time = now - timedelta(hours=24)
        self.access_pattern_cache[cache_key] = [
            access_time for access_time in access_history if access_time > cutoff_time
        ]

    def should_refresh_early(
        self, cache_key: str, current_ttl: int, original_ttl: int
    ) -> bool:
        """Determine if cache should be refreshed before expiration"""
        # Check if within refresh window
        refresh_threshold = original_ttl * self.refresh_window_ratio
        if current_ttl > refresh_threshold:
            return False

        # Check access patterns for frequently accessed keys
        access_history = self.access_pattern_cache.get(cache_key, [])
        if len(access_history) < 5:  # Not enough data
            return False

        # Calculate access frequency in last hour
        one_hour_ago = datetime.utcnow() - timedelta(hours=1)
        recent_accesses = [
            access_time for access_time in access_history if access_time > one_hour_ago
        ]

        # Refresh early if accessed more than 10 times in last hour
        return len(recent_accesses) > 10


class EventDrivenInvalidationStrategy(BaseInvalidationStrategy):
    """
    Handles event-driven cache invalidation including:
    - Real-time updates when data changes
    - User action triggers (login, logout, context changes)
    - Bulk invalidation for related data
    - Cascading invalidation with dependency tracking
    """

    def __init__(self, redis_cache: RedisCache):
        super().__init__(redis_cache)
        self.cascade_relationships = self._build_cascade_relationships()
        self.max_cascade_depth = 5

    def can_handle(self, event: InvalidationEvent) -> bool:
        """Handle all event-driven invalidation triggers"""
        return event.trigger in [
            InvalidationTrigger.USER_LOGIN,
            InvalidationTrigger.USER_LOGOUT,
            InvalidationTrigger.USER_CONTEXT_CHANGE,
            InvalidationTrigger.USER_ROLE_CHANGE,
            InvalidationTrigger.USER_PERMISSION_CHANGE,
            InvalidationTrigger.CLIENT_ACCESS_CHANGE,
            InvalidationTrigger.ENGAGEMENT_CHANGE,
            InvalidationTrigger.DATA_MODIFICATION,
            InvalidationTrigger.SECURITY_EVENT,
            InvalidationTrigger.MANUAL_INVALIDATION,
        ]

    async def invalidate(self, event: InvalidationEvent) -> InvalidationResult:
        """Execute event-driven invalidation"""
        start_time = time.time()

        try:
            if event.trigger in [
                InvalidationTrigger.USER_LOGIN,
                InvalidationTrigger.USER_LOGOUT,
            ]:
                return await self._handle_user_session_event(event)
            elif event.trigger == InvalidationTrigger.USER_CONTEXT_CHANGE:
                return await self._handle_user_context_change(event)
            elif event.trigger in [
                InvalidationTrigger.USER_ROLE_CHANGE,
                InvalidationTrigger.USER_PERMISSION_CHANGE,
            ]:
                return await self._handle_user_permission_change(event)
            elif event.trigger == InvalidationTrigger.CLIENT_ACCESS_CHANGE:
                return await self._handle_client_access_change(event)
            elif event.trigger == InvalidationTrigger.ENGAGEMENT_CHANGE:
                return await self._handle_engagement_change(event)
            elif event.trigger == InvalidationTrigger.DATA_MODIFICATION:
                return await self._handle_data_modification(event)
            elif event.trigger == InvalidationTrigger.SECURITY_EVENT:
                return await self._handle_security_event(event)
            elif event.trigger == InvalidationTrigger.MANUAL_INVALIDATION:
                return await self._handle_manual_invalidation(event)
            else:
                return InvalidationResult(
                    success=False,
                    keys_invalidated=0,
                    execution_time_ms=(time.time() - start_time) * 1000,
                    strategy_used=self.get_strategy_name(),
                    errors=[f"Unsupported trigger: {event.trigger}"],
                )

        except Exception as e:
            logger.error(f"Event-driven invalidation failed: {e}")
            return InvalidationResult(
                success=False,
                keys_invalidated=0,
                execution_time_ms=(time.time() - start_time) * 1000,
                strategy_used=self.get_strategy_name(),
                errors=[str(e)],
            )

    async def _handle_user_session_event(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle user login/logout events"""
        start_time = time.time()
        keys_invalidated = 0

        # Invalidate user session and context caches
        cache_keys = [
            CacheKeys.user_context(event.entity_id),
            CacheKeys.user_defaults(event.entity_id),
        ]

        # For logout, also invalidate client and engagement caches
        if event.trigger == InvalidationTrigger.USER_LOGOUT:
            cache_keys.extend(
                [
                    CacheKeys.user_clients(event.entity_id),
                ]
            )

            # If we have client context, invalidate engagement-specific caches
            if event.client_account_id:
                cache_keys.append(
                    CacheKeys.user_engagements(event.entity_id, event.client_account_id)
                )

        # Perform cascade invalidation
        keys_invalidated = await self._cascade_invalidate(cache_keys, event, depth=0)

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=keys_invalidated,
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={"trigger": event.trigger.value, "user_id": event.entity_id},
        )

    async def _handle_user_context_change(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle user context changes (client/engagement switches)"""
        start_time = time.time()

        # Invalidate all user context-related caches
        cache_keys = [
            CacheKeys.user_context(event.entity_id),
            CacheKeys.user_clients(event.entity_id),
        ]

        # If specific client context, invalidate engagement caches
        if event.client_account_id:
            cache_keys.append(
                CacheKeys.user_engagements(event.entity_id, event.client_account_id)
            )

        keys_invalidated = await self._cascade_invalidate(cache_keys, event, depth=0)

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=keys_invalidated,
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={"trigger": "user_context_change", "user_id": event.entity_id},
        )

    async def _handle_user_permission_change(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle user role/permission changes"""
        start_time = time.time()

        # Invalidate all user-related caches as permissions affect everything
        cache_keys = [
            CacheKeys.user_context(event.entity_id),
            CacheKeys.user_clients(event.entity_id),
            CacheKeys.user_defaults(event.entity_id),
        ]

        # Also invalidate admin caches that might be affected
        cache_keys.extend(
            [
                CacheKeys.admin_active_users(),
                CacheKeys.admin_pending_approvals(),
            ]
        )

        keys_invalidated = await self._cascade_invalidate(cache_keys, event, depth=0)

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=keys_invalidated,
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={"trigger": event.trigger.value, "user_id": event.entity_id},
        )

    async def _handle_client_access_change(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle client access changes"""
        start_time = time.time()

        # Invalidate client-related caches
        cache_keys = [
            CacheKeys.client_engagements(event.entity_id),
            CacheKeys.client_users(event.entity_id),
            CacheKeys.client_settings(event.entity_id),
        ]

        # If user context available, invalidate user's client list
        if event.user_id:
            cache_keys.append(CacheKeys.user_clients(event.user_id))

        keys_invalidated = await self._cascade_invalidate(cache_keys, event, depth=0)

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=keys_invalidated,
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={"trigger": "client_access_change", "client_id": event.entity_id},
        )

    async def _handle_engagement_change(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle engagement changes"""
        start_time = time.time()

        cache_keys = []

        # Invalidate client's engagement list
        if event.client_account_id:
            cache_keys.append(CacheKeys.client_engagements(event.client_account_id))

        # Invalidate engagement-specific flows
        if event.client_account_id and event.entity_id:
            cache_keys.append(
                CacheKeys.client_flows(event.client_account_id, event.entity_id)
            )

        keys_invalidated = await self._cascade_invalidate(cache_keys, event, depth=0)

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=keys_invalidated,
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={"trigger": "engagement_change", "engagement_id": event.entity_id},
        )

    async def _handle_data_modification(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle general data modification events"""
        start_time = time.time()

        # Generate cache keys based on entity type
        cache_keys = self._generate_data_modification_keys(event)

        keys_invalidated = await self._cascade_invalidate(cache_keys, event, depth=0)

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=keys_invalidated,
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={"trigger": "data_modification", "entity_type": event.entity_type},
        )

    async def _handle_security_event(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle security events - aggressive invalidation"""
        start_time = time.time()

        # For security events, invalidate broadly
        cache_keys = []

        if event.user_id:
            # Invalidate all user-related caches
            cache_keys.extend(
                [
                    CacheKeys.user_context(event.user_id),
                    CacheKeys.user_clients(event.user_id),
                    CacheKeys.user_defaults(event.user_id),
                ]
            )

        if event.client_account_id:
            # Invalidate client-specific caches
            cache_keys.extend(
                [
                    CacheKeys.client_users(event.client_account_id),
                    CacheKeys.client_settings(event.client_account_id),
                ]
            )

        # Always invalidate admin caches for security events
        cache_keys.extend(
            [
                CacheKeys.admin_active_users(),
                CacheKeys.admin_pending_approvals(),
            ]
        )

        keys_invalidated = await self._cascade_invalidate(cache_keys, event, depth=0)

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=keys_invalidated,
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={
                "trigger": "security_event",
                "severity": event.metadata.get("severity", "unknown"),
            },
        )

    async def _handle_manual_invalidation(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle manual invalidation requests"""
        start_time = time.time()

        # Use patterns or specific keys from metadata
        patterns = event.metadata.get("patterns", [])
        specific_keys = event.metadata.get("keys", [])

        keys_invalidated = 0

        # Handle specific keys
        for cache_key in specific_keys:
            try:
                deleted = await self.redis_cache.delete(cache_key)
                if deleted:
                    keys_invalidated += 1
            except Exception as e:
                logger.error(f"Failed to delete cache key {cache_key}: {e}")

        # Handle patterns (would need pattern matching support in Redis)
        for pattern in patterns:
            try:
                # This would require implementing pattern-based deletion
                pattern_keys = await self._find_keys_by_pattern(pattern)
                for key in pattern_keys:
                    deleted = await self.redis_cache.delete(key)
                    if deleted:
                        keys_invalidated += 1
            except Exception as e:
                logger.error(f"Failed to delete pattern {pattern}: {e}")

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=keys_invalidated,
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={
                "trigger": "manual_invalidation",
                "patterns": patterns,
                "keys": specific_keys,
            },
        )

    async def _cascade_invalidate(
        self, initial_keys: List[str], event: InvalidationEvent, depth: int
    ) -> int:
        """Perform cascade invalidation with cycle detection"""
        if depth >= self.max_cascade_depth:
            logger.warning(f"Max cascade depth reached: {depth}")
            return 0

        total_invalidated = 0

        # Delete initial keys
        for cache_key in initial_keys:
            try:
                deleted = await self.redis_cache.delete(cache_key)
                if deleted:
                    total_invalidated += 1
                    logger.debug(f"Invalidated cache key: {cache_key}")
            except Exception as e:
                logger.error(f"Failed to invalidate {cache_key}: {e}")

        # Find cascade relationships and continue invalidation
        cascade_keys = set()
        for cache_key in initial_keys:
            related_keys = self._get_cascade_keys(cache_key, event)
            cascade_keys.update(related_keys)

        # Remove already processed keys to avoid cycles
        cascade_keys -= set(initial_keys)

        if cascade_keys:
            cascade_invalidated = await self._cascade_invalidate(
                list(cascade_keys), event, depth + 1
            )
            total_invalidated += cascade_invalidated

        return total_invalidated

    def _get_cascade_keys(self, cache_key: str, event: InvalidationEvent) -> Set[str]:
        """Get keys that should be invalidated when the given key is invalidated"""
        cascade_keys = set()

        # Use the cascade relationships from CacheKeys
        patterns = CacheKeys.get_cascade_patterns(cache_key)

        # Convert patterns to actual keys where possible
        for pattern in patterns:
            # For now, add the pattern itself - would need pattern expansion
            cascade_keys.add(pattern)

        return cascade_keys

    def _generate_data_modification_keys(self, event: InvalidationEvent) -> List[str]:
        """Generate cache keys for data modification events"""
        cache_keys = []

        # Entity-specific cache keys
        if event.entity_type == "user":
            cache_keys.extend(
                [
                    CacheKeys.user_context(event.entity_id),
                    CacheKeys.user_clients(event.entity_id),
                ]
            )

        elif event.entity_type == "client":
            cache_keys.extend(
                [
                    CacheKeys.client_engagements(event.entity_id),
                    CacheKeys.client_users(event.entity_id),
                    CacheKeys.client_settings(event.entity_id),
                ]
            )

        elif event.entity_type == "engagement":
            if event.client_account_id:
                cache_keys.extend(
                    [
                        CacheKeys.client_engagements(event.client_account_id),
                        CacheKeys.client_flows(
                            event.client_account_id, event.entity_id
                        ),
                    ]
                )

        # Add analytics and admin caches that might be affected
        cache_keys.extend(
            [
                CacheKeys.admin_active_users(),
            ]
        )

        return cache_keys

    async def _find_keys_by_pattern(self, pattern: str) -> List[str]:
        """Find cache keys matching a pattern"""
        # This would require Redis SCAN functionality
        # For now, return empty list - would need to implement with RedisCache
        return []

    def _build_cascade_relationships(self) -> Dict[str, Set[str]]:
        """Build cache key cascade relationships"""
        # Use the relationships from CacheKeys constants
        from app.constants.cache_keys import CASCADE_RELATIONSHIPS

        return CASCADE_RELATIONSHIPS


class WritePatternInvalidationStrategy(BaseInvalidationStrategy):
    """
    Handles write-through and write-behind cache invalidation patterns:
    - Write-through: Critical data written to both cache and DB immediately
    - Write-behind: Non-critical data written to cache first, DB later
    - Selective patterns based on data criticality
    """

    def __init__(self, redis_cache: RedisCache):
        super().__init__(redis_cache)
        self.write_behind_queue: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        self.write_behind_batch_size = 50
        self.write_behind_flush_interval = 30  # seconds
        self.last_flush_time = time.time()

        # Define critical data types that use write-through
        self.critical_data_types = {
            "user_session",
            "user_context",
            "security_event",
            "authentication",
            "permission_change",
        }

    def can_handle(self, event: InvalidationEvent) -> bool:
        """Handle data modification events that need write pattern coordination"""
        return event.trigger == InvalidationTrigger.DATA_MODIFICATION

    async def invalidate(self, event: InvalidationEvent) -> InvalidationResult:
        """Execute write pattern invalidation"""
        start_time = time.time()

        try:
            data_type = event.metadata.get("data_type", "unknown")

            if self._is_critical_data(data_type):
                return await self._handle_write_through(event)
            else:
                return await self._handle_write_behind(event)

        except Exception as e:
            logger.error(f"Write pattern invalidation failed: {e}")
            return InvalidationResult(
                success=False,
                keys_invalidated=0,
                execution_time_ms=(time.time() - start_time) * 1000,
                strategy_used=self.get_strategy_name(),
                errors=[str(e)],
            )

    async def _handle_write_through(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle write-through pattern for critical data"""
        start_time = time.time()

        # For write-through, we immediately invalidate cache to ensure consistency
        cache_keys = self._generate_cache_keys_for_data_type(event)
        keys_invalidated = 0

        for cache_key in cache_keys:
            try:
                deleted = await self.redis_cache.delete(cache_key)
                if deleted:
                    keys_invalidated += 1
                    logger.debug(f"Write-through invalidated: {cache_key}")
            except Exception as e:
                logger.error(f"Failed write-through invalidation for {cache_key}: {e}")

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=keys_invalidated,
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={
                "pattern": "write_through",
                "data_type": event.metadata.get("data_type"),
            },
        )

    async def _handle_write_behind(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle write-behind pattern for non-critical data"""
        start_time = time.time()

        # For write-behind, we queue the invalidation for later processing
        cache_keys = self._generate_cache_keys_for_data_type(event)

        for cache_key in cache_keys:
            self.write_behind_queue[cache_key].append(
                {
                    "event": event.to_dict(),
                    "timestamp": datetime.utcnow().isoformat(),
                    "operation": "invalidate",
                }
            )

        # Check if we should flush the write-behind queue
        await self._maybe_flush_write_behind_queue()

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=len(cache_keys),  # Queued for invalidation
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={"pattern": "write_behind", "queued_keys": len(cache_keys)},
        )

    async def _maybe_flush_write_behind_queue(self) -> None:
        """Flush write-behind queue if conditions are met"""
        current_time = time.time()

        # Check if enough time has passed or queue is large enough
        should_flush = (
            current_time - self.last_flush_time > self.write_behind_flush_interval
            or sum(len(operations) for operations in self.write_behind_queue.values())
            >= self.write_behind_batch_size
        )

        if should_flush:
            await self._flush_write_behind_queue()

    async def _flush_write_behind_queue(self) -> None:
        """Flush all queued write-behind operations"""
        if not self.write_behind_queue:
            return

        logger.info(
            f"Flushing write-behind queue with {len(self.write_behind_queue)} keys"
        )

        flushed_keys = 0

        for cache_key, operations in list(self.write_behind_queue.items()):
            try:
                # Process all operations for this key
                for operation in operations:
                    if operation["operation"] == "invalidate":
                        deleted = await self.redis_cache.delete(cache_key)
                        if deleted:
                            flushed_keys += 1

                # Clear processed operations
                del self.write_behind_queue[cache_key]

            except Exception as e:
                logger.error(
                    f"Failed to flush write-behind operations for {cache_key}: {e}"
                )

        self.last_flush_time = time.time()
        logger.info(f"Flushed {flushed_keys} keys from write-behind queue")

    def _is_critical_data(self, data_type: str) -> bool:
        """Check if data type is critical and requires write-through"""
        return data_type.lower() in self.critical_data_types

    def _generate_cache_keys_for_data_type(self, event: InvalidationEvent) -> List[str]:
        """Generate cache keys based on data type and event"""
        cache_keys = []
        data_type = event.metadata.get("data_type", "")

        if data_type == "user_session":
            cache_keys.append(CacheKeys.user_context(event.entity_id))

        elif data_type == "user_context":
            cache_keys.extend(
                [
                    CacheKeys.user_context(event.entity_id),
                    CacheKeys.user_clients(event.entity_id),
                ]
            )

        elif data_type == "client_data":
            cache_keys.extend(
                [
                    CacheKeys.client_engagements(event.entity_id),
                    CacheKeys.client_users(event.entity_id),
                    CacheKeys.client_settings(event.entity_id),
                ]
            )

        elif data_type == "engagement_data":
            if event.client_account_id:
                cache_keys.extend(
                    [
                        CacheKeys.client_engagements(event.client_account_id),
                        CacheKeys.client_flows(
                            event.client_account_id, event.entity_id
                        ),
                    ]
                )

        return cache_keys

    async def force_flush_write_behind_queue(self) -> int:
        """Force flush write-behind queue (for shutdown/testing)"""
        await self._flush_write_behind_queue()
        return 0  # Return number of keys flushed


class CacheVersioningInvalidationStrategy(BaseInvalidationStrategy):
    """
    Handles cache invalidation with versioning and data integrity:
    - Version-based invalidation to ensure consistency
    - Checksum validation for data integrity
    - Conflict resolution for simultaneous updates
    - Data repair for inconsistent cache data
    """

    def __init__(self, redis_cache: RedisCache):
        super().__init__(redis_cache)
        self.version_cache: Dict[str, str] = {}
        self.checksum_cache: Dict[str, str] = {}

    def can_handle(self, event: InvalidationEvent) -> bool:
        """Handle data modification events that need versioning"""
        return (
            event.trigger == InvalidationTrigger.DATA_MODIFICATION
            and event.metadata.get("use_versioning", False)
        )

    async def invalidate(self, event: InvalidationEvent) -> InvalidationResult:
        """Execute version-based invalidation"""
        start_time = time.time()

        try:
            cache_keys = self._generate_cache_keys_for_versioning(event)
            keys_invalidated = 0

            for cache_key in cache_keys:
                # Check version before invalidation
                current_version = await self._get_cache_version(cache_key)
                expected_version = event.metadata.get("expected_version")

                if expected_version and current_version != expected_version:
                    logger.warning(
                        f"Version mismatch for {cache_key}: expected {expected_version}, got {current_version}"
                    )
                    continue

                # Invalidate with version increment
                deleted = await self._invalidate_with_version_increment(cache_key)
                if deleted:
                    keys_invalidated += 1

            execution_time = (time.time() - start_time) * 1000

            return InvalidationResult(
                success=True,
                keys_invalidated=keys_invalidated,
                execution_time_ms=execution_time,
                strategy_used=self.get_strategy_name(),
                metadata={"versioning": True, "keys_processed": len(cache_keys)},
            )

        except Exception as e:
            logger.error(f"Version-based invalidation failed: {e}")
            return InvalidationResult(
                success=False,
                keys_invalidated=0,
                execution_time_ms=(time.time() - start_time) * 1000,
                strategy_used=self.get_strategy_name(),
                errors=[str(e)],
            )

    async def _get_cache_version(self, cache_key: str) -> Optional[str]:
        """Get current version of cache key"""
        version_key = f"{cache_key}:version"
        return await self.redis_cache.get(version_key)

    async def _invalidate_with_version_increment(self, cache_key: str) -> bool:
        """Invalidate cache key and increment version"""
        try:
            # Delete the actual cache data
            deleted = await self.redis_cache.delete(cache_key)

            # Increment version
            version_key = f"{cache_key}:version"
            current_version = await self.redis_cache.get(version_key) or "0"
            new_version = str(int(current_version) + 1)
            await self.redis_cache.set(version_key, new_version, ttl=86400)  # 24 hours

            return deleted

        except Exception as e:
            logger.error(
                f"Failed to invalidate with version increment for {cache_key}: {e}"
            )
            return False

    def _generate_cache_keys_for_versioning(
        self, event: InvalidationEvent
    ) -> List[str]:
        """Generate cache keys that need versioning"""
        # Use similar logic to other strategies but for version-critical data
        cache_keys = []

        if event.entity_type == "user":
            cache_keys.extend(
                [
                    CacheKeys.user_context(event.entity_id),
                    CacheKeys.user_defaults(event.entity_id),
                ]
            )

        elif event.entity_type == "client":
            cache_keys.extend(
                [
                    CacheKeys.client_settings(event.entity_id),
                ]
            )

        return cache_keys


# Factory function to create appropriate strategy based on event
def create_invalidation_strategy(
    event: InvalidationEvent, redis_cache: RedisCache
) -> Optional[BaseInvalidationStrategy]:
    """Create the appropriate invalidation strategy for an event"""

    strategies = [
        TimeBasedInvalidationStrategy(redis_cache),
        EventDrivenInvalidationStrategy(redis_cache),
        WritePatternInvalidationStrategy(redis_cache),
        CacheVersioningInvalidationStrategy(redis_cache),
    ]

    for strategy in strategies:
        if strategy.can_handle(event):
            return strategy

    return None


# Export all classes
__all__ = [
    "InvalidationTrigger",
    "InvalidationPriority",
    "InvalidationEvent",
    "InvalidationResult",
    "BaseInvalidationStrategy",
    "TimeBasedInvalidationStrategy",
    "EventDrivenInvalidationStrategy",
    "WritePatternInvalidationStrategy",
    "CacheVersioningInvalidationStrategy",
    "create_invalidation_strategy",
]
