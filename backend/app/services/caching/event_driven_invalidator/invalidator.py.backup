"""
Main EventDrivenInvalidator class for cache invalidation processing

This module contains the core EventDrivenInvalidator class that manages
event-driven cache invalidation with SQLAlchemy ORM integration.

Generated by CC (Claude Code)
"""

import asyncio
import json
import time
from collections import deque
from typing import Any, Callable, Dict, List, Optional, Type, TYPE_CHECKING

from sqlalchemy import event, inspect

from app.core.config import settings
from app.core.logging import get_logger
from app.models.base import Base
from app.services.caching.invalidation_strategies import (
    InvalidationTrigger,
)

if TYPE_CHECKING:
    from app.services.caching.cache_invalidation_manager import CacheInvalidationManager

from .base import EventProcessingStats, EventSource
from .detectors import EntityChangeDetector
from .events import CacheInvalidationEvent

logger = get_logger(__name__)


class EventDrivenInvalidator:
    """
    Event-driven cache invalidator that listens for various events and
    triggers appropriate cache invalidation strategies.

    Features:
    - SQLAlchemy ORM event listeners for database changes
    - User action event processing
    - Bulk operation handling with batching
    - Real-time event processing with minimal latency
    - Comprehensive event tracking and metrics
    """

    def __init__(self, invalidation_manager: "CacheInvalidationManager"):
        self.invalidation_manager = invalidation_manager
        self.stats = EventProcessingStats()

        # Event queuing and batching
        self.event_queue: deque = deque()
        self.batch_size = settings.get("CACHE_INVALIDATION_BATCH_SIZE", 50)
        self.batch_timeout_ms = settings.get("CACHE_INVALIDATION_BATCH_TIMEOUT_MS", 100)
        self.last_batch_time = time.time()

        # Event filtering and deduplication
        self.event_filters: List[Callable[[CacheInvalidationEvent], bool]] = []
        self.recent_events: deque = deque(
            maxlen=1000
        )  # Keep last 1000 events for deduplication
        self.deduplication_window_ms = 5000  # 5 second deduplication window

        # SQLAlchemy event listeners
        self._orm_listeners_registered = False
        self._entity_change_detector = EntityChangeDetector()

        # Background processing
        self._processing_task: Optional[asyncio.Task] = None
        self._shutdown_event = asyncio.Event()

        logger.info("EventDrivenInvalidator initialized")

    def start(self) -> None:
        """Start the event-driven invalidator"""
        if not self._orm_listeners_registered:
            self._register_orm_listeners()

        if self._processing_task is None:
            self._shutdown_event.clear()
            self._processing_task = asyncio.create_task(self._background_processing())

        logger.info("EventDrivenInvalidator started")

    async def stop(self) -> None:
        """Stop the event-driven invalidator"""
        logger.info("Stopping EventDrivenInvalidator...")

        if self._processing_task:
            self._shutdown_event.set()
            try:
                await asyncio.wait_for(self._processing_task, timeout=10.0)
            except asyncio.TimeoutError:
                logger.warning("EventDrivenInvalidator shutdown timeout")
                self._processing_task.cancel()
            self._processing_task = None

        self._unregister_orm_listeners()
        logger.info("EventDrivenInvalidator stopped")

    def add_event_filter(
        self, filter_func: Callable[[CacheInvalidationEvent], bool]
    ) -> None:
        """Add event filter function"""
        self.event_filters.append(filter_func)

    def emit_user_login_event(
        self,
        user_id: str,
        client_account_id: str,
        session_id: Optional[str] = None,
        endpoint: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Emit user login event"""
        event = CacheInvalidationEvent(
            source=EventSource.USER_ACTION,
            trigger=InvalidationTrigger.USER_LOGIN,
            entity_type="user",
            entity_id=user_id,
            client_account_id=client_account_id,
            user_id=user_id,
            session_id=session_id,
            endpoint=endpoint,
            operation="LOGIN",
            metadata=metadata or {},
        )
        self._queue_event(event)

    def emit_user_logout_event(
        self,
        user_id: str,
        client_account_id: str,
        session_id: Optional[str] = None,
        endpoint: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Emit user logout event"""
        event = CacheInvalidationEvent(
            source=EventSource.USER_ACTION,
            trigger=InvalidationTrigger.USER_LOGOUT,
            entity_type="user",
            entity_id=user_id,
            client_account_id=client_account_id,
            user_id=user_id,
            session_id=session_id,
            endpoint=endpoint,
            operation="LOGOUT",
            metadata=metadata or {},
        )
        self._queue_event(event)

    def emit_user_context_change_event(
        self,
        user_id: str,
        client_account_id: str,
        old_context: Optional[Dict[str, Any]] = None,
        new_context: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None,
        endpoint: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Emit user context change event"""
        event = CacheInvalidationEvent(
            source=EventSource.USER_ACTION,
            trigger=InvalidationTrigger.USER_CONTEXT_CHANGE,
            entity_type="user",
            entity_id=user_id,
            client_account_id=client_account_id,
            user_id=user_id,
            session_id=session_id,
            endpoint=endpoint,
            operation="CONTEXT_CHANGE",
            old_values=old_context,
            new_values=new_context,
            metadata=metadata or {},
        )
        self._queue_event(event)

    def emit_user_role_change_event(
        self,
        user_id: str,
        client_account_id: str,
        old_role: Optional[str] = None,
        new_role: Optional[str] = None,
        session_id: Optional[str] = None,
        endpoint: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Emit user role change event"""
        event = CacheInvalidationEvent(
            source=EventSource.USER_ACTION,
            trigger=InvalidationTrigger.USER_ROLE_CHANGE,
            entity_type="user",
            entity_id=user_id,
            client_account_id=client_account_id,
            user_id=user_id,
            session_id=session_id,
            endpoint=endpoint,
            operation="ROLE_CHANGE",
            old_values={"role": old_role} if old_role else None,
            new_values={"role": new_role} if new_role else None,
            metadata=metadata or {},
        )
        self._queue_event(event)

    def emit_security_event(
        self,
        event_type: str,
        user_id: str,
        client_account_id: str,
        severity: str = "high",
        session_id: Optional[str] = None,
        endpoint: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Emit security event"""
        event_metadata = metadata or {}
        event_metadata.update(
            {
                "event_type": event_type,
                "severity": severity,
            }
        )

        event = CacheInvalidationEvent(
            source=EventSource.SECURITY_EVENT,
            trigger=InvalidationTrigger.SECURITY_VIOLATION,
            entity_type="user",
            entity_id=user_id,
            client_account_id=client_account_id,
            user_id=user_id,
            session_id=session_id,
            endpoint=endpoint,
            operation="SECURITY_EVENT",
            metadata=event_metadata,
        )
        self._queue_event(event)

    def emit_bulk_data_change_event(
        self,
        entity_type: str,
        entity_ids: List[str],
        client_account_id: str,
        operation: str = "BULK_UPDATE",
        user_id: Optional[str] = None,
        endpoint: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Emit bulk data change event"""
        event_metadata = metadata or {}
        event_metadata.update(
            {
                "bulk_operation": True,
                "entity_count": len(entity_ids),
                "all_entity_ids": entity_ids,
            }
        )

        # Create events for each entity but mark them as part of bulk operation
        for entity_id in entity_ids:
            event = CacheInvalidationEvent(
                source=EventSource.BULK_OPERATION,
                trigger=InvalidationTrigger.DATA_MODIFICATION,
                entity_type=entity_type,
                entity_id=entity_id,
                client_account_id=client_account_id,
                user_id=user_id,
                endpoint=endpoint,
                operation=operation,
                metadata=event_metadata,
            )
            self._queue_event(event)

    def get_stats(self) -> Dict[str, Any]:
        """Get event processing statistics"""
        return {
            "total_events": self.stats.total_events,
            "failed_events": self.stats.failed_events,
            "success_rate": (
                (self.stats.total_events - self.stats.failed_events)
                / max(self.stats.total_events, 1)
                * 100
            ),
            "average_processing_time_ms": self.stats.average_processing_time_ms,
            "events_by_source": {
                source.value: count
                for source, count in self.stats.events_by_source.items()
            },
            "events_by_trigger": {
                trigger.value: count
                for trigger, count in self.stats.events_by_trigger.items()
            },
            "events_by_entity_type": dict(self.stats.events_by_entity_type),
            "queue_size": len(self.event_queue),
            "last_reset": self.stats.last_reset.isoformat(),
        }

    def reset_stats(self) -> None:
        """Reset event processing statistics"""
        self.stats = EventProcessingStats()
        logger.info("Event processing statistics reset")

    # Private methods

    def _queue_event(self, event: CacheInvalidationEvent) -> None:
        """Queue event for processing with deduplication"""
        # Check if this is a duplicate recent event
        if self._is_duplicate_event(event):
            logger.debug(
                f"Skipping duplicate event: {event.entity_type}:{event.entity_id}"
            )
            return

        # Apply filters
        for filter_func in self.event_filters:
            try:
                if not filter_func(event):
                    logger.debug(
                        f"Event filtered out: {event.entity_type}:{event.entity_id}"
                    )
                    return
            except Exception as e:
                logger.error(f"Event filter error: {e}")

        # Add to queue
        self.event_queue.append(event)
        self.recent_events.append(event)

        logger.debug(
            f"Queued event: {event.trigger.value} for {event.entity_type}:{event.entity_id}"
        )

    def _is_duplicate_event(self, event: CacheInvalidationEvent) -> bool:
        """Check if event is a duplicate of a recent event"""
        from datetime import timedelta

        cutoff_time = event.timestamp - timedelta(
            milliseconds=self.deduplication_window_ms
        )

        for recent_event in reversed(self.recent_events):  # Check most recent first
            if recent_event.timestamp < cutoff_time:
                break  # No need to check older events

            if (
                recent_event.trigger == event.trigger
                and recent_event.entity_type == event.entity_type
                and recent_event.entity_id == event.entity_id
                and recent_event.client_account_id == event.client_account_id
                and recent_event.operation == event.operation
            ):
                return True

        return False

    async def _background_processing(self) -> None:
        """Background task to process queued events"""
        logger.info("Starting event processing background task")

        while not self._shutdown_event.is_set():
            try:
                # Process events in batches
                await self._process_event_batch()

                # Small delay to prevent tight loop
                await asyncio.sleep(0.01)  # 10ms

            except Exception as e:
                logger.error(f"Error in event processing: {e}")
                await asyncio.sleep(0.1)  # Longer delay on error

        # Process remaining events before shutdown
        await self._drain_remaining_events()
        logger.info("Event processing background task stopped")

    async def _process_event_batch(self) -> None:
        """Process a batch of events"""
        if not self.event_queue:
            return

        # Check if we should process a batch
        current_time = time.time()
        should_process_batch = (
            len(self.event_queue) >= self.batch_size
            or (current_time - self.last_batch_time) * 1000 >= self.batch_timeout_ms
        )

        if not should_process_batch:
            return

        # Collect batch
        batch = []
        batch_size = min(self.batch_size, len(self.event_queue))

        for _ in range(batch_size):
            if self.event_queue:
                batch.append(self.event_queue.popleft())

        if not batch:
            return

        # Process batch
        start_time = time.time()

        for cache_event in batch:
            await self._process_single_event(cache_event)

        processing_time = (time.time() - start_time) * 1000
        self.last_batch_time = current_time

        logger.debug(
            f"Processed batch of {len(batch)} events in {processing_time:.2f}ms"
        )

    async def _process_single_event(self, event: CacheInvalidationEvent) -> None:
        """Process a single cache invalidation event"""
        start_time = time.time()
        success = True

        try:
            # Convert to InvalidationEvent and submit to manager
            invalidation_event = event.to_invalidation_event()

            # Submit to invalidation manager (non-blocking)
            task_id = await self.invalidation_manager.invalidate(
                trigger=invalidation_event.trigger,
                entity_type=invalidation_event.entity_type,
                entity_id=invalidation_event.entity_id,
                client_account_id=invalidation_event.client_account_id,
                user_id=invalidation_event.user_id,
                engagement_id=invalidation_event.engagement_id,
                priority=invalidation_event.priority,
                metadata=invalidation_event.metadata,
                wait_for_completion=False,  # Non-blocking
            )

            logger.debug(
                f"Submitted invalidation task {task_id} for event {event.entity_type}:{event.entity_id}"
            )

        except Exception as e:
            logger.error(
                f"Failed to process event {event.entity_type}:{event.entity_id}: {e}"
            )
            success = False

        # Update statistics
        processing_time_ms = (time.time() - start_time) * 1000
        self.stats.update(event, processing_time_ms, success)

    async def _drain_remaining_events(self) -> None:
        """Process remaining events during shutdown"""
        logger.info(f"Draining {len(self.event_queue)} remaining events...")

        while self.event_queue:
            event = self.event_queue.popleft()
            await self._process_single_event(event)

        logger.info("Finished draining remaining events")

    def _register_orm_listeners(self) -> None:
        """Register SQLAlchemy ORM event listeners"""
        if self._orm_listeners_registered:
            return

        # Register listeners for all model classes
        for model_class in self._get_model_classes():
            event.listen(model_class, "after_insert", self._on_orm_insert)
            event.listen(model_class, "after_update", self._on_orm_update)
            event.listen(model_class, "after_delete", self._on_orm_delete)

        self._orm_listeners_registered = True
        logger.info("SQLAlchemy ORM event listeners registered")

    def _unregister_orm_listeners(self) -> None:
        """Unregister SQLAlchemy ORM event listeners"""
        if not self._orm_listeners_registered:
            return

        # Unregister listeners for all model classes
        for model_class in self._get_model_classes():
            event.remove(model_class, "after_insert", self._on_orm_insert)
            event.remove(model_class, "after_update", self._on_orm_update)
            event.remove(model_class, "after_delete", self._on_orm_delete)

        self._orm_listeners_registered = False
        logger.info("SQLAlchemy ORM event listeners unregistered")

    def _get_model_classes(self) -> List[Type]:
        """Get all SQLAlchemy model classes"""
        # Return all classes that inherit from Base
        model_classes = []
        for cls in Base.__subclasses__():
            if hasattr(cls, "__tablename__"):
                model_classes.append(cls)
        return model_classes

    def _on_orm_insert(self, mapper, connection, target) -> None:
        """Handle ORM insert events"""
        try:
            entity_info = self._extract_entity_info(target)
            if not entity_info:
                return

            event = CacheInvalidationEvent(
                source=EventSource.DATABASE_ORM,
                trigger=InvalidationTrigger.DATA_MODIFICATION,
                entity_type=entity_info["entity_type"],
                entity_id=entity_info["entity_id"],
                client_account_id=entity_info["client_account_id"],
                user_id=entity_info.get("user_id"),
                engagement_id=entity_info.get("engagement_id"),
                operation="INSERT",
                new_values=self._extract_entity_values(target),
            )

            self._queue_event(event)

        except Exception as e:
            logger.error(f"Error handling ORM insert: {e}")

    def _on_orm_update(self, mapper, connection, target) -> None:
        """Handle ORM update events"""
        try:
            entity_info = self._extract_entity_info(target)
            if not entity_info:
                return

            # Get old and new values
            old_values = {}
            new_values = self._extract_entity_values(target)

            # Get changed attributes from SQLAlchemy inspection
            changed_attributes = self._entity_change_detector.get_changed_attributes(
                target
            )

            # Check if changes are cache-relevant
            if not self._entity_change_detector.has_cache_relevant_changes(
                target, changed_attributes
            ):
                return

            event = CacheInvalidationEvent(
                source=EventSource.DATABASE_ORM,
                trigger=InvalidationTrigger.DATA_MODIFICATION,
                entity_type=entity_info["entity_type"],
                entity_id=entity_info["entity_id"],
                client_account_id=entity_info["client_account_id"],
                user_id=entity_info.get("user_id"),
                engagement_id=entity_info.get("engagement_id"),
                operation="UPDATE",
                old_values=old_values,
                new_values=new_values,
                changed_attributes=changed_attributes,
            )

            self._queue_event(event)

        except Exception as e:
            logger.error(f"Error handling ORM update: {e}")

    def _on_orm_delete(self, mapper, connection, target) -> None:
        """Handle ORM delete events"""
        try:
            entity_info = self._extract_entity_info(target)
            if not entity_info:
                return

            event = CacheInvalidationEvent(
                source=EventSource.DATABASE_ORM,
                trigger=InvalidationTrigger.DATA_MODIFICATION,
                entity_type=entity_info["entity_type"],
                entity_id=entity_info["entity_id"],
                client_account_id=entity_info["client_account_id"],
                user_id=entity_info.get("user_id"),
                engagement_id=entity_info.get("engagement_id"),
                operation="DELETE",
                old_values=self._extract_entity_values(target),
            )

            self._queue_event(event)

        except Exception as e:
            logger.error(f"Error handling ORM delete: {e}")

    def _extract_entity_info(self, entity: Any) -> Optional[Dict[str, Any]]:
        """Extract entity information for cache invalidation"""
        entity_type = entity.__class__.__name__.lower()

        # Extract entity ID
        entity_id = None
        if hasattr(entity, "id"):
            entity_id = str(entity.id)
        elif hasattr(entity, "uuid"):
            entity_id = str(entity.uuid)
        else:
            return None

        # Extract client account ID for multi-tenant isolation
        client_account_id = None
        if hasattr(entity, "client_account_id"):
            client_account_id = str(entity.client_account_id)
        elif hasattr(entity, "client_id"):
            client_account_id = str(entity.client_id)
        else:
            # Some entities might not have direct client association
            # For now, we'll skip them, but could be enhanced to look up relationships
            return None

        result = {
            "entity_type": entity_type,
            "entity_id": entity_id,
            "client_account_id": client_account_id,
        }

        # Extract optional fields
        if hasattr(entity, "user_id"):
            result["user_id"] = str(entity.user_id)
        if hasattr(entity, "engagement_id"):
            result["engagement_id"] = str(entity.engagement_id)

        return result

    def _extract_entity_values(self, entity: Any) -> Dict[str, Any]:
        """Extract entity attribute values for change tracking"""
        values = {}

        # Get all column attributes
        if hasattr(entity, "__table__"):
            for column in entity.__table__.columns:
                attr_name = column.name
                if hasattr(entity, attr_name):
                    attr_value = getattr(entity, attr_name)
                    # Convert to serializable format
                    if attr_value is not None:
                        try:
                            json.dumps(attr_value, default=str)  # Test serializability
                            values[attr_name] = attr_value
                        except (TypeError, ValueError):
                            values[attr_name] = str(attr_value)

        return values
