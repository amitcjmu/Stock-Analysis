"""
Cache Consistency Checker for Auth Performance Optimization

This module implements comprehensive cache consistency verification and repair including:
- Cache-database consistency verification across all data types
- Automatic detection and correction of inconsistent cache data
- Performance impact monitoring during consistency checks
- Scheduled consistency audits with configurable intervals
- Data repair mechanisms for corrupted or stale cache entries

The consistency checker ensures data integrity across the auth performance
system while minimizing impact on system performance and user experience.

ðŸ”’ Security: Multi-tenant isolation and secure consistency verification
âš¡ Performance: Optimized consistency checks with minimal database queries
ðŸŽ¯ Coherence: Comprehensive data integrity verification and repair
ðŸ“Š Analytics: Detailed consistency metrics and audit trails

Generated by CC (Claude Code)
"""

import asyncio
import hashlib
import json
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from sqlalchemy import and_, select
from sqlalchemy.ext.asyncio import AsyncSession

from app.constants.cache_keys import CACHE_VERSION, CacheKeys, CacheKeyType
from app.core.config import settings
from app.core.database import get_db
from app.core.logging import get_logger
from app.models.cache_metadata import CacheMetadata
from app.services.caching.redis_cache import RedisCache

logger = get_logger(__name__)


class ConsistencyCheckType(Enum):
    """Types of consistency checks"""

    BASIC = "basic"  # Basic existence and TTL checks
    DEEP = "deep"  # Full data comparison with database
    CHECKSUM = "checksum"  # Checksum-based integrity verification
    SCHEMA_VALIDATION = "schema"  # Data schema and format validation
    PERFORMANCE_IMPACT = "performance"  # Performance impact assessment


class ConsistencyStatus(Enum):
    """Cache consistency status"""

    CONSISTENT = "consistent"  # Cache and database are in sync
    INCONSISTENT = "inconsistent"  # Data mismatch detected
    STALE = "stale"  # Cache data is outdated
    CORRUPTED = "corrupted"  # Cache data is corrupted
    MISSING = "missing"  # Expected cache entry is missing
    ORPHANED = "orphaned"  # Cache entry has no database counterpart


class RepairAction(Enum):
    """Available repair actions"""

    REFRESH_FROM_DB = "refresh_from_db"  # Update cache from database
    INVALIDATE_CACHE = "invalidate_cache"  # Remove inconsistent cache entry
    UPDATE_DATABASE = "update_database"  # Update database from cache (rare)
    DELETE_ORPHANED = "delete_orphaned"  # Remove orphaned cache entries
    QUARANTINE = "quarantine"  # Move suspicious data to quarantine


@dataclass
class ConsistencyCheckResult:
    """Result of a consistency check operation"""

    cache_key: str
    status: ConsistencyStatus
    check_type: ConsistencyCheckType
    details: str
    database_value: Optional[Any] = None
    cache_value: Optional[Any] = None
    checksum_mismatch: bool = False
    data_diff: Optional[Dict[str, Any]] = None
    suggested_repair: Optional[RepairAction] = None
    confidence: float = 1.0  # 0.0 to 1.0
    execution_time_ms: float = 0
    timestamp: datetime = field(default_factory=datetime.utcnow)

    @property
    def needs_repair(self) -> bool:
        """Check if this result indicates repair is needed"""
        return self.status != ConsistencyStatus.CONSISTENT

    @property
    def is_critical(self) -> bool:
        """Check if this inconsistency is critical"""
        return self.status in [
            ConsistencyStatus.CORRUPTED,
            ConsistencyStatus.INCONSISTENT,
        ]


@dataclass
class ConsistencyAuditReport:
    """Comprehensive consistency audit report"""

    audit_id: str
    start_time: datetime
    end_time: Optional[datetime] = None
    total_keys_checked: int = 0
    consistent_keys: int = 0
    inconsistent_keys: int = 0
    corrupted_keys: int = 0
    missing_keys: int = 0
    orphaned_keys: int = 0
    repairs_attempted: int = 0
    repairs_successful: int = 0
    repairs_failed: int = 0
    total_execution_time_ms: float = 0
    performance_impact: Dict[str, float] = field(default_factory=dict)
    detailed_results: List[ConsistencyCheckResult] = field(default_factory=list)

    @property
    def consistency_score(self) -> float:
        """Calculate overall consistency score (0.0 to 1.0)"""
        if self.total_keys_checked == 0:
            return 1.0
        return self.consistent_keys / self.total_keys_checked

    @property
    def repair_success_rate(self) -> float:
        """Calculate repair success rate"""
        if self.repairs_attempted == 0:
            return 1.0
        return self.repairs_successful / self.repairs_attempted

    @property
    def duration_seconds(self) -> float:
        """Get audit duration in seconds"""
        if not self.end_time:
            return 0.0
        return (self.end_time - self.start_time).total_seconds()


@dataclass
class ConsistencyMetrics:
    """Consistency checking metrics"""

    total_checks: int = 0
    checks_by_type: Dict[ConsistencyCheckType, int] = field(
        default_factory=lambda: defaultdict(int)
    )
    status_distribution: Dict[ConsistencyStatus, int] = field(
        default_factory=lambda: defaultdict(int)
    )
    total_execution_time_ms: float = 0
    average_execution_time_ms: float = 0
    repairs_attempted: int = 0
    repairs_successful: int = 0
    last_audit_time: Optional[datetime] = None
    last_reset: datetime = field(default_factory=datetime.utcnow)

    def update_check(self, result: ConsistencyCheckResult) -> None:
        """Update metrics with check result"""
        self.total_checks += 1
        self.checks_by_type[result.check_type] += 1
        self.status_distribution[result.status] += 1
        self.total_execution_time_ms += result.execution_time_ms

        if self.total_checks > 0:
            self.average_execution_time_ms = (
                self.total_execution_time_ms / self.total_checks
            )

    def update_repair(self, success: bool) -> None:
        """Update repair metrics"""
        self.repairs_attempted += 1
        if success:
            self.repairs_successful += 1


class CacheConsistencyChecker:
    """
    Comprehensive cache consistency checker that verifies and repairs
    cache-database consistency across the auth performance system.

    Features:
    - Multiple consistency check types (basic, deep, checksum, schema)
    - Automatic inconsistency detection and repair
    - Scheduled consistency audits
    - Performance impact monitoring
    - Comprehensive metrics and reporting
    - Multi-tenant isolation and security
    """

    def __init__(self, redis_cache: RedisCache, db_session: AsyncSession):
        self.redis_cache = redis_cache
        self.db_session = db_session
        self.metrics = ConsistencyMetrics()

        # Configuration
        self.batch_size = settings.get("CACHE_CONSISTENCY_BATCH_SIZE", 100)
        self.max_concurrent_checks = settings.get("CACHE_CONSISTENCY_MAX_CONCURRENT", 5)
        self.default_check_interval_hours = settings.get(
            "CACHE_CONSISTENCY_CHECK_INTERVAL", 24
        )
        self.performance_impact_threshold = 0.1  # 10% performance impact threshold

        # Consistency check results history
        self.recent_results: deque = deque(maxlen=1000)
        self.quarantine_storage: Dict[str, Any] = {}

        # Data extractors for different entity types
        self.data_extractors = {
            "user": self._extract_user_data,
            "client": self._extract_client_data,
            "engagement": self._extract_engagement_data,
            "flow": self._extract_flow_data,
        }

        # Repair handlers for different inconsistency types
        self.repair_handlers = {
            RepairAction.REFRESH_FROM_DB: self._repair_refresh_from_db,
            RepairAction.INVALIDATE_CACHE: self._repair_invalidate_cache,
            RepairAction.DELETE_ORPHANED: self._repair_delete_orphaned,
            RepairAction.QUARANTINE: self._repair_quarantine,
        }

        # Background processing
        self._audit_task: Optional[asyncio.Task] = None
        self._shutdown_event = asyncio.Event()

        logger.info("CacheConsistencyChecker initialized")

    async def start_scheduled_audits(self) -> None:
        """Start scheduled consistency audits"""
        if self._audit_task is not None:
            logger.warning("Scheduled audits already running")
            return

        self._shutdown_event.clear()
        self._audit_task = asyncio.create_task(self._scheduled_audit_loop())
        logger.info("Scheduled consistency audits started")

    async def stop_scheduled_audits(self) -> None:
        """Stop scheduled consistency audits"""
        if self._audit_task is None:
            return

        logger.info("Stopping scheduled consistency audits...")
        self._shutdown_event.set()

        try:
            await asyncio.wait_for(self._audit_task, timeout=30.0)
        except asyncio.TimeoutError:
            logger.warning("Audit task shutdown timeout")
            self._audit_task.cancel()

        self._audit_task = None
        logger.info("Scheduled consistency audits stopped")

    async def check_cache_consistency(
        self,
        cache_key: str,
        check_type: ConsistencyCheckType = ConsistencyCheckType.BASIC,
        client_account_id: Optional[str] = None,
    ) -> ConsistencyCheckResult:
        """
        Check consistency of a single cache key.

        Args:
            cache_key: Cache key to check
            check_type: Type of consistency check to perform
            client_account_id: Client account for multi-tenant isolation

        Returns:
            ConsistencyCheckResult with details
        """
        start_time = time.time()

        try:
            if check_type == ConsistencyCheckType.BASIC:
                result = await self._check_basic_consistency(
                    cache_key, client_account_id
                )
            elif check_type == ConsistencyCheckType.DEEP:
                result = await self._check_deep_consistency(
                    cache_key, client_account_id
                )
            elif check_type == ConsistencyCheckType.CHECKSUM:
                result = await self._check_checksum_consistency(
                    cache_key, client_account_id
                )
            elif check_type == ConsistencyCheckType.SCHEMA_VALIDATION:
                result = await self._check_schema_consistency(
                    cache_key, client_account_id
                )
            else:
                result = await self._check_basic_consistency(
                    cache_key, client_account_id
                )

            result.execution_time_ms = (time.time() - start_time) * 1000

            # Update metrics
            self.metrics.update_check(result)
            self.recent_results.append(result)

            return result

        except Exception as e:
            logger.error(f"Consistency check failed for {cache_key}: {e}")
            return ConsistencyCheckResult(
                cache_key=cache_key,
                status=ConsistencyStatus.CORRUPTED,
                check_type=check_type,
                details=f"Check failed: {e}",
                execution_time_ms=(time.time() - start_time) * 1000,
            )

    async def check_bulk_consistency(
        self,
        cache_keys: List[str],
        check_type: ConsistencyCheckType = ConsistencyCheckType.BASIC,
        client_account_id: Optional[str] = None,
    ) -> List[ConsistencyCheckResult]:
        """Check consistency of multiple cache keys in batches"""
        results = []

        # Process in batches to avoid overwhelming the system
        for i in range(0, len(cache_keys), self.batch_size):
            batch = cache_keys[i : i + self.batch_size]

            # Process batch concurrently but with limited concurrency
            semaphore = asyncio.Semaphore(self.max_concurrent_checks)

            async def check_with_semaphore(key: str) -> ConsistencyCheckResult:
                async with semaphore:
                    return await self.check_cache_consistency(
                        key, check_type, client_account_id
                    )

            batch_tasks = [check_with_semaphore(key) for key in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            # Handle results and exceptions
            for key, result in zip(batch, batch_results):
                if isinstance(result, Exception):
                    logger.error(f"Consistency check failed for {key}: {result}")
                    results.append(
                        ConsistencyCheckResult(
                            cache_key=key,
                            status=ConsistencyStatus.CORRUPTED,
                            check_type=check_type,
                            details=f"Check failed: {result}",
                        )
                    )
                else:
                    results.append(result)

            # Small delay between batches to reduce system load
            if i + self.batch_size < len(cache_keys):
                await asyncio.sleep(0.1)

        return results

    async def run_full_consistency_audit(
        self,
        client_account_id: Optional[str] = None,
        check_type: ConsistencyCheckType = ConsistencyCheckType.DEEP,
    ) -> ConsistencyAuditReport:
        """
        Run comprehensive consistency audit across all cache entries.

        Args:
            client_account_id: Specific client to audit (None for platform-wide)
            check_type: Type of consistency check to perform

        Returns:
            ConsistencyAuditReport with comprehensive results
        """
        audit_id = f"audit_{int(time.time() * 1000)}"
        report = ConsistencyAuditReport(
            audit_id=audit_id,
            start_time=datetime.utcnow(),
        )

        logger.info(f"Starting consistency audit {audit_id}")

        try:
            # Get all cache keys to check
            cache_keys = await self._get_all_cache_keys(client_account_id)
            report.total_keys_checked = len(cache_keys)

            if not cache_keys:
                logger.info(f"No cache keys found for audit {audit_id}")
                report.end_time = datetime.utcnow()
                return report

            # Perform bulk consistency check
            results = await self.check_bulk_consistency(
                cache_keys, check_type, client_account_id
            )
            report.detailed_results = results

            # Aggregate results
            for result in results:
                if result.status == ConsistencyStatus.CONSISTENT:
                    report.consistent_keys += 1
                elif result.status == ConsistencyStatus.INCONSISTENT:
                    report.inconsistent_keys += 1
                elif result.status == ConsistencyStatus.CORRUPTED:
                    report.corrupted_keys += 1
                elif result.status == ConsistencyStatus.MISSING:
                    report.missing_keys += 1
                elif result.status == ConsistencyStatus.ORPHANED:
                    report.orphaned_keys += 1

            # Attempt repairs for inconsistent entries
            inconsistent_results = [r for r in results if r.needs_repair]
            repair_results = await self._repair_inconsistencies(inconsistent_results)

            report.repairs_attempted = len(repair_results)
            report.repairs_successful = sum(1 for success in repair_results if success)
            report.repairs_failed = report.repairs_attempted - report.repairs_successful

            report.end_time = datetime.utcnow()
            report.total_execution_time_ms = report.duration_seconds * 1000

            # Update metrics
            self.metrics.last_audit_time = report.end_time

            logger.info(
                f"Completed consistency audit {audit_id}: "
                f"{report.consistent_keys}/{report.total_keys_checked} consistent, "
                f"{report.repairs_successful}/{report.repairs_attempted} repairs successful"
            )

            return report

        except Exception as e:
            logger.error(f"Consistency audit {audit_id} failed: {e}")
            report.end_time = datetime.utcnow()
            return report

    async def repair_inconsistency(self, result: ConsistencyCheckResult) -> bool:
        """
        Repair a single inconsistency.

        Args:
            result: ConsistencyCheckResult indicating the inconsistency

        Returns:
            True if repair was successful
        """
        if not result.needs_repair:
            return True

        if not result.suggested_repair:
            # Determine appropriate repair action
            result.suggested_repair = self._determine_repair_action(result)

        repair_handler = self.repair_handlers.get(result.suggested_repair)
        if not repair_handler:
            logger.error(f"No repair handler for action: {result.suggested_repair}")
            return False

        try:
            success = await repair_handler(result)
            self.metrics.update_repair(success)

            if success:
                logger.info(
                    f"Successfully repaired inconsistency for {result.cache_key}"
                )
            else:
                logger.warning(f"Failed to repair inconsistency for {result.cache_key}")

            return success

        except Exception as e:
            logger.error(f"Repair failed for {result.cache_key}: {e}")
            self.metrics.update_repair(False)
            return False

    def get_consistency_metrics(self) -> Dict[str, Any]:
        """Get comprehensive consistency metrics"""
        return {
            "total_checks": self.metrics.total_checks,
            "average_execution_time_ms": self.metrics.average_execution_time_ms,
            "checks_by_type": {
                check_type.value: count
                for check_type, count in self.metrics.checks_by_type.items()
            },
            "status_distribution": {
                status.value: count
                for status, count in self.metrics.status_distribution.items()
            },
            "consistency_score": (
                self.metrics.status_distribution[ConsistencyStatus.CONSISTENT]
                / max(self.metrics.total_checks, 1)
                * 100
            ),
            "repairs": {
                "attempted": self.metrics.repairs_attempted,
                "successful": self.metrics.repairs_successful,
                "success_rate": (
                    self.metrics.repairs_successful
                    / max(self.metrics.repairs_attempted, 1)
                    * 100
                ),
            },
            "last_audit_time": (
                self.metrics.last_audit_time.isoformat()
                if self.metrics.last_audit_time
                else None
            ),
            "quarantined_items": len(self.quarantine_storage),
            "recent_results_count": len(self.recent_results),
            "last_reset": self.metrics.last_reset.isoformat(),
        }

    def reset_metrics(self) -> None:
        """Reset consistency metrics"""
        self.metrics = ConsistencyMetrics()
        self.recent_results.clear()
        logger.info("Consistency metrics reset")

    async def health_check(self) -> Dict[str, Any]:
        """Perform health check on consistency checker"""
        health = {
            "status": "healthy",
            "scheduled_audits_running": (
                self._audit_task is not None and not self._audit_task.done()
            ),
            "total_checks_performed": self.metrics.total_checks,
            "last_audit_time": (
                self.metrics.last_audit_time.isoformat()
                if self.metrics.last_audit_time
                else None
            ),
            "quarantined_items": len(self.quarantine_storage),
            "recent_consistency_score": 0.0,
            "issues": [],
        }

        # Calculate recent consistency score
        if self.recent_results:
            consistent_count = sum(
                1
                for result in self.recent_results
                if result.status == ConsistencyStatus.CONSISTENT
            )
            health["recent_consistency_score"] = (
                consistent_count / len(self.recent_results) * 100
            )

        # Check for issues
        if health["recent_consistency_score"] < 90:
            health["issues"].append(
                f"Low consistency score: {health['recent_consistency_score']:.1f}%"
            )
            health["status"] = "warning"

        if health["quarantined_items"] > 100:
            health["issues"].append(
                f"High number of quarantined items: {health['quarantined_items']}"
            )
            health["status"] = "warning"

        if not health["scheduled_audits_running"]:
            health["issues"].append("Scheduled audits not running")
            if health["status"] == "healthy":
                health["status"] = "degraded"

        return health

    # Private methods

    async def _check_basic_consistency(
        self, cache_key: str, client_account_id: Optional[str]
    ) -> ConsistencyCheckResult:
        """Perform basic consistency check (existence and TTL)"""
        # Check if cache key exists
        cache_exists = await self.redis_cache.exists(cache_key)

        if not cache_exists:
            return ConsistencyCheckResult(
                cache_key=cache_key,
                status=ConsistencyStatus.MISSING,
                check_type=ConsistencyCheckType.BASIC,
                details="Cache key does not exist",
                suggested_repair=RepairAction.REFRESH_FROM_DB,
            )

        # Basic check passes if key exists
        return ConsistencyCheckResult(
            cache_key=cache_key,
            status=ConsistencyStatus.CONSISTENT,
            check_type=ConsistencyCheckType.BASIC,
            details="Cache key exists",
        )

    async def _check_deep_consistency(
        self, cache_key: str, client_account_id: Optional[str]
    ) -> ConsistencyCheckResult:
        """Perform deep consistency check (full data comparison)"""
        # Get cache data
        cache_value = await self.redis_cache.get(cache_key)

        # Extract entity information from cache key
        entity_info = self._parse_cache_key(cache_key)
        if not entity_info:
            return ConsistencyCheckResult(
                cache_key=cache_key,
                status=ConsistencyStatus.CORRUPTED,
                check_type=ConsistencyCheckType.DEEP,
                details="Cannot parse cache key format",
                suggested_repair=RepairAction.INVALIDATE_CACHE,
            )

        # Get corresponding database data
        db_value = await self._get_database_value(entity_info, client_account_id)

        # Compare values
        if cache_value is None and db_value is None:
            return ConsistencyCheckResult(
                cache_key=cache_key,
                status=ConsistencyStatus.CONSISTENT,
                check_type=ConsistencyCheckType.DEEP,
                details="Both cache and database have no data",
            )
        elif cache_value is None:
            return ConsistencyCheckResult(
                cache_key=cache_key,
                status=ConsistencyStatus.MISSING,
                check_type=ConsistencyCheckType.DEEP,
                details="Cache missing but database has data",
                database_value=db_value,
                suggested_repair=RepairAction.REFRESH_FROM_DB,
            )
        elif db_value is None:
            return ConsistencyCheckResult(
                cache_key=cache_key,
                status=ConsistencyStatus.ORPHANED,
                check_type=ConsistencyCheckType.DEEP,
                details="Cache exists but no corresponding database data",
                cache_value=cache_value,
                suggested_repair=RepairAction.DELETE_ORPHANED,
            )
        else:
            # Compare data values
            if self._compare_data_values(cache_value, db_value):
                return ConsistencyCheckResult(
                    cache_key=cache_key,
                    status=ConsistencyStatus.CONSISTENT,
                    check_type=ConsistencyCheckType.DEEP,
                    details="Cache and database data match",
                    cache_value=cache_value,
                    database_value=db_value,
                )
            else:
                # Calculate data diff
                data_diff = self._calculate_data_diff(cache_value, db_value)

                return ConsistencyCheckResult(
                    cache_key=cache_key,
                    status=ConsistencyStatus.INCONSISTENT,
                    check_type=ConsistencyCheckType.DEEP,
                    details="Cache and database data do not match",
                    cache_value=cache_value,
                    database_value=db_value,
                    data_diff=data_diff,
                    suggested_repair=RepairAction.REFRESH_FROM_DB,
                )

    async def _check_checksum_consistency(
        self, cache_key: str, client_account_id: Optional[str]
    ) -> ConsistencyCheckResult:
        """Perform checksum-based consistency check"""
        # Get cache data
        cache_value = await self.redis_cache.get(cache_key)
        if cache_value is None:
            return ConsistencyCheckResult(
                cache_key=cache_key,
                status=ConsistencyStatus.MISSING,
                check_type=ConsistencyCheckType.CHECKSUM,
                details="Cache key does not exist",
                suggested_repair=RepairAction.REFRESH_FROM_DB,
            )

        # Calculate cache data checksum
        cache_checksum = self._calculate_checksum(cache_value)

        # Get expected checksum from metadata or calculate from database
        entity_info = self._parse_cache_key(cache_key)
        if entity_info:
            db_value = await self._get_database_value(entity_info, client_account_id)
            if db_value is not None:
                expected_checksum = self._calculate_checksum(db_value)

                if cache_checksum == expected_checksum:
                    return ConsistencyCheckResult(
                        cache_key=cache_key,
                        status=ConsistencyStatus.CONSISTENT,
                        check_type=ConsistencyCheckType.CHECKSUM,
                        details="Checksums match",
                    )
                else:
                    return ConsistencyCheckResult(
                        cache_key=cache_key,
                        status=ConsistencyStatus.INCONSISTENT,
                        check_type=ConsistencyCheckType.CHECKSUM,
                        details=f"Checksum mismatch: cache={cache_checksum}, db={expected_checksum}",
                        checksum_mismatch=True,
                        suggested_repair=RepairAction.REFRESH_FROM_DB,
                    )

        # Cannot verify checksum without database reference
        return ConsistencyCheckResult(
            cache_key=cache_key,
            status=ConsistencyStatus.CONSISTENT,
            check_type=ConsistencyCheckType.CHECKSUM,
            details="Cannot verify checksum without database reference",
            confidence=0.5,
        )

    async def _check_schema_consistency(
        self, cache_key: str, client_account_id: Optional[str]
    ) -> ConsistencyCheckResult:
        """Perform schema validation consistency check"""
        # Get cache data
        cache_value = await self.redis_cache.get(cache_key)
        if cache_value is None:
            return ConsistencyCheckResult(
                cache_key=cache_key,
                status=ConsistencyStatus.MISSING,
                check_type=ConsistencyCheckType.SCHEMA_VALIDATION,
                details="Cache key does not exist",
                suggested_repair=RepairAction.REFRESH_FROM_DB,
            )

        # Validate cache data schema
        schema_validation_result = self._validate_cache_data_schema(
            cache_key, cache_value
        )

        if schema_validation_result["valid"]:
            return ConsistencyCheckResult(
                cache_key=cache_key,
                status=ConsistencyStatus.CONSISTENT,
                check_type=ConsistencyCheckType.SCHEMA_VALIDATION,
                details="Cache data schema is valid",
            )
        else:
            return ConsistencyCheckResult(
                cache_key=cache_key,
                status=ConsistencyStatus.CORRUPTED,
                check_type=ConsistencyCheckType.SCHEMA_VALIDATION,
                details=f"Schema validation failed: {schema_validation_result['errors']}",
                suggested_repair=RepairAction.QUARANTINE,
            )

    def _parse_cache_key(self, cache_key: str) -> Optional[Dict[str, str]]:
        """Parse cache key to extract entity information"""
        # Remove version prefix
        if cache_key.startswith(f"{CACHE_VERSION}:"):
            key_parts = cache_key[len(f"{CACHE_VERSION}:") :].split(":")
        else:
            key_parts = cache_key.split(":")

        # Parse different cache key formats
        if len(key_parts) >= 2:
            if key_parts[0] == "user" and len(key_parts) >= 3:
                return {
                    "entity_type": "user",
                    "entity_id": (
                        key_parts[2] if key_parts[1] == "context" else key_parts[1]
                    ),
                    "data_type": (
                        key_parts[1]
                        if key_parts[1] in ["context", "clients", "defaults"]
                        else "unknown"
                    ),
                }
            elif key_parts[0] == "client" and len(key_parts) >= 2:
                return {
                    "entity_type": "client",
                    "entity_id": key_parts[1],
                    "data_type": key_parts[2] if len(key_parts) > 2 else "settings",
                }

        return None

    async def _get_database_value(
        self, entity_info: Dict[str, str], client_account_id: Optional[str]
    ) -> Optional[Any]:
        """Get corresponding value from database"""
        entity_type = entity_info["entity_type"]
        entity_id = entity_info["entity_id"]

        extractor = self.data_extractors.get(entity_type)
        if not extractor:
            return None

        try:
            return await extractor(entity_id, client_account_id)
        except Exception as e:
            logger.error(
                f"Failed to extract database value for {entity_type}:{entity_id}: {e}"
            )
            return None

    async def _extract_user_data(
        self, user_id: str, client_account_id: Optional[str]
    ) -> Optional[Dict[str, Any]]:
        """Extract user data from database"""
        try:
            from app.models.user import User

            stmt = select(User).where(User.id == user_id)
            result = await self.db_session.execute(stmt)
            user = result.scalar_one_or_none()

            if not user:
                return None

            return {
                "id": str(user.id),
                "email": user.email,
                "full_name": user.full_name,
                "is_active": user.is_active,
                "role": getattr(user, "role", None),
            }
        except Exception as e:
            logger.error(f"Failed to extract user data for {user_id}: {e}")
            return None

    async def _extract_client_data(
        self, client_id: str, client_account_id: Optional[str]
    ) -> Optional[Dict[str, Any]]:
        """Extract client data from database"""
        try:
            from app.models.client_account import ClientAccount

            stmt = select(ClientAccount).where(ClientAccount.id == client_id)
            result = await self.db_session.execute(stmt)
            client = result.scalar_one_or_none()

            if not client:
                return None

            return {
                "id": str(client.id),
                "name": client.name,
                "is_active": client.is_active,
            }
        except Exception as e:
            logger.error(f"Failed to extract client data for {client_id}: {e}")
            return None

    async def _extract_engagement_data(
        self, engagement_id: str, client_account_id: Optional[str]
    ) -> Optional[Dict[str, Any]]:
        """Extract engagement data from database"""
        try:
            from app.models.engagement import Engagement

            stmt = select(Engagement).where(Engagement.id == engagement_id)
            result = await self.db_session.execute(stmt)
            engagement = result.scalar_one_or_none()

            if not engagement:
                return None

            return {
                "id": str(engagement.id),
                "name": engagement.name,
                "status": getattr(engagement, "status", None),
                "is_active": getattr(engagement, "is_active", True),
            }
        except Exception as e:
            logger.error(f"Failed to extract engagement data for {engagement_id}: {e}")
            return None

    async def _extract_flow_data(
        self, flow_id: str, client_account_id: Optional[str]
    ) -> Optional[Dict[str, Any]]:
        """Extract flow data from database"""
        # This would depend on the specific flow model structure
        # For now, return None as a placeholder
        return None

    def _compare_data_values(self, cache_value: Any, db_value: Any) -> bool:
        """Compare cache and database values for equality"""
        try:
            # Convert both to JSON strings for comparison
            cache_json = json.dumps(cache_value, sort_keys=True, default=str)
            db_json = json.dumps(db_value, sort_keys=True, default=str)
            return cache_json == db_json
        except Exception:
            # Fallback to direct comparison
            return cache_value == db_value

    def _calculate_data_diff(self, cache_value: Any, db_value: Any) -> Dict[str, Any]:
        """Calculate difference between cache and database values"""
        diff = {
            "cache_only": {},
            "db_only": {},
            "different_values": {},
        }

        try:
            if isinstance(cache_value, dict) and isinstance(db_value, dict):
                # Compare dictionaries
                cache_keys = set(cache_value.keys())
                db_keys = set(db_value.keys())

                diff["cache_only"] = {k: cache_value[k] for k in cache_keys - db_keys}
                diff["db_only"] = {k: db_value[k] for k in db_keys - cache_keys}

                for key in cache_keys & db_keys:
                    if cache_value[key] != db_value[key]:
                        diff["different_values"][key] = {
                            "cache": cache_value[key],
                            "database": db_value[key],
                        }
            else:
                # Simple value comparison
                diff["different_values"]["value"] = {
                    "cache": cache_value,
                    "database": db_value,
                }
        except Exception as e:
            diff["error"] = str(e)

        return diff

    def _calculate_checksum(self, data: Any) -> str:
        """Calculate checksum for data"""
        try:
            data_str = json.dumps(data, sort_keys=True, default=str)
            return hashlib.sha256(data_str.encode()).hexdigest()[:16]
        except Exception:
            return hashlib.sha256(str(data).encode()).hexdigest()[:16]

    def _validate_cache_data_schema(
        self, cache_key: str, cache_value: Any
    ) -> Dict[str, Any]:
        """Validate cache data against expected schema"""
        # This is a simplified schema validation
        # In a real implementation, you'd use a proper schema validation library

        result = {"valid": True, "errors": []}

        try:
            # Basic validation - check if data is serializable
            json.dumps(cache_value, default=str)

            # Additional validations based on cache key type
            cache_type = CacheKeys.get_cache_type(cache_key)

            if cache_type == CacheKeyType.USER_CONTEXT:
                if not isinstance(cache_value, dict):
                    result["valid"] = False
                    result["errors"].append("User context must be a dictionary")
                elif "user_id" not in cache_value:
                    result["valid"] = False
                    result["errors"].append("User context must contain user_id")

        except Exception as e:
            result["valid"] = False
            result["errors"].append(f"Serialization error: {e}")

        return result

    async def _get_all_cache_keys(self, client_account_id: Optional[str]) -> List[str]:
        """Get all cache keys for consistency checking"""
        try:
            # This would require Redis SCAN functionality
            # For now, return a subset based on known patterns
            cache_keys = []

            # Get cache metadata from database
            stmt = (
                select(CacheMetadata.cache_key)
                .where(
                    and_(
                        CacheMetadata.is_active == True,  # noqa: E712
                        (
                            CacheMetadata.client_account_id == client_account_id
                            if client_account_id
                            else True
                        ),
                    )
                )
                .limit(1000)
            )  # Limit for performance

            result = await self.db_session.execute(stmt)
            cache_keys = [row[0] for row in result.fetchall()]

            return cache_keys

        except Exception as e:
            logger.error(f"Failed to get cache keys: {e}")
            return []

    async def _repair_inconsistencies(
        self, results: List[ConsistencyCheckResult]
    ) -> List[bool]:
        """Repair multiple inconsistencies"""
        repair_results = []

        for result in results:
            try:
                success = await self.repair_inconsistency(result)
                repair_results.append(success)
            except Exception as e:
                logger.error(f"Repair failed for {result.cache_key}: {e}")
                repair_results.append(False)

        return repair_results

    def _determine_repair_action(self, result: ConsistencyCheckResult) -> RepairAction:
        """Determine appropriate repair action for inconsistency"""
        if result.status == ConsistencyStatus.MISSING:
            return RepairAction.REFRESH_FROM_DB
        elif result.status == ConsistencyStatus.ORPHANED:
            return RepairAction.DELETE_ORPHANED
        elif result.status == ConsistencyStatus.CORRUPTED:
            return RepairAction.QUARANTINE
        elif result.status == ConsistencyStatus.INCONSISTENT:
            return RepairAction.REFRESH_FROM_DB
        else:
            return RepairAction.INVALIDATE_CACHE

    async def _repair_refresh_from_db(self, result: ConsistencyCheckResult) -> bool:
        """Repair by refreshing cache from database"""
        try:
            # Parse cache key to get entity info
            entity_info = self._parse_cache_key(result.cache_key)
            if not entity_info:
                return False

            # Get fresh data from database
            db_value = await self._get_database_value(entity_info, None)
            if db_value is None:
                # No database data, delete cache entry
                return await self.redis_cache.delete(result.cache_key)

            # Update cache with fresh data
            ttl = CacheKeys.get_ttl_recommendation(result.cache_key)
            return await self.redis_cache.set(result.cache_key, db_value, ttl)

        except Exception as e:
            logger.error(f"Refresh from DB failed for {result.cache_key}: {e}")
            return False

    async def _repair_invalidate_cache(self, result: ConsistencyCheckResult) -> bool:
        """Repair by invalidating cache entry"""
        try:
            return await self.redis_cache.delete(result.cache_key)
        except Exception as e:
            logger.error(f"Cache invalidation failed for {result.cache_key}: {e}")
            return False

    async def _repair_delete_orphaned(self, result: ConsistencyCheckResult) -> bool:
        """Repair by deleting orphaned cache entry"""
        try:
            return await self.redis_cache.delete(result.cache_key)
        except Exception as e:
            logger.error(f"Orphaned entry deletion failed for {result.cache_key}: {e}")
            return False

    async def _repair_quarantine(self, result: ConsistencyCheckResult) -> bool:
        """Repair by quarantining suspicious data"""
        try:
            # Move data to quarantine
            self.quarantine_storage[result.cache_key] = {
                "data": result.cache_value,
                "quarantined_at": datetime.utcnow().isoformat(),
                "reason": result.details,
            }

            # Delete from cache
            await self.redis_cache.delete(result.cache_key)

            logger.info(f"Quarantined suspicious data for {result.cache_key}")
            return True

        except Exception as e:
            logger.error(f"Quarantine failed for {result.cache_key}: {e}")
            return False

    async def _scheduled_audit_loop(self) -> None:
        """Background loop for scheduled consistency audits"""
        logger.info("Starting scheduled consistency audit loop")

        while not self._shutdown_event.is_set():
            try:
                # Wait for next audit interval
                await asyncio.sleep(self.default_check_interval_hours * 3600)

                if self._shutdown_event.is_set():
                    break

                # Run consistency audit
                logger.info("Starting scheduled consistency audit")
                report = await self.run_full_consistency_audit()

                # Log audit results
                logger.info(
                    f"Scheduled audit completed: "
                    f"consistency_score={report.consistency_score:.2%}, "
                    f"repairs={report.repairs_successful}/{report.repairs_attempted}"
                )

            except Exception as e:
                logger.error(f"Scheduled audit failed: {e}")
                # Continue with next iteration

        logger.info("Scheduled consistency audit loop stopped")


# Factory function for dependency injection
async def create_cache_consistency_checker(
    redis_cache: RedisCache,
    db_session: Optional[AsyncSession] = None,
) -> CacheConsistencyChecker:
    """Create CacheConsistencyChecker with dependencies"""
    if db_session is None:
        async for session in get_db():
            db_session = session
            break

    return CacheConsistencyChecker(redis_cache, db_session)


# Export all classes
__all__ = [
    "ConsistencyCheckType",
    "ConsistencyStatus",
    "RepairAction",
    "ConsistencyCheckResult",
    "ConsistencyAuditReport",
    "ConsistencyMetrics",
    "CacheConsistencyChecker",
    "create_cache_consistency_checker",
]
