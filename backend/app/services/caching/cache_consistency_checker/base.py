"""
Cache Consistency Checker Base Classes and Types

This module contains the fundamental building blocks for cache consistency checking:
- Enums for check types, statuses, and repair actions
- Dataclasses for results, reports, and metrics
- Constants and configuration values
- Base validation utilities

ðŸ”’ Security: Type-safe definitions for consistency operations
âš¡ Performance: Efficient dataclasses with optimized field access
ðŸŽ¯ Coherence: Centralized type definitions for system-wide consistency

Generated by CC (Claude Code)
"""

from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional


class ConsistencyCheckType(Enum):
    """Types of consistency checks"""

    BASIC = "basic"  # Basic existence and TTL checks
    DEEP = "deep"  # Full data comparison with database
    CHECKSUM = "checksum"  # Checksum-based integrity verification
    SCHEMA_VALIDATION = "schema"  # Data schema and format validation
    PERFORMANCE_IMPACT = "performance"  # Performance impact assessment


class ConsistencyStatus(Enum):
    """Cache consistency status"""

    CONSISTENT = "consistent"  # Cache and database are in sync
    INCONSISTENT = "inconsistent"  # Data mismatch detected
    STALE = "stale"  # Cache data is outdated
    CORRUPTED = "corrupted"  # Cache data is corrupted
    MISSING = "missing"  # Expected cache entry is missing
    ORPHANED = "orphaned"  # Cache entry has no database counterpart


class RepairAction(Enum):
    """Available repair actions"""

    REFRESH_FROM_DB = "refresh_from_db"  # Update cache from database
    INVALIDATE_CACHE = "invalidate_cache"  # Remove inconsistent cache entry
    UPDATE_DATABASE = "update_database"  # Update database from cache (rare)
    DELETE_ORPHANED = "delete_orphaned"  # Remove orphaned cache entries
    QUARANTINE = "quarantine"  # Move suspicious data to quarantine


@dataclass
class ConsistencyCheckResult:
    """Result of a consistency check operation"""

    cache_key: str
    status: ConsistencyStatus
    check_type: ConsistencyCheckType
    details: str
    database_value: Optional[Any] = None
    cache_value: Optional[Any] = None
    checksum_mismatch: bool = False
    data_diff: Optional[Dict[str, Any]] = None
    suggested_repair: Optional[RepairAction] = None
    confidence: float = 1.0  # 0.0 to 1.0
    execution_time_ms: float = 0
    timestamp: datetime = field(default_factory=datetime.utcnow)

    @property
    def needs_repair(self) -> bool:
        """Check if this result indicates repair is needed"""
        return self.status != ConsistencyStatus.CONSISTENT

    @property
    def is_critical(self) -> bool:
        """Check if this inconsistency is critical"""
        return self.status in [
            ConsistencyStatus.CORRUPTED,
            ConsistencyStatus.INCONSISTENT,
        ]


@dataclass
class ConsistencyAuditReport:
    """Comprehensive consistency audit report"""

    audit_id: str
    start_time: datetime
    end_time: Optional[datetime] = None
    total_keys_checked: int = 0
    consistent_keys: int = 0
    inconsistent_keys: int = 0
    corrupted_keys: int = 0
    missing_keys: int = 0
    orphaned_keys: int = 0
    repairs_attempted: int = 0
    repairs_successful: int = 0
    repairs_failed: int = 0
    total_execution_time_ms: float = 0
    performance_impact: Dict[str, float] = field(default_factory=dict)
    detailed_results: List[ConsistencyCheckResult] = field(default_factory=list)

    @property
    def consistency_score(self) -> float:
        """Calculate overall consistency score (0.0 to 1.0)"""
        if self.total_keys_checked == 0:
            return 1.0
        return self.consistent_keys / self.total_keys_checked

    @property
    def repair_success_rate(self) -> float:
        """Calculate repair success rate"""
        if self.repairs_attempted == 0:
            return 1.0
        return self.repairs_successful / self.repairs_attempted

    @property
    def duration_seconds(self) -> float:
        """Get audit duration in seconds"""
        if not self.end_time:
            return 0.0
        return (self.end_time - self.start_time).total_seconds()


@dataclass
class ConsistencyMetrics:
    """Consistency checking metrics"""

    total_checks: int = 0
    checks_by_type: Dict[ConsistencyCheckType, int] = field(
        default_factory=lambda: defaultdict(int)
    )
    status_distribution: Dict[ConsistencyStatus, int] = field(
        default_factory=lambda: defaultdict(int)
    )
    total_execution_time_ms: float = 0
    average_execution_time_ms: float = 0
    repairs_attempted: int = 0
    repairs_successful: int = 0
    last_audit_time: Optional[datetime] = None
    last_reset: datetime = field(default_factory=datetime.utcnow)

    def update_check(self, result: ConsistencyCheckResult) -> None:
        """Update metrics with check result"""
        self.total_checks += 1
        self.checks_by_type[result.check_type] += 1
        self.status_distribution[result.status] += 1
        self.total_execution_time_ms += result.execution_time_ms

        if self.total_checks > 0:
            self.average_execution_time_ms = (
                self.total_execution_time_ms / self.total_checks
            )

    def update_repair(self, success: bool) -> None:
        """Update repair metrics"""
        self.repairs_attempted += 1
        if success:
            self.repairs_successful += 1


@dataclass
class ConsistencyConfiguration:
    """Configuration for consistency checker operations"""

    batch_size: int = 100
    max_concurrent_checks: int = 5
    default_check_interval_hours: int = 24
    performance_impact_threshold: float = 0.1  # 10% performance impact threshold
    max_recent_results: int = 1000
    enable_quarantine: bool = True
    quarantine_max_items: int = 10000
    metrics_retention_days: int = 30
    audit_timeout_seconds: int = 3600  # 1 hour
    repair_retry_attempts: int = 3
    repair_retry_delay_seconds: float = 1.0


@dataclass
class EntityInfo:
    """Parsed entity information from cache keys"""

    entity_type: str
    entity_id: str
    data_type: str = "unknown"
    client_account_id: Optional[str] = None

    @property
    def is_valid(self) -> bool:
        """Check if entity info is valid"""
        return bool(self.entity_type and self.entity_id)


# Constants for configuration and defaults
DEFAULT_CONFIG = ConsistencyConfiguration()

# Cache key patterns for entity extraction
CACHE_KEY_PATTERNS = {
    "user": [
        "user:context:{user_id}",
        "user:clients:{user_id}",
        "user:defaults:{user_id}",
    ],
    "client": ["client:{client_id}:{data_type}", "client:{client_id}"],
    "engagement": [
        "engagement:{engagement_id}:{data_type}",
        "engagement:{engagement_id}",
    ],
    "flow": ["flow:{flow_id}:{data_type}", "flow:{flow_id}"],
}

# Performance thresholds
PERFORMANCE_THRESHOLDS = {
    "check_time_ms": 1000,  # Maximum acceptable check time
    "repair_time_ms": 5000,  # Maximum acceptable repair time
    "audit_time_seconds": 3600,  # Maximum acceptable audit time
    "memory_usage_mb": 100,  # Maximum memory usage during operations
}

# Validation constants
MAX_CACHE_KEY_LENGTH = 250
MAX_DATA_SIZE_BYTES = 1024 * 1024  # 1MB
MIN_CONFIDENCE_THRESHOLD = 0.5


def create_default_metrics() -> ConsistencyMetrics:
    """Create a new ConsistencyMetrics instance with defaults"""
    return ConsistencyMetrics()


def create_default_configuration(**overrides) -> ConsistencyConfiguration:
    """Create a new ConsistencyConfiguration with optional overrides"""
    config_dict = {
        field.name: getattr(DEFAULT_CONFIG, field.name)
        for field in DEFAULT_CONFIG.__dataclass_fields__.values()
    }
    config_dict.update(overrides)
    return ConsistencyConfiguration(**config_dict)


def validate_cache_key(cache_key: str) -> bool:
    """Validate cache key format and length"""
    if not cache_key or not isinstance(cache_key, str):
        return False

    if len(cache_key) > MAX_CACHE_KEY_LENGTH:
        return False

    # Check for basic key structure (must contain at least one colon)
    return ":" in cache_key


def validate_data_size(data: Any) -> bool:
    """Validate data size for consistency checking"""
    try:
        import json

        data_str = json.dumps(data, default=str)
        return len(data_str.encode("utf-8")) <= MAX_DATA_SIZE_BYTES
    except Exception:
        # If we can't serialize, assume it's too large
        return False


def calculate_confidence_score(
    check_type: ConsistencyCheckType,
    has_database_reference: bool,
    data_completeness: float = 1.0,
) -> float:
    """Calculate confidence score for consistency check result"""
    base_scores = {
        ConsistencyCheckType.BASIC: 0.6,
        ConsistencyCheckType.DEEP: 0.9,
        ConsistencyCheckType.CHECKSUM: 0.8,
        ConsistencyCheckType.SCHEMA_VALIDATION: 0.7,
        ConsistencyCheckType.PERFORMANCE_IMPACT: 0.5,
    }

    base_score = base_scores.get(check_type, 0.5)

    # Adjust based on database reference availability
    if not has_database_reference:
        base_score *= 0.7

    # Adjust based on data completeness
    base_score *= data_completeness

    return min(1.0, max(0.0, base_score))


# Export all classes and functions
__all__ = [
    # Enums
    "ConsistencyCheckType",
    "ConsistencyStatus",
    "RepairAction",
    # Dataclasses
    "ConsistencyCheckResult",
    "ConsistencyAuditReport",
    "ConsistencyMetrics",
    "ConsistencyConfiguration",
    "EntityInfo",
    # Constants
    "DEFAULT_CONFIG",
    "CACHE_KEY_PATTERNS",
    "PERFORMANCE_THRESHOLDS",
    "MAX_CACHE_KEY_LENGTH",
    "MAX_DATA_SIZE_BYTES",
    "MIN_CONFIDENCE_THRESHOLD",
    # Factory functions
    "create_default_metrics",
    "create_default_configuration",
    # Validation functions
    "validate_cache_key",
    "validate_data_size",
    "calculate_confidence_score",
]
