"""
Bulk cache invalidation operations.

This module provides efficient bulk invalidation operations for handling
large-scale cache updates and batch processing scenarios.

Generated by CC (Claude Code)
"""

import asyncio
import time
from typing import Any, Dict, List, Optional

from app.core.logging import get_logger

from .base import (
    BaseInvalidationStrategy,
    InvalidationEvent,
    InvalidationResult,
    InvalidationTrigger,
)

logger = get_logger(__name__)


class BulkInvalidationStrategy(BaseInvalidationStrategy):
    """
    Handles bulk cache invalidation operations with:
    - Batch processing for multiple keys
    - Parallel invalidation with concurrency control
    - Progress tracking and error recovery
    - Optimized Redis operations
    """

    def __init__(self, redis_cache):
        super().__init__(redis_cache)
        self.max_batch_size = 100
        self.max_concurrency = 10
        self.retry_attempts = 3

    def can_handle(self, event: InvalidationEvent) -> bool:
        """Handle bulk invalidation events"""
        return (
            event.trigger == InvalidationTrigger.MANUAL_INVALIDATION
            and event.metadata.get("bulk_operation", False)
        )

    async def invalidate(self, event: InvalidationEvent) -> InvalidationResult:
        """Execute bulk invalidation operation"""
        start_time = time.time()

        try:
            # Extract bulk operation parameters
            keys_to_invalidate = event.metadata.get("keys", [])
            patterns = event.metadata.get("patterns", [])
            operation_type = event.metadata.get("operation_type", "delete")

            if not keys_to_invalidate and not patterns:
                return InvalidationResult(
                    success=False,
                    keys_invalidated=0,
                    execution_time_ms=(time.time() - start_time) * 1000,
                    strategy_used=self.get_strategy_name(),
                    errors=["No keys or patterns provided for bulk operation"],
                )

            # Expand patterns to actual keys
            all_keys = list(keys_to_invalidate)
            for pattern in patterns:
                pattern_keys = await self._find_keys_by_pattern(pattern)
                all_keys.extend(pattern_keys)

            # Remove duplicates while preserving order
            unique_keys = list(dict.fromkeys(all_keys))

            # Execute bulk operation
            if operation_type == "delete":
                result = await self._bulk_delete(unique_keys)
            else:
                return InvalidationResult(
                    success=False,
                    keys_invalidated=0,
                    execution_time_ms=(time.time() - start_time) * 1000,
                    strategy_used=self.get_strategy_name(),
                    errors=[f"Unsupported bulk operation type: {operation_type}"],
                )

            execution_time = (time.time() - start_time) * 1000

            return InvalidationResult(
                success=result["success"],
                keys_invalidated=result["keys_invalidated"],
                execution_time_ms=execution_time,
                strategy_used=self.get_strategy_name(),
                metadata={
                    "total_keys": len(unique_keys),
                    "failed_keys": result["failed_keys"],
                    "operation_type": operation_type,
                },
                errors=result.get("errors", []),
            )

        except Exception as e:
            logger.error(f"Bulk invalidation failed: {e}")
            return InvalidationResult(
                success=False,
                keys_invalidated=0,
                execution_time_ms=(time.time() - start_time) * 1000,
                strategy_used=self.get_strategy_name(),
                errors=[str(e)],
            )

    async def _bulk_delete(self, keys: List[str]) -> Dict[str, Any]:
        """Perform bulk deletion of cache keys"""
        total_keys = len(keys)
        keys_invalidated = 0
        failed_keys = 0
        errors = []

        # Process keys in batches
        batches = [
            keys[i : i + self.max_batch_size]
            for i in range(0, len(keys), self.max_batch_size)
        ]

        # Create semaphore for concurrency control
        semaphore = asyncio.Semaphore(self.max_concurrency)

        async def process_batch(batch: List[str]) -> Dict[str, Any]:
            """Process a single batch of keys"""
            async with semaphore:
                batch_invalidated = 0
                batch_failed = 0
                batch_errors = []

                for cache_key in batch:
                    for attempt in range(self.retry_attempts):
                        try:
                            deleted = await self.redis_cache.delete(cache_key)
                            if deleted:
                                batch_invalidated += 1
                            break  # Success, exit retry loop
                        except Exception as e:
                            if attempt == self.retry_attempts - 1:
                                # Final attempt failed
                                batch_failed += 1
                                batch_errors.append(
                                    f"Failed to delete {cache_key}: {str(e)}"
                                )
                                logger.error(f"Failed to delete {cache_key}: {e}")
                            else:
                                # Retry after a short delay
                                await asyncio.sleep(0.1 * (attempt + 1))

                return {
                    "invalidated": batch_invalidated,
                    "failed": batch_failed,
                    "errors": batch_errors,
                }

        # Process all batches concurrently
        try:
            batch_results = await asyncio.gather(
                *[process_batch(batch) for batch in batches], return_exceptions=True
            )

            # Aggregate results
            for result in batch_results:
                if isinstance(result, Exception):
                    errors.append(f"Batch processing error: {str(result)}")
                    logger.error(f"Batch processing error: {result}")
                else:
                    keys_invalidated += result["invalidated"]
                    failed_keys += result["failed"]
                    errors.extend(result["errors"])

        except Exception as e:
            errors.append(f"Bulk processing error: {str(e)}")
            logger.error(f"Bulk processing error: {e}")

        success = failed_keys == 0

        if failed_keys > 0:
            logger.warning(
                f"Bulk deletion completed with {failed_keys} failures out of {total_keys} keys"
            )

        return {
            "success": success,
            "keys_invalidated": keys_invalidated,
            "failed_keys": failed_keys,
            "errors": errors,
        }

    async def _find_keys_by_pattern(self, pattern: str) -> List[str]:
        """Find cache keys matching a pattern"""
        try:
            # This would require Redis SCAN functionality
            # For now, return empty list - would need to implement with RedisCache
            logger.warning(f"Pattern matching not implemented for pattern: {pattern}")
            return []
        except Exception as e:
            logger.error(f"Error finding keys by pattern {pattern}: {e}")
            return []

    async def bulk_invalidate_by_prefix(
        self, prefix: str, max_keys: Optional[int] = None
    ) -> InvalidationResult:
        """
        Convenience method for bulk invalidation by key prefix.

        Args:
            prefix: Key prefix to match
            max_keys: Maximum number of keys to invalidate (safety limit)

        Returns:
            InvalidationResult with operation details
        """
        start_time = time.time()

        try:
            # This would require implementing SCAN in RedisCache
            # For now, return empty result
            logger.warning(f"Prefix invalidation not implemented for prefix: {prefix}")

            return InvalidationResult(
                success=True,
                keys_invalidated=0,
                execution_time_ms=(time.time() - start_time) * 1000,
                strategy_used=self.get_strategy_name(),
                metadata={
                    "prefix": prefix,
                    "max_keys": max_keys,
                    "implementation_status": "not_implemented",
                },
            )

        except Exception as e:
            logger.error(f"Bulk prefix invalidation failed: {e}")
            return InvalidationResult(
                success=False,
                keys_invalidated=0,
                execution_time_ms=(time.time() - start_time) * 1000,
                strategy_used=self.get_strategy_name(),
                errors=[str(e)],
            )

    async def bulk_invalidate_by_tags(
        self, tags: List[str], operation: str = "union"
    ) -> InvalidationResult:
        """
        Bulk invalidate cache keys by tags.

        Args:
            tags: List of tags to match
            operation: "union" (any tag) or "intersection" (all tags)

        Returns:
            InvalidationResult with operation details
        """
        start_time = time.time()

        try:
            # This would require implementing tag-based caching
            # For now, return empty result
            logger.warning(f"Tag-based invalidation not implemented for tags: {tags}")

            return InvalidationResult(
                success=True,
                keys_invalidated=0,
                execution_time_ms=(time.time() - start_time) * 1000,
                strategy_used=self.get_strategy_name(),
                metadata={
                    "tags": tags,
                    "operation": operation,
                    "implementation_status": "not_implemented",
                },
            )

        except Exception as e:
            logger.error(f"Bulk tag invalidation failed: {e}")
            return InvalidationResult(
                success=False,
                keys_invalidated=0,
                execution_time_ms=(time.time() - start_time) * 1000,
                strategy_used=self.get_strategy_name(),
                errors=[str(e)],
            )


__all__ = [
    "BulkInvalidationStrategy",
]
