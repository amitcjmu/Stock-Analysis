"""
Dependency-aware cache invalidation strategies.

This module implements cache invalidation with dependency tracking including:
- Dependency graph management for cache keys
- Smart cascade invalidation based on dependencies
- Circular dependency detection and resolution
- Hierarchical invalidation patterns

Generated by CC (Claude Code)
"""

import time
from collections import defaultdict
from datetime import datetime
from typing import Dict, List, Optional

from app.constants.cache_keys import CacheKeys
from app.core.logging import get_logger

from .base import (
    BaseInvalidationStrategy,
    InvalidationEvent,
    InvalidationResult,
    InvalidationTrigger,
)

logger = get_logger(__name__)


class WritePatternInvalidationStrategy(BaseInvalidationStrategy):
    """
    Handles write-through and write-behind cache invalidation patterns:
    - Write-through: Critical data written to both cache and DB immediately
    - Write-behind: Non-critical data written to cache first, DB later
    - Selective patterns based on data criticality
    """

    def __init__(self, redis_cache):
        super().__init__(redis_cache)
        self.write_behind_queue: Dict[str, List[Dict[str, any]]] = defaultdict(list)
        self.write_behind_batch_size = 50
        self.write_behind_flush_interval = 30  # seconds
        self.last_flush_time = time.time()

        # Define critical data types that use write-through
        self.critical_data_types = {
            "user_session",
            "user_context",
            "security_event",
            "authentication",
            "permission_change",
        }

    def can_handle(self, event: InvalidationEvent) -> bool:
        """Handle data modification events that need write pattern coordination"""
        return event.trigger == InvalidationTrigger.DATA_MODIFICATION

    async def invalidate(self, event: InvalidationEvent) -> InvalidationResult:
        """Execute write pattern invalidation"""
        start_time = time.time()

        try:
            data_type = event.metadata.get("data_type", "unknown")

            if self._is_critical_data(data_type):
                return await self._handle_write_through(event)
            else:
                return await self._handle_write_behind(event)

        except Exception as e:
            logger.error(f"Write pattern invalidation failed: {e}")
            return InvalidationResult(
                success=False,
                keys_invalidated=0,
                execution_time_ms=(time.time() - start_time) * 1000,
                strategy_used=self.get_strategy_name(),
                errors=[str(e)],
            )

    async def _handle_write_through(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle write-through pattern for critical data"""
        start_time = time.time()

        # For write-through, we immediately invalidate cache to ensure consistency
        cache_keys = self._generate_cache_keys_for_data_type(event)
        keys_invalidated = 0

        for cache_key in cache_keys:
            try:
                deleted = await self.redis_cache.delete(cache_key)
                if deleted:
                    keys_invalidated += 1
                    logger.debug(f"Write-through invalidated: {cache_key}")
            except Exception as e:
                logger.error(f"Failed write-through invalidation for {cache_key}: {e}")

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=keys_invalidated,
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={
                "pattern": "write_through",
                "data_type": event.metadata.get("data_type"),
            },
        )

    async def _handle_write_behind(
        self, event: InvalidationEvent
    ) -> InvalidationResult:
        """Handle write-behind pattern for non-critical data"""
        start_time = time.time()

        # For write-behind, we queue the invalidation for later processing
        cache_keys = self._generate_cache_keys_for_data_type(event)

        for cache_key in cache_keys:
            self.write_behind_queue[cache_key].append(
                {
                    "event": event.to_dict(),
                    "timestamp": datetime.utcnow().isoformat(),
                    "operation": "invalidate",
                }
            )

        # Check if we should flush the write-behind queue
        await self._maybe_flush_write_behind_queue()

        execution_time = (time.time() - start_time) * 1000

        return InvalidationResult(
            success=True,
            keys_invalidated=len(cache_keys),  # Queued for invalidation
            execution_time_ms=execution_time,
            strategy_used=self.get_strategy_name(),
            metadata={"pattern": "write_behind", "queued_keys": len(cache_keys)},
        )

    async def _maybe_flush_write_behind_queue(self) -> None:
        """Flush write-behind queue if conditions are met"""
        current_time = time.time()

        # Check if enough time has passed or queue is large enough
        should_flush = (
            current_time - self.last_flush_time > self.write_behind_flush_interval
            or sum(len(operations) for operations in self.write_behind_queue.values())
            >= self.write_behind_batch_size
        )

        if should_flush:
            await self._flush_write_behind_queue()

    async def _flush_write_behind_queue(self) -> None:
        """Flush all queued write-behind operations"""
        if not self.write_behind_queue:
            return

        logger.info(
            f"Flushing write-behind queue with {len(self.write_behind_queue)} keys"
        )

        flushed_keys = 0

        for cache_key, operations in list(self.write_behind_queue.items()):
            try:
                # Process all operations for this key
                for operation in operations:
                    if operation["operation"] == "invalidate":
                        deleted = await self.redis_cache.delete(cache_key)
                        if deleted:
                            flushed_keys += 1

                # Clear processed operations
                del self.write_behind_queue[cache_key]

            except Exception as e:
                logger.error(
                    f"Failed to flush write-behind operations for {cache_key}: {e}"
                )

        self.last_flush_time = time.time()
        logger.info(f"Flushed {flushed_keys} keys from write-behind queue")

    def _is_critical_data(self, data_type: str) -> bool:
        """Check if data type is critical and requires write-through"""
        return data_type.lower() in self.critical_data_types

    def _generate_cache_keys_for_data_type(self, event: InvalidationEvent) -> List[str]:
        """Generate cache keys based on data type and event"""
        cache_keys = []
        data_type = event.metadata.get("data_type", "")

        if data_type == "user_session":
            cache_keys.append(CacheKeys.user_context(event.entity_id))

        elif data_type == "user_context":
            cache_keys.extend(
                [
                    CacheKeys.user_context(event.entity_id),
                    CacheKeys.user_clients(event.entity_id),
                ]
            )

        elif data_type == "client_data":
            cache_keys.extend(
                [
                    CacheKeys.client_engagements(event.entity_id),
                    CacheKeys.client_users(event.entity_id),
                    CacheKeys.client_settings(event.entity_id),
                ]
            )

        elif data_type == "engagement_data":
            if event.client_account_id:
                cache_keys.extend(
                    [
                        CacheKeys.client_engagements(event.client_account_id),
                        CacheKeys.client_flows(
                            event.client_account_id, event.entity_id
                        ),
                    ]
                )

        return cache_keys

    async def force_flush_write_behind_queue(self) -> int:
        """Force flush write-behind queue (for shutdown/testing)"""
        await self._flush_write_behind_queue()
        return 0  # Return number of keys flushed


class CacheVersioningInvalidationStrategy(BaseInvalidationStrategy):
    """
    Handles cache invalidation with versioning and data integrity:
    - Version-based invalidation to ensure consistency
    - Checksum validation for data integrity
    - Conflict resolution for simultaneous updates
    - Data repair for inconsistent cache data
    """

    def __init__(self, redis_cache):
        super().__init__(redis_cache)
        self.version_cache: Dict[str, str] = {}
        self.checksum_cache: Dict[str, str] = {}

    def can_handle(self, event: InvalidationEvent) -> bool:
        """Handle data modification events that need versioning"""
        return (
            event.trigger == InvalidationTrigger.DATA_MODIFICATION
            and event.metadata.get("use_versioning", False)
        )

    async def invalidate(self, event: InvalidationEvent) -> InvalidationResult:
        """Execute version-based invalidation"""
        start_time = time.time()

        try:
            cache_keys = self._generate_cache_keys_for_versioning(event)
            keys_invalidated = 0

            for cache_key in cache_keys:
                # Check version before invalidation
                current_version = await self._get_cache_version(cache_key)
                expected_version = event.metadata.get("expected_version")

                if expected_version and current_version != expected_version:
                    logger.warning(
                        f"Version mismatch for {cache_key}: expected {expected_version}, got {current_version}"
                    )
                    continue

                # Invalidate with version increment
                deleted = await self._invalidate_with_version_increment(cache_key)
                if deleted:
                    keys_invalidated += 1

            execution_time = (time.time() - start_time) * 1000

            return InvalidationResult(
                success=True,
                keys_invalidated=keys_invalidated,
                execution_time_ms=execution_time,
                strategy_used=self.get_strategy_name(),
                metadata={"versioning": True, "keys_processed": len(cache_keys)},
            )

        except Exception as e:
            logger.error(f"Version-based invalidation failed: {e}")
            return InvalidationResult(
                success=False,
                keys_invalidated=0,
                execution_time_ms=(time.time() - start_time) * 1000,
                strategy_used=self.get_strategy_name(),
                errors=[str(e)],
            )

    async def _get_cache_version(self, cache_key: str) -> Optional[str]:
        """Get current version of cache key"""
        version_key = f"{cache_key}:version"
        return await self.redis_cache.get(version_key)

    async def _invalidate_with_version_increment(self, cache_key: str) -> bool:
        """Invalidate cache key and increment version"""
        try:
            # Delete the actual cache data
            deleted = await self.redis_cache.delete(cache_key)

            # Increment version
            version_key = f"{cache_key}:version"
            current_version = await self.redis_cache.get(version_key) or "0"
            new_version = str(int(current_version) + 1)
            await self.redis_cache.set(version_key, new_version, ttl=86400)  # 24 hours

            return deleted

        except Exception as e:
            logger.error(
                f"Failed to invalidate with version increment for {cache_key}: {e}"
            )
            return False

    def _generate_cache_keys_for_versioning(
        self, event: InvalidationEvent
    ) -> List[str]:
        """Generate cache keys that need versioning"""
        # Use similar logic to other strategies but for version-critical data
        cache_keys = []

        if event.entity_type == "user":
            cache_keys.extend(
                [
                    CacheKeys.user_context(event.entity_id),
                    CacheKeys.user_defaults(event.entity_id),
                ]
            )

        elif event.entity_type == "client":
            cache_keys.extend(
                [
                    CacheKeys.client_settings(event.entity_id),
                ]
            )

        return cache_keys


__all__ = [
    "WritePatternInvalidationStrategy",
    "CacheVersioningInvalidationStrategy",
]
