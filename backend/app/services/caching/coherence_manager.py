"""
Cache Coherence Manager for AI Force Migration Platform

This module implements a sophisticated cache coherence protocol that ensures
data consistency across the distributed Redis cache while maintaining high
performance and supporting complex invalidation patterns.

ðŸ”’ Security: Multi-tenant isolation and secure invalidation
âš¡ Performance: Optimized batch operations and minimal DB queries
ðŸŽ¯ Coherence: Cascade invalidation with cycle detection
ðŸ“Š Analytics: Comprehensive invalidation tracking and metrics

Generated by CC (Claude Code)
"""

import asyncio
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Set

# from typing import Tuple, Union  # Unused
from collections import defaultdict

# from collections import deque  # Unused

from sqlalchemy import and_, or_, select, update

# from sqlalchemy import delete  # Unused
from sqlalchemy.ext.asyncio import AsyncSession

# from sqlalchemy.orm import selectinload  # Unused

from app.constants.cache_keys import CacheKeys, CACHE_VERSION

# from app.constants.cache_keys import CacheKeyType  # Unused
from app.core.database import get_db
from app.core.logging import get_logger
from app.models.cache_metadata import (
    CacheMetadata,
    CacheInvalidationLog,
    # CachePerformanceLog,  # Unused
    # CacheConfiguration,  # Unused
)
from app.services.caching.redis_cache import RedisCache

logger = get_logger(__name__)


class CacheCoherenceError(Exception):
    """Raised when cache coherence operations fail."""

    pass


class InvalidationContext:
    """Context object to track invalidation operations and prevent cycles."""

    def __init__(
        self,
        trigger_entity_type: str,
        trigger_entity_id: str,
        user_id: Optional[str] = None,
        endpoint: Optional[str] = None,
    ):
        self.trigger_entity_type = trigger_entity_type
        self.trigger_entity_id = trigger_entity_id
        self.user_id = user_id
        self.endpoint = endpoint
        self.start_time = time.time()
        self.invalidated_keys: Set[str] = set()
        self.cascade_depth = 0
        self.max_cascade_depth = 10  # Prevent infinite loops
        self.invalidation_logs: List[Dict] = []

    def add_invalidated_key(self, cache_key: str, invalidation_type: str = "CASCADE"):
        """Track an invalidated cache key."""
        self.invalidated_keys.add(cache_key)
        self.invalidation_logs.append(
            {
                "cache_key": cache_key,
                "invalidation_type": invalidation_type,
                "cascade_depth": self.cascade_depth,
                "timestamp": datetime.utcnow(),
            }
        )

    def can_cascade(self) -> bool:
        """Check if cascade invalidation can continue."""
        return self.cascade_depth < self.max_cascade_depth

    def increment_cascade_depth(self):
        """Increment cascade depth for tracking."""
        self.cascade_depth += 1

    @property
    def total_invalidated(self) -> int:
        """Get total number of invalidated keys."""
        return len(self.invalidated_keys)

    @property
    def elapsed_time_ms(self) -> float:
        """Get elapsed time since invalidation started."""
        return (time.time() - self.start_time) * 1000


class CacheCoherenceManager:
    """
    Manages cache coherence across the distributed Redis cache system.

    Key Features:
    - Cascade invalidation with cycle detection
    - Batch operations for performance
    - Multi-tenant isolation
    - Comprehensive audit logging
    - Performance monitoring
    - Configurable invalidation policies
    """

    def __init__(self, redis_cache: RedisCache, db: AsyncSession):
        self.redis = redis_cache
        self.db = db
        self.stats = {
            "total_invalidations": 0,
            "cascade_invalidations": 0,
            "batch_invalidations": 0,
            "errors": 0,
        }

        # Cache for frequently accessed invalidation patterns
        self._pattern_cache: Dict[str, List[str]] = {}
        self._cache_metadata_lookup: Dict[str, CacheMetadata] = {}

    async def invalidate_cascade(
        self,
        entity_type: str,
        entity_id: str,
        client_account_id: str,
        trigger_operation: str = "UPDATE",
        user_id: Optional[str] = None,
        endpoint: Optional[str] = None,
        reason: Optional[str] = None,
    ) -> InvalidationContext:
        """
        Perform cascade invalidation for an entity change.

        Args:
            entity_type: Type of entity that changed (user, flow, import, etc.)
            entity_id: ID of the entity that changed
            client_account_id: Client account for tenant isolation
            trigger_operation: Operation that triggered invalidation
            user_id: User who initiated the operation
            endpoint: API endpoint that triggered the change
            reason: Human-readable reason for invalidation

        Returns:
            InvalidationContext with details of what was invalidated
        """
        context = InvalidationContext(entity_type, entity_id, user_id, endpoint)

        try:
            logger.info(
                f"Starting cascade invalidation for {entity_type}:{entity_id} "
                f"in client {client_account_id}"
            )

            # Get all cache entries related to this entity
            affected_entries = await self._find_affected_cache_entries(
                entity_type, entity_id, client_account_id
            )

            if not affected_entries:
                logger.debug(f"No cache entries found for {entity_type}:{entity_id}")
                return context

            # Build invalidation tree to optimize batch operations
            invalidation_tree = await self._build_invalidation_tree(
                affected_entries, context
            )

            # Execute invalidation in optimal order (bottom-up to minimize cascades)
            await self._execute_invalidation_tree(invalidation_tree, context)

            # Log the cascade invalidation
            await self._log_cascade_invalidation(
                context, client_account_id, trigger_operation, reason
            )

            self.stats["total_invalidations"] += 1
            self.stats["cascade_invalidations"] += context.total_invalidated

            logger.info(
                f"Cascade invalidation completed: {context.total_invalidated} keys "
                f"invalidated in {context.elapsed_time_ms:.2f}ms"
            )

        except Exception as e:
            self.stats["errors"] += 1
            logger.error(f"Cascade invalidation failed: {e}")
            raise CacheCoherenceError(f"Cascade invalidation failed: {e}")

        return context

    async def invalidate_pattern(
        self,
        key_pattern: str,
        client_account_id: str,
        invalidation_type: str = "PATTERN",
        user_id: Optional[str] = None,
        reason: Optional[str] = None,
    ) -> int:
        """
        Invalidate all cache keys matching a pattern within a tenant.

        Args:
            key_pattern: Redis key pattern (supports wildcards)
            client_account_id: Client account for tenant isolation
            invalidation_type: Type of invalidation for logging
            user_id: User who initiated the operation
            reason: Human-readable reason

        Returns:
            Number of keys invalidated
        """
        start_time = time.time()

        try:
            # Ensure pattern is tenant-scoped for security
            if client_account_id not in key_pattern:
                logger.warning(
                    f"Pattern '{key_pattern}' doesn't include client_account_id, "
                    f"adding tenant scope"
                )
                key_pattern = f"{key_pattern}:client:{client_account_id}:*"

            # Get matching cache metadata entries
            stmt = select(CacheMetadata).where(
                and_(
                    CacheMetadata.client_account_id == client_account_id,
                    CacheMetadata.is_active == True,  # noqa: E712
                    CacheMetadata.cache_key.like(key_pattern.replace("*", "%")),
                )
            )
            result = await self.db.execute(stmt)
            matching_entries = result.scalars().all()

            if not matching_entries:
                logger.debug(f"No cache entries found for pattern: {key_pattern}")
                return 0

            # Batch invalidate in Redis
            cache_keys = [entry.cache_key for entry in matching_entries]
            invalidated_count = await self._batch_invalidate_redis_keys(cache_keys)

            # Update database entries
            await self._deactivate_cache_entries(
                [entry.id for entry in matching_entries]
            )

            # Log the pattern invalidation
            await self._log_pattern_invalidation(
                key_pattern,
                invalidated_count,
                client_account_id,
                invalidation_type,
                user_id,
                reason,
            )

            elapsed_ms = (time.time() - start_time) * 1000
            self.stats["batch_invalidations"] += 1

            logger.info(
                f"Pattern invalidation completed: {invalidated_count} keys "
                f"invalidated for pattern '{key_pattern}' in {elapsed_ms:.2f}ms"
            )

            return invalidated_count

        except Exception as e:
            self.stats["errors"] += 1
            logger.error(f"Pattern invalidation failed: {e}")
            raise CacheCoherenceError(f"Pattern invalidation failed: {e}")

    async def invalidate_by_tags(
        self,
        tags: List[str],
        client_account_id: str,
        user_id: Optional[str] = None,
        reason: Optional[str] = None,
    ) -> int:
        """
        Invalidate all cache entries with specific tags within a tenant.

        Args:
            tags: List of cache tags to match
            client_account_id: Client account for tenant isolation
            user_id: User who initiated the operation
            reason: Human-readable reason

        Returns:
            Number of keys invalidated
        """
        try:
            # Find cache entries with matching tags
            stmt = select(CacheMetadata).where(
                and_(
                    CacheMetadata.client_account_id == client_account_id,
                    CacheMetadata.is_active == True,  # noqa: E712
                    CacheMetadata.cache_tags.op("@>")(
                        tags
                    ),  # PostgreSQL array contains
                )
            )
            result = await self.db.execute(stmt)
            matching_entries = result.scalars().all()

            if not matching_entries:
                return 0

            # Batch invalidate
            cache_keys = [entry.cache_key for entry in matching_entries]
            invalidated_count = await self._batch_invalidate_redis_keys(cache_keys)

            # Update database
            await self._deactivate_cache_entries(
                [entry.id for entry in matching_entries]
            )

            # Log the tag invalidation
            await self._log_pattern_invalidation(
                f"tags:{','.join(tags)}",
                invalidated_count,
                client_account_id,
                "TAG",
                user_id,
                reason,
            )

            return invalidated_count

        except Exception as e:
            logger.error(f"Tag invalidation failed: {e}")
            raise CacheCoherenceError(f"Tag invalidation failed: {e}")

    async def cleanup_expired_entries(self, batch_size: int = 1000) -> int:
        """
        Clean up expired cache entries from both Redis and database.

        Args:
            batch_size: Number of entries to process per batch

        Returns:
            Number of entries cleaned up
        """
        total_cleaned = 0

        try:
            while True:
                # Get batch of expired entries
                stmt = (
                    select(CacheMetadata)
                    .where(
                        and_(
                            CacheMetadata.is_active == True,  # noqa: E712
                            CacheMetadata.expires_at <= datetime.utcnow(),
                        )
                    )
                    .limit(batch_size)
                )

                result = await self.db.execute(stmt)
                expired_entries = result.scalars().all()

                if not expired_entries:
                    break

                # Remove from Redis
                cache_keys = [entry.cache_key for entry in expired_entries]
                await self._batch_invalidate_redis_keys(cache_keys)

                # Deactivate in database
                entry_ids = [entry.id for entry in expired_entries]
                await self._deactivate_cache_entries(entry_ids)

                # Log cleanup
                await self._log_bulk_expiration(expired_entries)

                total_cleaned += len(expired_entries)

                # Short pause to avoid overwhelming the system
                await asyncio.sleep(0.1)

            if total_cleaned > 0:
                logger.info(f"Cleaned up {total_cleaned} expired cache entries")

        except Exception as e:
            logger.error(f"Cache cleanup failed: {e}")
            raise CacheCoherenceError(f"Cache cleanup failed: {e}")

        return total_cleaned

    async def get_coherence_metrics(
        self, client_account_id: Optional[str] = None, hours_back: int = 24
    ) -> Dict:
        """
        Get cache coherence metrics for monitoring and optimization.

        Args:
            client_account_id: Specific client to analyze (None for platform-wide)
            hours_back: How many hours of data to analyze

        Returns:
            Dictionary with coherence metrics
        """
        since_time = datetime.utcnow() - timedelta(hours=hours_back)

        try:
            base_conditions = [CacheInvalidationLog.created_at >= since_time]
            if client_account_id:
                base_conditions.append(
                    CacheInvalidationLog.client_account_id == client_account_id
                )

            # Count invalidations by type
            stmt = select(
                CacheInvalidationLog.invalidation_type,
                CacheInvalidationLog.keys_invalidated_count,
            ).where(and_(*base_conditions))

            result = await self.db.execute(stmt)
            invalidation_data = result.fetchall()

            # Aggregate metrics
            metrics = {
                "timeframe_hours": hours_back,
                "total_invalidations": 0,
                "total_keys_invalidated": 0,
                "invalidation_by_type": defaultdict(int),
                "cascade_depth_distribution": defaultdict(int),
                "average_invalidation_time_ms": 0,
                "client_account_id": client_account_id,
            }

            # total_time = 0  # Unused
            # count = 0  # Unused

            for row in invalidation_data:
                inv_type, keys_count = row
                metrics["total_invalidations"] += 1
                metrics["total_keys_invalidated"] += keys_count or 1
                metrics["invalidation_by_type"][inv_type] += 1

            # Get cascade depth distribution
            depth_stmt = select(CacheInvalidationLog.cascade_depth).where(
                and_(*base_conditions, CacheInvalidationLog.cascade_depth > 0)
            )

            result = await self.db.execute(depth_stmt)
            depths = result.scalars().all()

            for depth in depths:
                metrics["cascade_depth_distribution"][depth] += 1

            # Get average invalidation time
            time_stmt = select(CacheInvalidationLog.invalidation_time_ms).where(
                and_(
                    *base_conditions,
                    CacheInvalidationLog.invalidation_time_ms.isnot(None),
                )
            )

            result = await self.db.execute(time_stmt)
            times = result.scalars().all()

            if times:
                metrics["average_invalidation_time_ms"] = sum(times) / len(times)

            # Add current cache statistics
            active_cache_stmt = select(CacheMetadata).where(
                and_(
                    CacheMetadata.is_active == True,  # noqa: E712
                    (
                        CacheMetadata.client_account_id == client_account_id
                        if client_account_id
                        else True
                    ),
                )
            )

            result = await self.db.execute(active_cache_stmt)
            active_entries = result.scalars().all()

            metrics["active_cache_entries"] = len(active_entries)
            metrics["cache_memory_usage_mb"] = sum(
                entry.data_size_bytes or 0 for entry in active_entries
            ) / (1024 * 1024)

            return dict(metrics)

        except Exception as e:
            logger.error(f"Failed to get coherence metrics: {e}")
            return {"error": str(e)}

    # ========================================
    # PRIVATE HELPER METHODS
    # ========================================

    async def _find_affected_cache_entries(
        self, entity_type: str, entity_id: str, client_account_id: str
    ) -> List[CacheMetadata]:
        """Find all cache entries that should be invalidated for an entity change."""

        # Direct entity matches
        direct_conditions = [
            CacheMetadata.client_account_id == client_account_id,
            CacheMetadata.is_active == True,  # noqa: E712
            or_(
                and_(
                    CacheMetadata.entity_type == entity_type,
                    CacheMetadata.entity_id == entity_id,
                ),
                # Also match cache keys that contain the entity
                CacheMetadata.cache_key.like(f"%:{entity_type}:{entity_id}:%"),
                CacheMetadata.cache_key.like(f"%:{entity_type}:{entity_id}"),
            ),
        ]

        stmt = select(CacheMetadata).where(and_(*direct_conditions))
        result = await self.db.execute(stmt)
        direct_matches = result.scalars().all()

        # Use cascade patterns from CacheKeys for additional matches
        pattern_matches = []
        for entry in direct_matches:
            patterns = CacheKeys.get_cascade_patterns(entry.cache_key)
            for pattern in patterns:
                pattern_stmt = select(CacheMetadata).where(
                    and_(
                        CacheMetadata.client_account_id == client_account_id,
                        CacheMetadata.is_active == True,  # noqa: E712
                        CacheMetadata.cache_key.like(pattern.replace("*", "%")),
                    )
                )
                result = await self.db.execute(pattern_stmt)
                pattern_matches.extend(result.scalars().all())

        # Combine and deduplicate
        all_matches = {entry.id: entry for entry in direct_matches + pattern_matches}
        return list(all_matches.values())

    async def _build_invalidation_tree(
        self, cache_entries: List[CacheMetadata], context: InvalidationContext
    ) -> Dict[int, List[CacheMetadata]]:
        """Build a tree of cache dependencies for optimal invalidation order."""

        # Group by cascade depth (calculated from cache key structure)
        tree = defaultdict(list)

        for entry in cache_entries:
            # Calculate depth based on cache key structure
            depth = entry.cache_key.count(":") - 2  # Subtract version and first entity
            depth = max(0, min(depth, 5))  # Limit depth to reasonable range
            tree[depth].append(entry)

        return dict(tree)

    async def _execute_invalidation_tree(
        self,
        invalidation_tree: Dict[int, List[CacheMetadata]],
        context: InvalidationContext,
    ):
        """Execute invalidation tree in optimal order."""

        # Invalidate from highest depth to lowest (children before parents)
        for depth in sorted(invalidation_tree.keys(), reverse=True):
            entries = invalidation_tree[depth]
            context.cascade_depth = depth

            if not context.can_cascade():
                logger.warning(f"Max cascade depth reached, stopping at depth {depth}")
                break

            # Batch invalidate Redis keys
            cache_keys = [entry.cache_key for entry in entries]
            await self._batch_invalidate_redis_keys(cache_keys)

            # Update database entries
            entry_ids = [entry.id for entry in entries]
            await self._deactivate_cache_entries(entry_ids)

            # Track in context
            for cache_key in cache_keys:
                context.add_invalidated_key(cache_key, "CASCADE")

            context.increment_cascade_depth()

    async def _batch_invalidate_redis_keys(self, cache_keys: List[str]) -> int:
        """Batch invalidate keys in Redis for performance."""
        if not cache_keys:
            return 0

        try:
            # Use Redis pipeline for batch operations
            if hasattr(self.redis, "delete_batch"):
                return await self.redis.delete_batch(cache_keys)
            else:
                # Fallback to individual deletes
                count = 0
                for key in cache_keys:
                    deleted = await self.redis.delete(key)
                    if deleted:
                        count += 1
                return count
        except Exception as e:
            logger.error(f"Batch Redis invalidation failed: {e}")
            raise

    async def _deactivate_cache_entries(self, entry_ids: List[int]):
        """Mark cache entries as inactive in the database."""
        if not entry_ids:
            return

        try:
            stmt = (
                update(CacheMetadata)
                .where(CacheMetadata.id.in_(entry_ids))
                .values(is_active=False, updated_at=datetime.utcnow())
            )

            await self.db.execute(stmt)
            await self.db.commit()

        except Exception as e:
            await self.db.rollback()
            logger.error(f"Failed to deactivate cache entries: {e}")
            raise

    async def _log_cascade_invalidation(
        self,
        context: InvalidationContext,
        client_account_id: str,
        trigger_operation: str,
        reason: Optional[str],
    ):
        """Log cascade invalidation for audit and analytics."""

        try:
            log_entry = CacheInvalidationLog(
                client_account_id=client_account_id,
                invalidation_type="CASCADE",
                cache_key_pattern=f"{context.trigger_entity_type}:{context.trigger_entity_id}",
                keys_invalidated_count=context.total_invalidated,
                trigger_entity_type=context.trigger_entity_type,
                trigger_entity_id=context.trigger_entity_id,
                trigger_operation=trigger_operation,
                user_id=context.user_id,
                endpoint=context.endpoint,
                invalidation_time_ms=context.elapsed_time_ms,
                cascade_depth=context.cascade_depth,
                reason=reason,
                metadata={
                    "invalidated_keys": list(context.invalidated_keys)[
                        :100
                    ],  # Limit size
                    "cascade_steps": len(context.invalidation_logs),
                },
            )

            self.db.add(log_entry)
            await self.db.commit()

        except Exception as e:
            await self.db.rollback()
            logger.error(f"Failed to log cascade invalidation: {e}")

    async def _log_pattern_invalidation(
        self,
        pattern: str,
        count: int,
        client_account_id: str,
        invalidation_type: str,
        user_id: Optional[str],
        reason: Optional[str],
    ):
        """Log pattern invalidation for audit."""

        try:
            log_entry = CacheInvalidationLog(
                client_account_id=client_account_id,
                invalidation_type=invalidation_type,
                cache_key_pattern=pattern,
                keys_invalidated_count=count,
                user_id=user_id,
                reason=reason,
                cascade_depth=0,
            )

            self.db.add(log_entry)
            await self.db.commit()

        except Exception as e:
            await self.db.rollback()
            logger.error(f"Failed to log pattern invalidation: {e}")

    async def _log_bulk_expiration(self, expired_entries: List[CacheMetadata]):
        """Log bulk expiration cleanup."""

        # Group by client for efficient logging
        by_client = defaultdict(list)
        for entry in expired_entries:
            by_client[entry.client_account_id].append(entry)

        try:
            for client_id, entries in by_client.items():
                log_entry = CacheInvalidationLog(
                    client_account_id=client_id,
                    invalidation_type="EXPIRATION",
                    cache_key_pattern="bulk_cleanup",
                    keys_invalidated_count=len(entries),
                    reason="Automatic cleanup of expired cache entries",
                    cascade_depth=0,
                )

                self.db.add(log_entry)

            await self.db.commit()

        except Exception as e:
            await self.db.rollback()
            logger.error(f"Failed to log bulk expiration: {e}")


# Factory function for dependency injection
async def get_cache_coherence_manager(
    redis_cache: RedisCache = None, db: AsyncSession = None
) -> CacheCoherenceManager:
    """
    Factory function to create CacheCoherenceManager with dependencies.
    """
    if redis_cache is None:
        from app.services.caching.redis_cache import get_redis_cache

        redis_cache = await get_redis_cache()

    if db is None:
        async for database in get_db():
            db = database
            break

    return CacheCoherenceManager(redis_cache, db)


# Event handlers for common invalidation scenarios
class CacheInvalidationService:
    """
    High-level service for common cache invalidation patterns.
    Integrates with the CacheCoherenceManager to provide simple APIs.
    """

    def __init__(self, coherence_manager: CacheCoherenceManager):
        self.coherence = coherence_manager

    async def on_user_updated(self, user_id: str, client_account_id: str):
        """Invalidate when user roles/permissions change."""
        await self.coherence.invalidate_cascade(
            "user",
            user_id,
            client_account_id,
            trigger_operation="UPDATE",
            reason="User roles or permissions updated",
        )

    async def on_user_client_access_changed(self, user_id: str, client_id: str):
        """Invalidate when user gains/loses client access."""
        await self.coherence.invalidate_pattern(
            f"{CACHE_VERSION}:*:user:{user_id}:*",
            client_id,
            reason="User client access changed",
        )

    async def on_engagement_modified(self, engagement_id: str, client_id: str):
        """Invalidate when engagement is created/updated/deleted."""
        await self.coherence.invalidate_cascade(
            "engagement",
            engagement_id,
            client_id,
            trigger_operation="UPDATE",
            reason="Engagement modified",
        )

    async def on_flow_updated(self, flow_id: str, client_id: str):
        """Invalidate when flow state changes."""
        await self.coherence.invalidate_cascade(
            "flow",
            flow_id,
            client_id,
            trigger_operation="UPDATE",
            reason="Flow state updated",
        )

    async def on_field_mapping_bulk_operation(self, import_id: str, client_id: str):
        """Invalidate field mappings after bulk approval/rejection."""
        await self.coherence.invalidate_pattern(
            f"{CACHE_VERSION}:*:*:{import_id}:*",
            client_id,
            reason="Field mapping bulk operation",
        )

    async def on_user_defaults_updated(
        self,
        user_id: str,
        client_id: str,
        default_client_id: Optional[str] = None,
        default_engagement_id: Optional[str] = None,
    ):
        """Invalidate when user's default client/engagement is updated."""
        await self.coherence.invalidate_cascade(
            "user",
            user_id,
            client_id,
            trigger_operation="UPDATE",
            reason="User defaults updated",
        )


__all__ = [
    "CacheCoherenceManager",
    "CacheInvalidationService",
    "InvalidationContext",
    "CacheCoherenceError",
    "get_cache_coherence_manager",
]
