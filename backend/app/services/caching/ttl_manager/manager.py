"""
Main TTL Manager class for dynamic cache management.

This module contains the primary TTLManager class that orchestrates
all TTL management functionality.

Generated by CC (Claude Code)
"""

import time
from typing import Any, Callable, Dict, List, Optional

from app.core.config import settings
from app.core.logging import get_logger
from app.services.caching.redis_cache import RedisCache
from app.services.shared_state_manager import SharedStateManager, get_ttl_state_manager

from .base import (
    TTLStrategy,
    RefreshPriority,
    CacheAccessPattern,
    TTLRecommendation,
    TTLMetrics,
)
from .strategies import TTLStrategies
from .expiry import RefreshTaskScheduler, BackgroundProcessor
from .utils import evict_old_patterns, warm_cache_keys, validate_ttl_recommendation

logger = get_logger(__name__)


class TTLManager:
    """
    Dynamic TTL manager that optimizes cache performance through intelligent
    TTL calculation and background refresh scheduling.

    Features:
    - Multiple TTL calculation strategies (static, adaptive, predictive)
    - Background refresh scheduling with priority queues
    - Cache warming for frequently accessed data
    - System load-aware TTL adjustments
    - Comprehensive metrics and monitoring
    """

    def __init__(
        self,
        redis_cache: RedisCache,
        shared_state_manager: Optional[SharedStateManager] = None,
    ):
        self.redis_cache = redis_cache
        self.metrics = TTLMetrics()

        # Access pattern tracking with shared state support
        self.access_patterns: Dict[str, CacheAccessPattern] = {}
        self.max_tracked_patterns = settings.get("CACHE_MAX_TRACKED_PATTERNS", 10000)
        self._shared_state = shared_state_manager or get_ttl_state_manager()

        # Initialize components
        self.strategies = TTLStrategies(self.access_patterns)
        self.scheduler = RefreshTaskScheduler(
            max_concurrent_refreshes=settings.get("CACHE_MAX_CONCURRENT_REFRESHES", 5)
        )
        self.scheduler.metrics = self.metrics
        self.background_processor = BackgroundProcessor(self.scheduler)

        # TTL strategy configuration
        self.default_strategy = TTLStrategy.HYBRID

        logger.info("TTLManager initialized")

    async def _get_access_pattern(self, cache_key: str) -> Optional[CacheAccessPattern]:
        """Get access pattern from shared storage or in-memory fallback"""
        if self._shared_state:
            try:
                pattern_data = await self._shared_state.get(
                    f"access_pattern:{cache_key}"
                )
                if pattern_data:
                    # Deserialize access pattern data
                    from collections import deque
                    from datetime import datetime

                    pattern = CacheAccessPattern(cache_key=cache_key)
                    pattern.total_accesses = pattern_data.get("total_accesses", 0)
                    pattern.hits = pattern_data.get("hits", 0)
                    pattern.misses = pattern_data.get("misses", 0)
                    pattern.last_access = (
                        datetime.fromisoformat(pattern_data["last_access"])
                        if pattern_data.get("last_access")
                        else None
                    )
                    pattern.creation_time = (
                        datetime.fromisoformat(pattern_data["creation_time"])
                        if pattern_data.get("creation_time")
                        else datetime.utcnow()
                    )
                    pattern.average_access_interval_seconds = pattern_data.get(
                        "average_access_interval_seconds", 0
                    )

                    # Restore access times (limited to recent ones)
                    access_times_iso = pattern_data.get("access_times", [])
                    pattern.access_times = deque(
                        [
                            datetime.fromisoformat(t) for t in access_times_iso[-50:]
                        ],  # Keep last 50 for performance
                        maxlen=100,
                    )

                    return pattern
            except Exception as e:
                logger.warning(f"Failed to get access pattern from shared storage: {e}")

        # Fallback to in-memory storage
        return self.access_patterns.get(cache_key)

    async def _set_access_pattern(self, cache_key: str, pattern: CacheAccessPattern):
        """Set access pattern in shared storage and in-memory fallback"""
        # Always update in-memory fallback
        self.access_patterns[cache_key] = pattern

        if self._shared_state:
            try:
                # Serialize access pattern data
                pattern_data = {
                    "total_accesses": pattern.total_accesses,
                    "hits": pattern.hits,
                    "misses": pattern.misses,
                    "last_access": (
                        pattern.last_access.isoformat() if pattern.last_access else None
                    ),
                    "creation_time": pattern.creation_time.isoformat(),
                    "average_access_interval_seconds": pattern.average_access_interval_seconds,
                    "access_times": [
                        t.isoformat() for t in list(pattern.access_times)[-20:]
                    ],  # Keep last 20 for storage efficiency
                }
                await self._shared_state.set(
                    f"access_pattern:{cache_key}", pattern_data, ttl=86400
                )  # 24 hour TTL
            except Exception as e:
                logger.warning(f"Failed to set access pattern in shared storage: {e}")

    async def start(self) -> None:
        """Start the TTL manager background processing"""
        await self.background_processor.start()
        logger.info("TTLManager background processing started")

    async def stop(self) -> None:
        """Stop the TTL manager"""
        await self.background_processor.stop()
        logger.info("TTLManager stopped")

    def calculate_optimal_ttl(
        self,
        cache_key: str,
        data_type: Optional[str] = None,
        strategy: Optional[TTLStrategy] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> TTLRecommendation:
        """
        Calculate optimal TTL for a cache key using specified or default strategy.

        Args:
            cache_key: The cache key to calculate TTL for
            data_type: Type of data being cached (for static strategy)
            strategy: TTL calculation strategy to use
            metadata: Additional metadata for TTL calculation

        Returns:
            TTL recommendation with reasoning
        """
        start_time = time.time()
        strategy = strategy or self.default_strategy
        metadata = metadata or {}

        try:
            if strategy == TTLStrategy.STATIC:
                recommendation = self.strategies.calculate_static_ttl(
                    cache_key, data_type, metadata
                )
            elif strategy == TTLStrategy.ADAPTIVE:
                recommendation = self.strategies.calculate_adaptive_ttl(
                    cache_key, metadata
                )
            elif strategy == TTLStrategy.PREDICTIVE:
                recommendation = self.strategies.calculate_predictive_ttl(
                    cache_key, metadata
                )
            elif strategy == TTLStrategy.SYSTEM_LOAD_AWARE:
                recommendation = self.strategies.calculate_system_aware_ttl(
                    cache_key, metadata
                )
            elif strategy == TTLStrategy.HYBRID:
                recommendation = self.strategies.calculate_hybrid_ttl(
                    cache_key, data_type, metadata
                )
            else:
                # Fallback to static
                recommendation = self.strategies.calculate_static_ttl(
                    cache_key, data_type, metadata
                )

            # Validate recommendation
            if not validate_ttl_recommendation(recommendation):
                logger.warning(
                    f"Invalid TTL recommendation for {cache_key}, using fallback"
                )
                recommendation = self._get_fallback_recommendation(cache_key)

            # Update metrics
            processing_time_ms = (time.time() - start_time) * 1000
            self.metrics.update_calculation(strategy, processing_time_ms)

            return recommendation

        except Exception as e:
            logger.error(f"TTL calculation failed for {cache_key}: {e}")
            # Return safe default
            return self._get_fallback_recommendation(cache_key)

    async def record_cache_access(self, cache_key: str, was_hit: bool) -> None:
        """Record cache access for pattern tracking"""
        pattern = await self._get_access_pattern(cache_key)

        if pattern is None:
            # Check if we need to evict old patterns to save memory
            if len(self.access_patterns) >= self.max_tracked_patterns:
                evict_old_patterns(self.access_patterns, self.max_tracked_patterns)
            pattern = CacheAccessPattern(cache_key=cache_key)

        pattern.record_access(was_hit)
        await self._set_access_pattern(cache_key, pattern)

    def schedule_background_refresh(
        self,
        cache_key: str,
        refresh_callback: Callable[[str], Any],
        priority: RefreshPriority = RefreshPriority.MEDIUM,
        delay_seconds: int = 0,
    ) -> str:
        """
        Schedule background refresh for a cache key.

        Args:
            cache_key: Key to refresh
            refresh_callback: Function to call for refresh
            priority: Refresh priority
            delay_seconds: Delay before refresh

        Returns:
            Task ID for tracking
        """
        return self.scheduler.schedule_refresh(
            cache_key, refresh_callback, priority, delay_seconds
        )

    def schedule_predictive_refresh(
        self, cache_key: str, refresh_callback: Callable[[str], Any]
    ) -> Optional[str]:
        """Schedule predictive refresh based on access patterns"""
        return self.scheduler.schedule_predictive_refresh(
            cache_key, refresh_callback, self.access_patterns
        )

    def warm_cache(
        self, cache_keys: List[str], warm_callback: Callable[[List[str]], None]
    ) -> None:
        """Warm cache for frequently accessed keys"""
        warm_cache_keys(cache_keys, self.access_patterns, warm_callback)

    async def get_access_pattern(self, cache_key: str) -> Optional[CacheAccessPattern]:
        """Get access pattern for a cache key"""
        return await self._get_access_pattern(cache_key)

    def get_metrics(self) -> Dict[str, Any]:
        """Get TTL management metrics"""
        return {
            "total_ttl_calculations": self.metrics.total_ttl_calculations,
            "average_processing_time_ms": self.metrics.average_processing_time_ms,
            "strategy_usage": {
                strategy.value: count
                for strategy, count in self.metrics.strategy_usage.items()
            },
            "refresh_tasks": {
                "scheduled": self.metrics.refresh_tasks_scheduled,
                "completed": self.metrics.refresh_tasks_completed,
                "failed": self.metrics.refresh_tasks_failed,
                "success_rate": (
                    self.metrics.refresh_tasks_completed
                    / max(self.metrics.refresh_tasks_scheduled, 1)
                    * 100
                ),
            },
            "cache_optimization": {
                "tracked_patterns": len(self.access_patterns),
                "hot_data_keys": len(
                    [p for p in self.access_patterns.values() if p.is_hot_data]
                ),
                "cold_data_keys": len(
                    [p for p in self.access_patterns.values() if p.is_cold_data]
                ),
                "hits_prevented": self.metrics.cache_hits_prevented,
            },
            "refresh_queues": self.scheduler.get_queue_stats(),
            "active_refresh_tasks": len(self.scheduler.active_refresh_tasks),
            "last_reset": self.metrics.last_reset.isoformat(),
        }

    def reset_metrics(self) -> None:
        """Reset TTL metrics"""
        self.metrics = TTLMetrics()
        self.scheduler.metrics = self.metrics
        logger.info("TTL metrics reset")

    async def health_check(self) -> Dict[str, Any]:
        """Perform health check on TTL manager"""
        processor_health = await self.background_processor.health_check()

        health = {
            "status": "healthy",
            "tracked_patterns": len(self.access_patterns),
            "max_tracked_patterns": self.max_tracked_patterns,
            "issues": [],
        }

        # Merge processor health
        health.update(processor_health)

        # Check for issues
        if not health["background_processing_active"]:
            health["issues"].append("Background processing not active")
            health["status"] = "degraded"

        if health["tracked_patterns"] >= health["max_tracked_patterns"] * 0.9:
            health["issues"].append("Approaching maximum tracked patterns limit")

        if health["active_refresh_tasks"] >= health["max_concurrent_refreshes"]:
            health["issues"].append("At maximum concurrent refresh limit")

        if health["issues"]:
            health["status"] = (
                "warning" if health["status"] == "healthy" else health["status"]
            )

        return health

    def _get_fallback_recommendation(self, cache_key: str) -> TTLRecommendation:
        """Get a safe fallback TTL recommendation"""
        return TTLRecommendation(
            cache_key=cache_key,
            recommended_ttl_seconds=600,  # 10 minutes default
            strategy_used=TTLStrategy.STATIC,
            confidence=0.5,
            reasoning="Fallback TTL due to calculation error",
        )


# Factory function for dependency injection
def create_ttl_manager(
    redis_cache: RedisCache, shared_state_manager: Optional[SharedStateManager] = None
) -> TTLManager:
    """Create TTLManager with dependencies"""
    return TTLManager(redis_cache, shared_state_manager)


# Singleton instance for global access
_ttl_manager: Optional[TTLManager] = None


def get_ttl_manager() -> Optional[TTLManager]:
    """Get singleton TTLManager instance"""
    return _ttl_manager


def set_ttl_manager(manager: TTLManager) -> None:
    """Set singleton TTLManager instance"""
    global _ttl_manager
    _ttl_manager = manager
