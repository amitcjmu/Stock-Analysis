"""
Main TTL Manager class for dynamic cache management.

This module contains the primary TTLManager class that orchestrates
all TTL management functionality.

Generated by CC (Claude Code)
"""

import time
from typing import Any, Callable, Dict, List, Optional

from app.core.config import settings
from app.core.logging import get_logger
from app.services.caching.redis_cache import RedisCache

from .base import (
    TTLStrategy,
    RefreshPriority,
    CacheAccessPattern,
    TTLRecommendation,
    TTLMetrics,
)
from .strategies import TTLStrategies
from .expiry import RefreshTaskScheduler, BackgroundProcessor
from .utils import evict_old_patterns, warm_cache_keys, validate_ttl_recommendation

logger = get_logger(__name__)


class TTLManager:
    """
    Dynamic TTL manager that optimizes cache performance through intelligent
    TTL calculation and background refresh scheduling.

    Features:
    - Multiple TTL calculation strategies (static, adaptive, predictive)
    - Background refresh scheduling with priority queues
    - Cache warming for frequently accessed data
    - System load-aware TTL adjustments
    - Comprehensive metrics and monitoring
    """

    def __init__(self, redis_cache: RedisCache):
        self.redis_cache = redis_cache
        self.metrics = TTLMetrics()

        # Access pattern tracking
        self.access_patterns: Dict[str, CacheAccessPattern] = {}
        self.max_tracked_patterns = settings.get("CACHE_MAX_TRACKED_PATTERNS", 10000)

        # Initialize components
        self.strategies = TTLStrategies(self.access_patterns)
        self.scheduler = RefreshTaskScheduler(
            max_concurrent_refreshes=settings.get("CACHE_MAX_CONCURRENT_REFRESHES", 5)
        )
        self.scheduler.metrics = self.metrics
        self.background_processor = BackgroundProcessor(self.scheduler)

        # TTL strategy configuration
        self.default_strategy = TTLStrategy.HYBRID

        logger.info("TTLManager initialized")

    async def start(self) -> None:
        """Start the TTL manager background processing"""
        await self.background_processor.start()
        logger.info("TTLManager background processing started")

    async def stop(self) -> None:
        """Stop the TTL manager"""
        await self.background_processor.stop()
        logger.info("TTLManager stopped")

    def calculate_optimal_ttl(
        self,
        cache_key: str,
        data_type: Optional[str] = None,
        strategy: Optional[TTLStrategy] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> TTLRecommendation:
        """
        Calculate optimal TTL for a cache key using specified or default strategy.

        Args:
            cache_key: The cache key to calculate TTL for
            data_type: Type of data being cached (for static strategy)
            strategy: TTL calculation strategy to use
            metadata: Additional metadata for TTL calculation

        Returns:
            TTL recommendation with reasoning
        """
        start_time = time.time()
        strategy = strategy or self.default_strategy
        metadata = metadata or {}

        try:
            if strategy == TTLStrategy.STATIC:
                recommendation = self.strategies.calculate_static_ttl(
                    cache_key, data_type, metadata
                )
            elif strategy == TTLStrategy.ADAPTIVE:
                recommendation = self.strategies.calculate_adaptive_ttl(
                    cache_key, metadata
                )
            elif strategy == TTLStrategy.PREDICTIVE:
                recommendation = self.strategies.calculate_predictive_ttl(
                    cache_key, metadata
                )
            elif strategy == TTLStrategy.SYSTEM_LOAD_AWARE:
                recommendation = self.strategies.calculate_system_aware_ttl(
                    cache_key, metadata
                )
            elif strategy == TTLStrategy.HYBRID:
                recommendation = self.strategies.calculate_hybrid_ttl(
                    cache_key, data_type, metadata
                )
            else:
                # Fallback to static
                recommendation = self.strategies.calculate_static_ttl(
                    cache_key, data_type, metadata
                )

            # Validate recommendation
            if not validate_ttl_recommendation(recommendation):
                logger.warning(
                    f"Invalid TTL recommendation for {cache_key}, using fallback"
                )
                recommendation = self._get_fallback_recommendation(cache_key)

            # Update metrics
            processing_time_ms = (time.time() - start_time) * 1000
            self.metrics.update_calculation(strategy, processing_time_ms)

            return recommendation

        except Exception as e:
            logger.error(f"TTL calculation failed for {cache_key}: {e}")
            # Return safe default
            return self._get_fallback_recommendation(cache_key)

    def record_cache_access(self, cache_key: str, was_hit: bool) -> None:
        """Record cache access for pattern tracking"""
        if cache_key not in self.access_patterns:
            # Check if we need to evict old patterns to save memory
            if len(self.access_patterns) >= self.max_tracked_patterns:
                evict_old_patterns(self.access_patterns, self.max_tracked_patterns)
            self.access_patterns[cache_key] = CacheAccessPattern(cache_key=cache_key)

        self.access_patterns[cache_key].record_access(was_hit)

    def schedule_background_refresh(
        self,
        cache_key: str,
        refresh_callback: Callable[[str], Any],
        priority: RefreshPriority = RefreshPriority.MEDIUM,
        delay_seconds: int = 0,
    ) -> str:
        """
        Schedule background refresh for a cache key.

        Args:
            cache_key: Key to refresh
            refresh_callback: Function to call for refresh
            priority: Refresh priority
            delay_seconds: Delay before refresh

        Returns:
            Task ID for tracking
        """
        return self.scheduler.schedule_refresh(
            cache_key, refresh_callback, priority, delay_seconds
        )

    def schedule_predictive_refresh(
        self, cache_key: str, refresh_callback: Callable[[str], Any]
    ) -> Optional[str]:
        """Schedule predictive refresh based on access patterns"""
        return self.scheduler.schedule_predictive_refresh(
            cache_key, refresh_callback, self.access_patterns
        )

    def warm_cache(
        self, cache_keys: List[str], warm_callback: Callable[[List[str]], None]
    ) -> None:
        """Warm cache for frequently accessed keys"""
        warm_cache_keys(cache_keys, self.access_patterns, warm_callback)

    def get_access_pattern(self, cache_key: str) -> Optional[CacheAccessPattern]:
        """Get access pattern for a cache key"""
        return self.access_patterns.get(cache_key)

    def get_metrics(self) -> Dict[str, Any]:
        """Get TTL management metrics"""
        return {
            "total_ttl_calculations": self.metrics.total_ttl_calculations,
            "average_processing_time_ms": self.metrics.average_processing_time_ms,
            "strategy_usage": {
                strategy.value: count
                for strategy, count in self.metrics.strategy_usage.items()
            },
            "refresh_tasks": {
                "scheduled": self.metrics.refresh_tasks_scheduled,
                "completed": self.metrics.refresh_tasks_completed,
                "failed": self.metrics.refresh_tasks_failed,
                "success_rate": (
                    self.metrics.refresh_tasks_completed
                    / max(self.metrics.refresh_tasks_scheduled, 1)
                    * 100
                ),
            },
            "cache_optimization": {
                "tracked_patterns": len(self.access_patterns),
                "hot_data_keys": len(
                    [p for p in self.access_patterns.values() if p.is_hot_data]
                ),
                "cold_data_keys": len(
                    [p for p in self.access_patterns.values() if p.is_cold_data]
                ),
                "hits_prevented": self.metrics.cache_hits_prevented,
            },
            "refresh_queues": self.scheduler.get_queue_stats(),
            "active_refresh_tasks": len(self.scheduler.active_refresh_tasks),
            "last_reset": self.metrics.last_reset.isoformat(),
        }

    def reset_metrics(self) -> None:
        """Reset TTL metrics"""
        self.metrics = TTLMetrics()
        self.scheduler.metrics = self.metrics
        logger.info("TTL metrics reset")

    async def health_check(self) -> Dict[str, Any]:
        """Perform health check on TTL manager"""
        processor_health = await self.background_processor.health_check()

        health = {
            "status": "healthy",
            "tracked_patterns": len(self.access_patterns),
            "max_tracked_patterns": self.max_tracked_patterns,
            "issues": [],
        }

        # Merge processor health
        health.update(processor_health)

        # Check for issues
        if not health["background_processing_active"]:
            health["issues"].append("Background processing not active")
            health["status"] = "degraded"

        if health["tracked_patterns"] >= health["max_tracked_patterns"] * 0.9:
            health["issues"].append("Approaching maximum tracked patterns limit")

        if health["active_refresh_tasks"] >= health["max_concurrent_refreshes"]:
            health["issues"].append("At maximum concurrent refresh limit")

        if health["issues"]:
            health["status"] = (
                "warning" if health["status"] == "healthy" else health["status"]
            )

        return health

    def _get_fallback_recommendation(self, cache_key: str) -> TTLRecommendation:
        """Get a safe fallback TTL recommendation"""
        return TTLRecommendation(
            cache_key=cache_key,
            recommended_ttl_seconds=600,  # 10 minutes default
            strategy_used=TTLStrategy.STATIC,
            confidence=0.5,
            reasoning="Fallback TTL due to calculation error",
        )


# Factory function for dependency injection
def create_ttl_manager(redis_cache: RedisCache) -> TTLManager:
    """Create TTLManager with dependencies"""
    return TTLManager(redis_cache)


# Singleton instance for global access
_ttl_manager: Optional[TTLManager] = None


def get_ttl_manager() -> Optional[TTLManager]:
    """Get singleton TTLManager instance"""
    return _ttl_manager


def set_ttl_manager(manager: TTLManager) -> None:
    """Set singleton TTLManager instance"""
    global _ttl_manager
    _ttl_manager = manager
