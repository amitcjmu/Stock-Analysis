"""
TTL Manager for Auth Performance Optimization

This module implements dynamic TTL (Time-To-Live) management for cache entries including:
- Dynamic TTL calculation based on data type, usage patterns, and system load
- Background refresh scheduling to prevent cache misses
- Predictive refresh based on access patterns and trends
- Cache warming for frequently accessed data
- Adaptive TTL based on system performance metrics

The TTL manager optimizes cache performance by ensuring hot data stays cached
while allowing cold data to expire naturally, reducing memory usage and
improving overall system performance.

ðŸ”’ Security: Multi-tenant isolation and secure TTL management
âš¡ Performance: Optimized background refresh with minimal system impact
ðŸŽ¯ Coherence: Coordinated TTL management across cache layers
ðŸ“Š Analytics: Comprehensive TTL metrics and optimization tracking

Generated by CC (Claude Code)
"""

import asyncio
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple

from app.constants.cache_keys import CacheKeys
from app.core.config import settings
from app.core.logging import get_logger
from app.services.caching.redis_cache import RedisCache

logger = get_logger(__name__)


class TTLStrategy(Enum):
    """TTL calculation strategies"""

    STATIC = "static"  # Fixed TTL based on data type
    ADAPTIVE = "adaptive"  # TTL adapts based on access patterns
    PREDICTIVE = "predictive"  # TTL based on predicted access patterns
    SYSTEM_LOAD_AWARE = "system_aware"  # TTL adapts to system load
    HYBRID = "hybrid"  # Combination of multiple strategies


class RefreshPriority(Enum):
    """Background refresh priority levels"""

    CRITICAL = 1  # Authentication, security data
    HIGH = 2  # User context, active sessions
    MEDIUM = 3  # Client data, engagement data
    LOW = 4  # Analytics, reports
    BACKGROUND = 5  # Optimization, cleanup


@dataclass
class CacheAccessPattern:
    """Tracks access patterns for a cache key"""

    cache_key: str
    total_accesses: int = 0
    hits: int = 0
    misses: int = 0
    access_times: deque = field(default_factory=lambda: deque(maxlen=100))
    last_access: Optional[datetime] = None
    creation_time: datetime = field(default_factory=datetime.utcnow)
    average_access_interval_seconds: float = 0
    peak_access_periods: List[Tuple[int, int]] = field(
        default_factory=list
    )  # (hour_start, hour_end)

    @property
    def hit_ratio(self) -> float:
        """Calculate hit ratio"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0

    @property
    def access_frequency_per_hour(self) -> float:
        """Calculate access frequency per hour"""
        if not self.access_times or len(self.access_times) < 2:
            return 0.0

        oldest_access = self.access_times[0]
        newest_access = self.access_times[-1]
        time_span_hours = (newest_access - oldest_access).total_seconds() / 3600

        return len(self.access_times) / max(time_span_hours, 1.0)

    @property
    def is_hot_data(self) -> bool:
        """Check if this is frequently accessed hot data"""
        return (
            self.access_frequency_per_hour > 10  # More than 10 accesses per hour
            and self.hit_ratio > 0.8  # High hit ratio
        )

    @property
    def is_cold_data(self) -> bool:
        """Check if this is infrequently accessed cold data"""
        return (
            self.access_frequency_per_hour < 1  # Less than 1 access per hour
            and self.last_access
            and (datetime.utcnow() - self.last_access).total_seconds()
            > 3600  # No access in last hour
        )

    def record_access(self, was_hit: bool) -> None:
        """Record a cache access"""
        now = datetime.utcnow()
        self.total_accesses += 1
        self.access_times.append(now)
        self.last_access = now

        if was_hit:
            self.hits += 1
        else:
            self.misses += 1

        # Update average access interval
        if len(self.access_times) >= 2:
            intervals = []
            for i in range(1, len(self.access_times)):
                interval = (
                    self.access_times[i] - self.access_times[i - 1]
                ).total_seconds()
                intervals.append(interval)
            self.average_access_interval_seconds = sum(intervals) / len(intervals)

    def predict_next_access(self) -> Optional[datetime]:
        """Predict when this key will likely be accessed next"""
        if not self.last_access or self.average_access_interval_seconds == 0:
            return None

        return self.last_access + timedelta(
            seconds=self.average_access_interval_seconds
        )


@dataclass
class TTLRecommendation:
    """TTL recommendation with reasoning"""

    cache_key: str
    recommended_ttl_seconds: int
    strategy_used: TTLStrategy
    confidence: float  # 0.0 to 1.0
    reasoning: str
    factors: Dict[str, Any] = field(default_factory=dict)
    refresh_before_expiry: bool = False
    refresh_window_seconds: int = 0

    def should_refresh_early(self, current_ttl_remaining: int) -> bool:
        """Check if cache should be refreshed before expiry"""
        if not self.refresh_before_expiry:
            return False
        return current_ttl_remaining <= self.refresh_window_seconds


@dataclass
class RefreshTask:
    """Background refresh task"""

    cache_key: str
    priority: RefreshPriority
    refresh_callback: Callable[[str], Any]
    scheduled_time: datetime
    created_at: datetime = field(default_factory=datetime.utcnow)
    retry_count: int = 0
    max_retries: int = 3

    @property
    def is_due(self) -> bool:
        """Check if refresh task is due"""
        return datetime.utcnow() >= self.scheduled_time

    @property
    def should_retry(self) -> bool:
        """Check if task should be retried"""
        return self.retry_count < self.max_retries


@dataclass
class TTLMetrics:
    """TTL management metrics"""

    total_ttl_calculations: int = 0
    strategy_usage: Dict[TTLStrategy, int] = field(
        default_factory=lambda: defaultdict(int)
    )
    refresh_tasks_scheduled: int = 0
    refresh_tasks_completed: int = 0
    refresh_tasks_failed: int = 0
    cache_hits_prevented: int = 0  # Estimated cache misses prevented by refresh
    total_processing_time_ms: float = 0
    average_processing_time_ms: float = 0
    last_reset: datetime = field(default_factory=datetime.utcnow)

    def update_calculation(
        self, strategy: TTLStrategy, processing_time_ms: float
    ) -> None:
        """Update metrics for TTL calculation"""
        self.total_ttl_calculations += 1
        self.strategy_usage[strategy] += 1
        self.total_processing_time_ms += processing_time_ms

        if self.total_ttl_calculations > 0:
            self.average_processing_time_ms = (
                self.total_processing_time_ms / self.total_ttl_calculations
            )


class TTLManager:
    """
    Dynamic TTL manager that optimizes cache performance through intelligent
    TTL calculation and background refresh scheduling.

    Features:
    - Multiple TTL calculation strategies (static, adaptive, predictive)
    - Background refresh scheduling with priority queues
    - Cache warming for frequently accessed data
    - System load-aware TTL adjustments
    - Comprehensive metrics and monitoring
    """

    def __init__(self, redis_cache: RedisCache):
        self.redis_cache = redis_cache
        self.metrics = TTLMetrics()

        # Access pattern tracking
        self.access_patterns: Dict[str, CacheAccessPattern] = {}
        self.max_tracked_patterns = 10000  # Limit memory usage

        # System metrics for load-aware TTL
        self.system_load_history: deque = deque(maxlen=100)
        self.memory_usage_history: deque = deque(maxlen=100)

        # Background refresh management
        self.refresh_queues: Dict[RefreshPriority, deque] = {
            priority: deque() for priority in RefreshPriority
        }
        self.active_refresh_tasks: Dict[str, RefreshTask] = {}
        self.max_concurrent_refreshes = settings.get(
            "CACHE_MAX_CONCURRENT_REFRESHES", 5
        )

        # TTL strategy configuration
        self.default_strategy = TTLStrategy.HYBRID
        self.strategy_weights = {
            TTLStrategy.STATIC: 0.2,
            TTLStrategy.ADAPTIVE: 0.3,
            TTLStrategy.PREDICTIVE: 0.3,
            TTLStrategy.SYSTEM_LOAD_AWARE: 0.2,
        }

        # Background processing
        self._processing_task: Optional[asyncio.Task] = None
        self._metrics_task: Optional[asyncio.Task] = None
        self._shutdown_event = asyncio.Event()

        logger.info("TTLManager initialized")

    async def start(self) -> None:
        """Start the TTL manager background processing"""
        if self._processing_task is not None:
            logger.warning("TTLManager already started")
            return

        self._shutdown_event.clear()
        self._processing_task = asyncio.create_task(self._background_processing())
        self._metrics_task = asyncio.create_task(self._metrics_collection())

        logger.info("TTLManager background processing started")

    async def stop(self) -> None:
        """Stop the TTL manager"""
        if self._processing_task is None:
            return

        logger.info("Stopping TTLManager...")
        self._shutdown_event.set()

        try:
            if self._processing_task:
                await asyncio.wait_for(self._processing_task, timeout=10.0)
            if self._metrics_task:
                await asyncio.wait_for(self._metrics_task, timeout=5.0)
        except asyncio.TimeoutError:
            logger.warning("TTLManager shutdown timeout")
            if self._processing_task:
                self._processing_task.cancel()
            if self._metrics_task:
                self._metrics_task.cancel()

        self._processing_task = None
        self._metrics_task = None
        logger.info("TTLManager stopped")

    def calculate_optimal_ttl(
        self,
        cache_key: str,
        data_type: Optional[str] = None,
        strategy: Optional[TTLStrategy] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> TTLRecommendation:
        """
        Calculate optimal TTL for a cache key using specified or default strategy.

        Args:
            cache_key: The cache key to calculate TTL for
            data_type: Type of data being cached (for static strategy)
            strategy: TTL calculation strategy to use
            metadata: Additional metadata for TTL calculation

        Returns:
            TTL recommendation with reasoning
        """
        start_time = time.time()
        strategy = strategy or self.default_strategy
        metadata = metadata or {}

        try:
            if strategy == TTLStrategy.STATIC:
                recommendation = self._calculate_static_ttl(
                    cache_key, data_type, metadata
                )
            elif strategy == TTLStrategy.ADAPTIVE:
                recommendation = self._calculate_adaptive_ttl(cache_key, metadata)
            elif strategy == TTLStrategy.PREDICTIVE:
                recommendation = self._calculate_predictive_ttl(cache_key, metadata)
            elif strategy == TTLStrategy.SYSTEM_LOAD_AWARE:
                recommendation = self._calculate_system_aware_ttl(cache_key, metadata)
            elif strategy == TTLStrategy.HYBRID:
                recommendation = self._calculate_hybrid_ttl(
                    cache_key, data_type, metadata
                )
            else:
                # Fallback to static
                recommendation = self._calculate_static_ttl(
                    cache_key, data_type, metadata
                )

            # Update metrics
            processing_time_ms = (time.time() - start_time) * 1000
            self.metrics.update_calculation(strategy, processing_time_ms)

            return recommendation

        except Exception as e:
            logger.error(f"TTL calculation failed for {cache_key}: {e}")
            # Return safe default
            return TTLRecommendation(
                cache_key=cache_key,
                recommended_ttl_seconds=600,  # 10 minutes default
                strategy_used=TTLStrategy.STATIC,
                confidence=0.5,
                reasoning=f"Error in calculation, using default: {e}",
            )

    def record_cache_access(self, cache_key: str, was_hit: bool) -> None:
        """Record cache access for pattern tracking"""
        if cache_key not in self.access_patterns:
            # Check if we need to evict old patterns to save memory
            if len(self.access_patterns) >= self.max_tracked_patterns:
                self._evict_old_patterns()
            self.access_patterns[cache_key] = CacheAccessPattern(cache_key=cache_key)

        self.access_patterns[cache_key].record_access(was_hit)

    def schedule_background_refresh(
        self,
        cache_key: str,
        refresh_callback: Callable[[str], Any],
        priority: RefreshPriority = RefreshPriority.MEDIUM,
        delay_seconds: int = 0,
    ) -> str:
        """
        Schedule background refresh for a cache key.

        Args:
            cache_key: Key to refresh
            refresh_callback: Function to call for refresh
            priority: Refresh priority
            delay_seconds: Delay before refresh

        Returns:
            Task ID for tracking
        """
        scheduled_time = datetime.utcnow() + timedelta(seconds=delay_seconds)

        task = RefreshTask(
            cache_key=cache_key,
            priority=priority,
            refresh_callback=refresh_callback,
            scheduled_time=scheduled_time,
        )

        self.refresh_queues[priority].append(task)
        self.metrics.refresh_tasks_scheduled += 1

        logger.debug(f"Scheduled refresh for {cache_key} with priority {priority.name}")
        return f"refresh_{int(time.time() * 1000)}_{cache_key}"

    def schedule_predictive_refresh(
        self, cache_key: str, refresh_callback: Callable[[str], Any]
    ) -> Optional[str]:
        """Schedule predictive refresh based on access patterns"""
        pattern = self.access_patterns.get(cache_key)
        if not pattern:
            return None

        next_access = pattern.predict_next_access()
        if not next_access:
            return None

        # Schedule refresh slightly before predicted access
        refresh_time = next_access - timedelta(minutes=5)  # 5 minutes buffer
        if refresh_time <= datetime.utcnow():
            return None

        delay_seconds = (refresh_time - datetime.utcnow()).total_seconds()
        priority = (
            RefreshPriority.HIGH if pattern.is_hot_data else RefreshPriority.MEDIUM
        )

        return self.schedule_background_refresh(
            cache_key=cache_key,
            refresh_callback=refresh_callback,
            priority=priority,
            delay_seconds=int(delay_seconds),
        )

    def warm_cache(
        self, cache_keys: List[str], warm_callback: Callable[[List[str]], None]
    ) -> None:
        """Warm cache for frequently accessed keys"""
        hot_keys = []

        for cache_key in cache_keys:
            pattern = self.access_patterns.get(cache_key)
            if pattern and pattern.is_hot_data:
                hot_keys.append(cache_key)

        if hot_keys:
            try:
                warm_callback(hot_keys)
                logger.info(f"Warmed cache for {len(hot_keys)} hot keys")
            except Exception as e:
                logger.error(f"Cache warming failed: {e}")

    def get_access_pattern(self, cache_key: str) -> Optional[CacheAccessPattern]:
        """Get access pattern for a cache key"""
        return self.access_patterns.get(cache_key)

    def get_metrics(self) -> Dict[str, Any]:
        """Get TTL management metrics"""
        return {
            "total_ttl_calculations": self.metrics.total_ttl_calculations,
            "average_processing_time_ms": self.metrics.average_processing_time_ms,
            "strategy_usage": {
                strategy.value: count
                for strategy, count in self.metrics.strategy_usage.items()
            },
            "refresh_tasks": {
                "scheduled": self.metrics.refresh_tasks_scheduled,
                "completed": self.metrics.refresh_tasks_completed,
                "failed": self.metrics.refresh_tasks_failed,
                "success_rate": (
                    self.metrics.refresh_tasks_completed
                    / max(self.metrics.refresh_tasks_scheduled, 1)
                    * 100
                ),
            },
            "cache_optimization": {
                "tracked_patterns": len(self.access_patterns),
                "hot_data_keys": len(
                    [p for p in self.access_patterns.values() if p.is_hot_data]
                ),
                "cold_data_keys": len(
                    [p for p in self.access_patterns.values() if p.is_cold_data]
                ),
                "hits_prevented": self.metrics.cache_hits_prevented,
            },
            "refresh_queues": {
                priority.name: len(queue)
                for priority, queue in self.refresh_queues.items()
            },
            "active_refresh_tasks": len(self.active_refresh_tasks),
            "last_reset": self.metrics.last_reset.isoformat(),
        }

    def reset_metrics(self) -> None:
        """Reset TTL metrics"""
        self.metrics = TTLMetrics()
        logger.info("TTL metrics reset")

    async def health_check(self) -> Dict[str, Any]:
        """Perform health check on TTL manager"""
        health = {
            "status": "healthy",
            "background_processing_active": (
                self._processing_task is not None and not self._processing_task.done()
            ),
            "metrics_collection_active": (
                self._metrics_task is not None and not self._metrics_task.done()
            ),
            "tracked_patterns": len(self.access_patterns),
            "max_tracked_patterns": self.max_tracked_patterns,
            "active_refresh_tasks": len(self.active_refresh_tasks),
            "max_concurrent_refreshes": self.max_concurrent_refreshes,
            "issues": [],
        }

        # Check for issues
        if not health["background_processing_active"]:
            health["issues"].append("Background processing not active")
            health["status"] = "degraded"

        if health["tracked_patterns"] >= health["max_tracked_patterns"] * 0.9:
            health["issues"].append("Approaching maximum tracked patterns limit")

        if health["active_refresh_tasks"] >= health["max_concurrent_refreshes"]:
            health["issues"].append("At maximum concurrent refresh limit")

        if health["issues"]:
            health["status"] = (
                "warning" if health["status"] == "healthy" else health["status"]
            )

        return health

    # Private methods

    def _calculate_static_ttl(
        self, cache_key: str, data_type: Optional[str], metadata: Dict[str, Any]
    ) -> TTLRecommendation:
        """Calculate static TTL based on data type"""
        # Use CacheKeys TTL recommendations
        ttl_seconds = CacheKeys.get_ttl_recommendation(cache_key)

        # Apply data type specific adjustments
        if data_type:
            if data_type in ["user_session", "authentication"]:
                ttl_seconds = max(ttl_seconds, 3600)  # At least 1 hour
            elif data_type in ["analytics", "reports"]:
                ttl_seconds = min(ttl_seconds, 1800)  # At most 30 minutes

        return TTLRecommendation(
            cache_key=cache_key,
            recommended_ttl_seconds=ttl_seconds,
            strategy_used=TTLStrategy.STATIC,
            confidence=0.8,
            reasoning=f"Static TTL based on cache key type and data type: {data_type}",
            factors={"data_type": data_type, "base_ttl": ttl_seconds},
        )

    def _calculate_adaptive_ttl(
        self, cache_key: str, metadata: Dict[str, Any]
    ) -> TTLRecommendation:
        """Calculate adaptive TTL based on access patterns"""
        pattern = self.access_patterns.get(cache_key)
        base_ttl = CacheKeys.get_ttl_recommendation(cache_key)

        if not pattern:
            return TTLRecommendation(
                cache_key=cache_key,
                recommended_ttl_seconds=base_ttl,
                strategy_used=TTLStrategy.ADAPTIVE,
                confidence=0.5,
                reasoning="No access pattern data available, using base TTL",
                factors={"base_ttl": base_ttl},
            )

        # Adjust TTL based on access patterns
        multiplier = 1.0

        if pattern.is_hot_data:
            # Hot data gets longer TTL to reduce refresh overhead
            multiplier = 2.0
        elif pattern.is_cold_data:
            # Cold data gets shorter TTL to save memory
            multiplier = 0.5
        else:
            # Normal data, adjust based on hit ratio
            multiplier = 0.5 + (pattern.hit_ratio * 1.5)  # 0.5 to 2.0 range

        adjusted_ttl = int(base_ttl * multiplier)

        # Set refresh window for hot data
        refresh_before_expiry = pattern.is_hot_data
        refresh_window = int(adjusted_ttl * 0.1) if refresh_before_expiry else 0

        return TTLRecommendation(
            cache_key=cache_key,
            recommended_ttl_seconds=adjusted_ttl,
            strategy_used=TTLStrategy.ADAPTIVE,
            confidence=0.9,
            reasoning=(
                f"Adaptive TTL based on access pattern: "
                f"{'hot' if pattern.is_hot_data else 'cold' if pattern.is_cold_data else 'normal'} data"
            ),
            factors={
                "base_ttl": base_ttl,
                "multiplier": multiplier,
                "hit_ratio": pattern.hit_ratio,
                "access_frequency": pattern.access_frequency_per_hour,
                "is_hot_data": pattern.is_hot_data,
                "is_cold_data": pattern.is_cold_data,
            },
            refresh_before_expiry=refresh_before_expiry,
            refresh_window_seconds=refresh_window,
        )

    def _calculate_predictive_ttl(
        self, cache_key: str, metadata: Dict[str, Any]
    ) -> TTLRecommendation:
        """Calculate predictive TTL based on predicted access patterns"""
        pattern = self.access_patterns.get(cache_key)
        base_ttl = CacheKeys.get_ttl_recommendation(cache_key)

        if not pattern or not pattern.access_times:
            return TTLRecommendation(
                cache_key=cache_key,
                recommended_ttl_seconds=base_ttl,
                strategy_used=TTLStrategy.PREDICTIVE,
                confidence=0.5,
                reasoning="Insufficient data for prediction, using base TTL",
                factors={"base_ttl": base_ttl},
            )

        # Predict next access time
        next_access = pattern.predict_next_access()
        if not next_access:
            return self._calculate_adaptive_ttl(cache_key, metadata)

        # Calculate TTL to cover until next predicted access with some buffer
        time_until_next_access = (next_access - datetime.utcnow()).total_seconds()
        buffer_seconds = max(
            300, time_until_next_access * 0.2
        )  # 20% buffer, minimum 5 minutes
        predicted_ttl = int(time_until_next_access + buffer_seconds)

        # Bound the TTL to reasonable limits
        min_ttl = base_ttl // 2
        max_ttl = base_ttl * 3
        predicted_ttl = max(min_ttl, min(predicted_ttl, max_ttl))

        return TTLRecommendation(
            cache_key=cache_key,
            recommended_ttl_seconds=predicted_ttl,
            strategy_used=TTLStrategy.PREDICTIVE,
            confidence=0.8,
            reasoning=f"Predictive TTL based on next access prediction: {next_access.isoformat()}",
            factors={
                "base_ttl": base_ttl,
                "predicted_next_access": next_access.isoformat(),
                "time_until_next_access": time_until_next_access,
                "buffer_seconds": buffer_seconds,
                "average_interval": pattern.average_access_interval_seconds,
            },
            refresh_before_expiry=True,
            refresh_window_seconds=int(buffer_seconds),
        )

    def _calculate_system_aware_ttl(
        self, cache_key: str, metadata: Dict[str, Any]
    ) -> TTLRecommendation:
        """Calculate system load-aware TTL"""
        base_ttl = CacheKeys.get_ttl_recommendation(cache_key)

        # Get current system load metrics
        current_load = self._get_current_system_load()
        memory_pressure = self._get_memory_pressure()

        # Adjust TTL based on system metrics
        load_multiplier = 1.0

        if current_load > 0.8:  # High system load
            load_multiplier *= 0.7  # Shorter TTL to reduce memory pressure
        elif current_load < 0.3:  # Low system load
            load_multiplier *= 1.3  # Longer TTL since we have capacity

        if memory_pressure > 0.8:  # High memory pressure
            load_multiplier *= 0.6  # Significantly shorter TTL
        elif memory_pressure < 0.4:  # Low memory pressure
            load_multiplier *= 1.2  # Slightly longer TTL

        adjusted_ttl = int(base_ttl * load_multiplier)

        return TTLRecommendation(
            cache_key=cache_key,
            recommended_ttl_seconds=adjusted_ttl,
            strategy_used=TTLStrategy.SYSTEM_LOAD_AWARE,
            confidence=0.7,
            reasoning=f"System-aware TTL: load={current_load:.2f}, memory_pressure={memory_pressure:.2f}",
            factors={
                "base_ttl": base_ttl,
                "system_load": current_load,
                "memory_pressure": memory_pressure,
                "load_multiplier": load_multiplier,
            },
        )

    def _calculate_hybrid_ttl(
        self, cache_key: str, data_type: Optional[str], metadata: Dict[str, Any]
    ) -> TTLRecommendation:
        """Calculate hybrid TTL using weighted combination of strategies"""
        # Get recommendations from different strategies
        static_rec = self._calculate_static_ttl(cache_key, data_type, metadata)
        adaptive_rec = self._calculate_adaptive_ttl(cache_key, metadata)
        predictive_rec = self._calculate_predictive_ttl(cache_key, metadata)
        system_rec = self._calculate_system_aware_ttl(cache_key, metadata)

        # Calculate weighted average
        recommendations = [
            (static_rec, self.strategy_weights[TTLStrategy.STATIC]),
            (adaptive_rec, self.strategy_weights[TTLStrategy.ADAPTIVE]),
            (predictive_rec, self.strategy_weights[TTLStrategy.PREDICTIVE]),
            (system_rec, self.strategy_weights[TTLStrategy.SYSTEM_LOAD_AWARE]),
        ]

        total_weight = sum(weight for _, weight in recommendations)
        weighted_ttl = (
            sum(rec.recommended_ttl_seconds * weight for rec, weight in recommendations)
            / total_weight
        )

        final_ttl = int(weighted_ttl)

        # Calculate overall confidence
        confidence = (
            sum(rec.confidence * weight for rec, weight in recommendations)
            / total_weight
        )

        # Determine refresh settings
        refresh_recommendations = [
            rec for rec, _ in recommendations if rec.refresh_before_expiry
        ]
        refresh_before_expiry = len(refresh_recommendations) >= 2  # Majority vote
        refresh_window = int(final_ttl * 0.1) if refresh_before_expiry else 0

        return TTLRecommendation(
            cache_key=cache_key,
            recommended_ttl_seconds=final_ttl,
            strategy_used=TTLStrategy.HYBRID,
            confidence=confidence,
            reasoning="Hybrid TTL using weighted combination of all strategies",
            factors={
                "static_ttl": static_rec.recommended_ttl_seconds,
                "adaptive_ttl": adaptive_rec.recommended_ttl_seconds,
                "predictive_ttl": predictive_rec.recommended_ttl_seconds,
                "system_aware_ttl": system_rec.recommended_ttl_seconds,
                "weights": dict(self.strategy_weights),
                "final_ttl": final_ttl,
            },
            refresh_before_expiry=refresh_before_expiry,
            refresh_window_seconds=refresh_window,
        )

    def _get_current_system_load(self) -> float:
        """Get current system load (0.0 to 1.0)"""
        try:
            import psutil

            return psutil.cpu_percent(interval=0.1) / 100.0
        except ImportError:
            # Fallback if psutil not available
            return 0.5

    def _get_memory_pressure(self) -> float:
        """Get current memory pressure (0.0 to 1.0)"""
        try:
            import psutil

            memory = psutil.virtual_memory()
            return memory.percent / 100.0
        except ImportError:
            # Fallback if psutil not available
            return 0.5

    def _evict_old_patterns(self) -> None:
        """Evict old access patterns to control memory usage"""
        # Sort patterns by last access time and remove oldest 10%
        patterns_by_age = sorted(
            self.access_patterns.items(),
            key=lambda x: x[1].last_access or x[1].creation_time,
        )

        evict_count = len(patterns_by_age) // 10  # Remove 10%
        for cache_key, _ in patterns_by_age[:evict_count]:
            del self.access_patterns[cache_key]

        logger.debug(f"Evicted {evict_count} old access patterns")

    async def _background_processing(self) -> None:
        """Background task to process refresh queues"""
        logger.info("Starting TTL manager background processing")

        while not self._shutdown_event.is_set():
            try:
                # Process refresh queues
                await self._process_refresh_queues()

                # Clean up completed tasks
                self._cleanup_completed_refresh_tasks()

                # Small delay to prevent tight loop
                await asyncio.sleep(1.0)  # 1 second

            except Exception as e:
                logger.error(f"Error in TTL manager background processing: {e}")
                await asyncio.sleep(5.0)  # Longer delay on error

        logger.info("TTL manager background processing stopped")

    async def _process_refresh_queues(self) -> None:
        """Process refresh tasks from priority queues"""
        if len(self.active_refresh_tasks) >= self.max_concurrent_refreshes:
            return

        # Process from highest priority to lowest
        for priority in RefreshPriority:
            queue = self.refresh_queues[priority]

            while (
                queue and len(self.active_refresh_tasks) < self.max_concurrent_refreshes
            ):
                task = queue.popleft()

                if task.is_due:
                    # Execute refresh task
                    self.active_refresh_tasks[task.cache_key] = task
                    asyncio.create_task(self._execute_refresh_task(task))
                else:
                    # Put it back for later
                    queue.appendleft(task)
                    break

    async def _execute_refresh_task(self, task: RefreshTask) -> None:
        """Execute a refresh task"""
        try:
            # Call the refresh callback
            if asyncio.iscoroutinefunction(task.refresh_callback):
                await task.refresh_callback(task.cache_key)
            else:
                task.refresh_callback(task.cache_key)

            self.metrics.refresh_tasks_completed += 1
            self.metrics.cache_hits_prevented += 1  # Estimate

            logger.debug(f"Completed refresh task for {task.cache_key}")

        except Exception as e:
            logger.error(f"Refresh task failed for {task.cache_key}: {e}")
            task.retry_count += 1
            self.metrics.refresh_tasks_failed += 1

            # Retry if appropriate
            if task.should_retry:
                # Reschedule with delay
                task.scheduled_time = datetime.utcnow() + timedelta(
                    seconds=60
                )  # 1 minute delay
                self.refresh_queues[RefreshPriority.LOW].append(task)

        finally:
            # Remove from active tasks
            self.active_refresh_tasks.pop(task.cache_key, None)

    def _cleanup_completed_refresh_tasks(self) -> None:
        """Clean up completed refresh tasks"""
        # This is handled in _execute_refresh_task, but we could add
        # additional cleanup logic here if needed
        pass

    async def _metrics_collection(self) -> None:
        """Background task to collect system metrics"""
        logger.info("Starting TTL manager metrics collection")

        while not self._shutdown_event.is_set():
            try:
                # Collect system metrics
                system_load = self._get_current_system_load()
                memory_pressure = self._get_memory_pressure()

                self.system_load_history.append(system_load)
                self.memory_usage_history.append(memory_pressure)

                # Sleep for 30 seconds between collections
                await asyncio.sleep(30.0)

            except Exception as e:
                logger.error(f"Error in metrics collection: {e}")
                await asyncio.sleep(60.0)  # Longer delay on error

        logger.info("TTL manager metrics collection stopped")


# Factory function for dependency injection
def create_ttl_manager(redis_cache: RedisCache) -> TTLManager:
    """Create TTLManager with dependencies"""
    return TTLManager(redis_cache)


# Singleton instance for global access
_ttl_manager: Optional[TTLManager] = None


def get_ttl_manager() -> Optional[TTLManager]:
    """Get singleton TTLManager instance"""
    return _ttl_manager


def set_ttl_manager(manager: TTLManager) -> None:
    """Set singleton TTLManager instance"""
    global _ttl_manager
    _ttl_manager = manager


# Export all classes
__all__ = [
    "TTLStrategy",
    "RefreshPriority",
    "CacheAccessPattern",
    "TTLRecommendation",
    "RefreshTask",
    "TTLMetrics",
    "TTLManager",
    "create_ttl_manager",
    "get_ttl_manager",
    "set_ttl_manager",
]
