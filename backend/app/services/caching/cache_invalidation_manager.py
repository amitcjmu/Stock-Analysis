"""
Cache Invalidation Manager for Auth Performance Optimization

This module provides a centralized cache invalidation management system that:
- Coordinates invalidation across multiple cache layers (Redis, in-memory, etc.)
- Handles different invalidation strategies based on data type and criticality
- Supports bulk operations with efficient batching
- Provides real-time cache updates for critical data changes
- Includes performance monitoring and optimization for invalidation operations

The manager integrates with existing AuthCacheService, RedisCache, and
CacheCoherenceManager to provide comprehensive invalidation coverage.

ðŸ”’ Security: Multi-tenant isolation and secure invalidation operations
âš¡ Performance: Optimized batch operations and minimal overhead on critical paths
ðŸŽ¯ Coherence: Cascade invalidation with cycle detection and dependency tracking
ðŸ“Š Analytics: Comprehensive invalidation tracking and metrics collection

Generated by CC (Claude Code)
"""

import asyncio
import time
import uuid
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

from app.core.config import settings
from app.core.logging import get_logger
from app.services.caching.auth_cache_service import AuthCacheService
from app.services.caching.coherence_manager import CacheCoherenceManager
from app.services.caching.invalidation_strategies import (
    BaseInvalidationStrategy,
    InvalidationEvent,
    InvalidationPriority,
    InvalidationResult,
    InvalidationTrigger,
    create_invalidation_strategy,
)
from app.services.caching.redis_cache import RedisCache

logger = get_logger(__name__)


@dataclass
class InvalidationTask:
    """Represents a cache invalidation task"""

    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    event: InvalidationEvent = None
    strategy: Optional[BaseInvalidationStrategy] = None
    result: Optional[InvalidationResult] = None
    created_at: datetime = field(default_factory=datetime.utcnow)
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    retry_count: int = 0
    max_retries: int = 3

    @property
    def is_completed(self) -> bool:
        return self.completed_at is not None

    @property
    def execution_time_ms(self) -> Optional[float]:
        if self.started_at and self.completed_at:
            return (self.completed_at - self.started_at).total_seconds() * 1000
        return None

    @property
    def should_retry(self) -> bool:
        return (
            not self.is_completed
            and self.retry_count < self.max_retries
            and self.result is not None
            and not self.result.success
        )


@dataclass
class InvalidationMetrics:
    """Cache invalidation performance metrics"""

    total_tasks: int = 0
    successful_tasks: int = 0
    failed_tasks: int = 0
    total_keys_invalidated: int = 0
    total_execution_time_ms: float = 0
    average_execution_time_ms: float = 0
    tasks_by_priority: Dict[InvalidationPriority, int] = field(
        default_factory=lambda: defaultdict(int)
    )
    tasks_by_trigger: Dict[InvalidationTrigger, int] = field(
        default_factory=lambda: defaultdict(int)
    )
    errors_by_type: Dict[str, int] = field(default_factory=lambda: defaultdict(int))
    last_reset: datetime = field(default_factory=datetime.utcnow)

    def update(self, task: InvalidationTask) -> None:
        """Update metrics with completed task"""
        self.total_tasks += 1

        if task.result:
            if task.result.success:
                self.successful_tasks += 1
                self.total_keys_invalidated += task.result.keys_invalidated
                self.total_execution_time_ms += task.result.execution_time_ms

                # Update average
                if self.successful_tasks > 0:
                    self.average_execution_time_ms = (
                        self.total_execution_time_ms / self.successful_tasks
                    )
            else:
                self.failed_tasks += 1
                for error in task.result.errors:
                    self.errors_by_type[error] += 1

        if task.event:
            self.tasks_by_priority[task.event.priority] += 1
            self.tasks_by_trigger[task.event.trigger] += 1

    def reset(self) -> None:
        """Reset all metrics"""
        self.__init__()


class CacheInvalidationManager:
    """
    Central cache invalidation manager that coordinates invalidation across
    multiple cache layers and strategies.

    Features:
    - Multi-strategy invalidation based on event type and data criticality
    - Priority-based task queuing and execution
    - Bulk operations with efficient batching
    - Rate limiting to prevent invalidation storms
    - Comprehensive metrics and monitoring
    - Integration with existing cache services
    """

    def __init__(
        self,
        redis_cache: RedisCache,
        auth_cache_service: AuthCacheService,
        coherence_manager: Optional[CacheCoherenceManager] = None,
    ):
        self.redis_cache = redis_cache
        self.auth_cache_service = auth_cache_service
        self.coherence_manager = coherence_manager

        # Task management
        self.task_queues: Dict[InvalidationPriority, deque] = {
            priority: deque() for priority in InvalidationPriority
        }
        self.active_tasks: Dict[str, InvalidationTask] = {}
        self.completed_tasks: deque = deque(
            maxlen=1000
        )  # Keep last 1000 completed tasks

        # Rate limiting and batching
        self.max_concurrent_tasks = settings.get(
            "CACHE_MAX_CONCURRENT_INVALIDATIONS", 10
        )
        self.batch_size = settings.get("CACHE_INVALIDATION_BATCH_SIZE", 50)
        self.rate_limit_per_second = settings.get("CACHE_INVALIDATION_RATE_LIMIT", 100)
        self.rate_limit_window = 1.0  # 1 second window
        self.rate_limit_tokens = self.rate_limit_per_second
        self.last_token_refresh = time.time()

        # Metrics and monitoring
        self.metrics = InvalidationMetrics()
        self.performance_history: deque = deque(
            maxlen=10000
        )  # Keep last 10k operations

        # Background processing
        self._processing_task: Optional[asyncio.Task] = None
        self._shutdown_event = asyncio.Event()

        # Event handlers for integration with existing services
        self._event_handlers: Dict[InvalidationTrigger, List[Callable]] = defaultdict(
            list
        )

        logger.info("CacheInvalidationManager initialized")

    async def start(self) -> None:
        """Start the invalidation manager background processing"""
        if self._processing_task is not None:
            logger.warning("CacheInvalidationManager already started")
            return

        self._shutdown_event.clear()
        self._processing_task = asyncio.create_task(self._background_processing())
        logger.info("CacheInvalidationManager background processing started")

    async def stop(self) -> None:
        """Stop the invalidation manager and wait for pending tasks"""
        if self._processing_task is None:
            return

        logger.info("Stopping CacheInvalidationManager...")
        self._shutdown_event.set()

        try:
            await asyncio.wait_for(self._processing_task, timeout=30.0)
        except asyncio.TimeoutError:
            logger.warning("CacheInvalidationManager shutdown timeout, cancelling task")
            self._processing_task.cancel()

        self._processing_task = None
        logger.info("CacheInvalidationManager stopped")

    async def invalidate(
        self,
        trigger: InvalidationTrigger,
        entity_type: str,
        entity_id: str,
        client_account_id: str,
        user_id: Optional[str] = None,
        engagement_id: Optional[str] = None,
        priority: Optional[InvalidationPriority] = None,
        metadata: Optional[Dict[str, Any]] = None,
        wait_for_completion: bool = False,
    ) -> Union[str, InvalidationResult]:
        """
        Schedule cache invalidation for an entity.

        Args:
            trigger: What triggered the invalidation
            entity_type: Type of entity (user, client, engagement, etc.)
            entity_id: ID of the entity
            client_account_id: Client account for multi-tenant isolation
            user_id: Optional user context
            engagement_id: Optional engagement context
            priority: Invalidation priority (auto-determined if not provided)
            metadata: Additional metadata for the invalidation
            wait_for_completion: If True, wait for invalidation to complete

        Returns:
            Task ID if wait_for_completion=False, otherwise InvalidationResult
        """
        # Create invalidation event
        event = InvalidationEvent(
            trigger=trigger,
            priority=priority or self._determine_priority(trigger),
            entity_type=entity_type,
            entity_id=entity_id,
            client_account_id=client_account_id,
            user_id=user_id,
            engagement_id=engagement_id,
            metadata=metadata or {},
            correlation_id=str(uuid.uuid4()),
        )

        # Create invalidation task
        task = InvalidationTask(event=event)

        # Add to appropriate priority queue
        self.task_queues[event.priority].append(task)

        logger.debug(
            f"Queued invalidation task {task.id} for {entity_type}:{entity_id} "
            f"with priority {event.priority.name}"
        )

        if wait_for_completion:
            return await self._wait_for_task_completion(task.id)
        else:
            return task.id

    async def invalidate_pattern(
        self,
        pattern: str,
        client_account_id: str,
        priority: InvalidationPriority = InvalidationPriority.MEDIUM,
        metadata: Optional[Dict[str, Any]] = None,
        wait_for_completion: bool = False,
    ) -> Union[str, InvalidationResult]:
        """
        Invalidate all cache keys matching a pattern.

        Args:
            pattern: Cache key pattern (supports wildcards)
            client_account_id: Client account for tenant isolation
            priority: Invalidation priority
            metadata: Additional metadata
            wait_for_completion: If True, wait for completion

        Returns:
            Task ID or InvalidationResult
        """
        metadata = metadata or {}
        metadata.update(
            {
                "patterns": [pattern],
                "invalidation_type": "pattern",
            }
        )

        return await self.invalidate(
            trigger=InvalidationTrigger.MANUAL_INVALIDATION,
            entity_type="pattern",
            entity_id=pattern,
            client_account_id=client_account_id,
            priority=priority,
            metadata=metadata,
            wait_for_completion=wait_for_completion,
        )

    async def invalidate_bulk(
        self,
        events: List[
            Tuple[InvalidationTrigger, str, str, str]
        ],  # (trigger, entity_type, entity_id, client_account_id)
        priority: InvalidationPriority = InvalidationPriority.MEDIUM,
        metadata: Optional[Dict[str, Any]] = None,
        wait_for_completion: bool = False,
    ) -> Union[List[str], List[InvalidationResult]]:
        """
        Bulk invalidate multiple entities efficiently.

        Args:
            events: List of (trigger, entity_type, entity_id, client_account_id) tuples
            priority: Invalidation priority for all events
            metadata: Shared metadata for all events
            wait_for_completion: If True, wait for all completions

        Returns:
            List of task IDs or InvalidationResults
        """
        tasks = []

        for trigger, entity_type, entity_id, client_account_id in events:
            event_metadata = (metadata or {}).copy()
            event_metadata["bulk_operation"] = True
            event_metadata["bulk_size"] = len(events)

            task_id = await self.invalidate(
                trigger=trigger,
                entity_type=entity_type,
                entity_id=entity_id,
                client_account_id=client_account_id,
                priority=priority,
                metadata=event_metadata,
                wait_for_completion=False,
            )
            tasks.append(task_id)

        if wait_for_completion:
            results = []
            for task_id in tasks:
                result = await self._wait_for_task_completion(task_id)
                results.append(result)
            return results
        else:
            return tasks

    async def invalidate_user_session(
        self,
        user_id: str,
        client_account_id: str,
        trigger: InvalidationTrigger = InvalidationTrigger.USER_LOGOUT,
        wait_for_completion: bool = False,
    ) -> Union[str, InvalidationResult]:
        """Convenience method for user session invalidation"""
        return await self.invalidate(
            trigger=trigger,
            entity_type="user",
            entity_id=user_id,
            client_account_id=client_account_id,
            priority=InvalidationPriority.CRITICAL,
            metadata={"invalidation_scope": "user_session"},
            wait_for_completion=wait_for_completion,
        )

    async def invalidate_user_context(
        self,
        user_id: str,
        client_account_id: str,
        wait_for_completion: bool = False,
    ) -> Union[str, InvalidationResult]:
        """Convenience method for user context invalidation"""
        return await self.invalidate(
            trigger=InvalidationTrigger.USER_CONTEXT_CHANGE,
            entity_type="user",
            entity_id=user_id,
            client_account_id=client_account_id,
            priority=InvalidationPriority.HIGH,
            metadata={"invalidation_scope": "user_context"},
            wait_for_completion=wait_for_completion,
        )

    async def invalidate_on_security_event(
        self,
        user_id: str,
        client_account_id: str,
        event_type: str,
        severity: str = "high",
        wait_for_completion: bool = True,
    ) -> Union[str, InvalidationResult]:
        """Convenience method for security event invalidation"""
        return await self.invalidate(
            trigger=InvalidationTrigger.SECURITY_EVENT,
            entity_type="user",
            entity_id=user_id,
            client_account_id=client_account_id,
            priority=InvalidationPriority.CRITICAL,
            metadata={
                "event_type": event_type,
                "severity": severity,
                "security_event": True,
            },
            wait_for_completion=wait_for_completion,
        )

    def register_event_handler(
        self,
        trigger: InvalidationTrigger,
        handler: Callable[[InvalidationEvent], None],
    ) -> None:
        """Register custom event handler for invalidation events"""
        self._event_handlers[trigger].append(handler)

    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """Get status of a specific invalidation task"""
        # Check active tasks
        if task_id in self.active_tasks:
            task = self.active_tasks[task_id]
            return {
                "id": task.id,
                "status": "active",
                "created_at": task.created_at.isoformat(),
                "started_at": task.started_at.isoformat() if task.started_at else None,
                "retry_count": task.retry_count,
                "event": task.event.to_dict() if task.event else None,
            }

        # Check completed tasks
        for task in self.completed_tasks:
            if task.id == task_id:
                return {
                    "id": task.id,
                    "status": "completed",
                    "created_at": task.created_at.isoformat(),
                    "started_at": (
                        task.started_at.isoformat() if task.started_at else None
                    ),
                    "completed_at": (
                        task.completed_at.isoformat() if task.completed_at else None
                    ),
                    "execution_time_ms": task.execution_time_ms,
                    "retry_count": task.retry_count,
                    "success": task.result.success if task.result else False,
                    "keys_invalidated": (
                        task.result.keys_invalidated if task.result else 0
                    ),
                }

        # Check queues
        for priority, queue in self.task_queues.items():
            for task in queue:
                if task.id == task_id:
                    return {
                        "id": task.id,
                        "status": "queued",
                        "priority": priority.name,
                        "created_at": task.created_at.isoformat(),
                        "queue_position": list(queue).index(task),
                    }

        return None

    def get_metrics(self) -> Dict[str, Any]:
        """Get comprehensive invalidation metrics"""
        return {
            "metrics": {
                "total_tasks": self.metrics.total_tasks,
                "successful_tasks": self.metrics.successful_tasks,
                "failed_tasks": self.metrics.failed_tasks,
                "success_rate": (
                    self.metrics.successful_tasks
                    / max(self.metrics.total_tasks, 1)
                    * 100
                ),
                "total_keys_invalidated": self.metrics.total_keys_invalidated,
                "average_execution_time_ms": self.metrics.average_execution_time_ms,
                "last_reset": self.metrics.last_reset.isoformat(),
            },
            "tasks_by_priority": {
                priority.name: count
                for priority, count in self.metrics.tasks_by_priority.items()
            },
            "tasks_by_trigger": {
                trigger.name: count
                for trigger, count in self.metrics.tasks_by_trigger.items()
            },
            "errors_by_type": dict(self.metrics.errors_by_type),
            "queue_status": {
                priority.name: len(queue)
                for priority, queue in self.task_queues.items()
            },
            "active_tasks": len(self.active_tasks),
            "rate_limit": {
                "tokens_remaining": self.rate_limit_tokens,
                "tokens_per_second": self.rate_limit_per_second,
            },
        }

    def reset_metrics(self) -> None:
        """Reset all metrics"""
        self.metrics.reset()
        self.performance_history.clear()
        logger.info("Cache invalidation metrics reset")

    async def health_check(self) -> Dict[str, Any]:
        """Perform health check on the invalidation manager"""
        health = {
            "status": "healthy",
            "active_tasks": len(self.active_tasks),
            "max_concurrent_tasks": self.max_concurrent_tasks,
            "queue_backlog": sum(len(queue) for queue in self.task_queues.values()),
            "processing_task_running": self._processing_task is not None
            and not self._processing_task.done(),
            "rate_limit_tokens": self.rate_limit_tokens,
            "issues": [],
        }

        # Check for issues
        if health["active_tasks"] >= health["max_concurrent_tasks"]:
            health["issues"].append("At maximum concurrent task limit")

        if health["queue_backlog"] > 1000:
            health["issues"].append(
                f"High queue backlog: {health['queue_backlog']} tasks"
            )

        if not health["processing_task_running"]:
            health["issues"].append("Background processing task not running")
            health["status"] = "degraded"

        if health["rate_limit_tokens"] <= 0:
            health["issues"].append("Rate limit exhausted")

        if health["issues"]:
            health["status"] = (
                "warning" if health["status"] == "healthy" else health["status"]
            )

        return health

    # Private methods

    async def _background_processing(self) -> None:
        """Background task processing loop"""
        logger.info("Starting cache invalidation background processing")

        while not self._shutdown_event.is_set():
            try:
                # Refresh rate limit tokens
                self._refresh_rate_limit_tokens()

                # Process tasks from priority queues
                await self._process_task_queues()

                # Handle completed tasks
                self._handle_completed_tasks()

                # Handle retries
                await self._handle_retries()

                # Small delay to prevent tight loop
                await asyncio.sleep(0.01)  # 10ms

            except Exception as e:
                logger.error(f"Error in background processing: {e}")
                await asyncio.sleep(0.1)  # Longer delay on error

        # Process remaining tasks before shutdown
        await self._drain_remaining_tasks()
        logger.info("Cache invalidation background processing stopped")

    async def _process_task_queues(self) -> None:
        """Process tasks from priority queues"""
        # Check if we can process more tasks
        if len(self.active_tasks) >= self.max_concurrent_tasks:
            return

        if self.rate_limit_tokens <= 0:
            return

        # Process from highest priority to lowest
        for priority in InvalidationPriority:
            queue = self.task_queues[priority]

            while (
                queue
                and len(self.active_tasks) < self.max_concurrent_tasks
                and self.rate_limit_tokens > 0
            ):
                task = queue.popleft()
                await self._execute_task(task)

    async def _execute_task(self, task: InvalidationTask) -> None:
        """Execute a single invalidation task"""
        task.started_at = datetime.utcnow()
        self.active_tasks[task.id] = task
        self.rate_limit_tokens -= 1

        try:
            # Create appropriate strategy for the task
            strategy = create_invalidation_strategy(task.event, self.redis_cache)

            if strategy is None:
                # Fallback to direct cache invalidation
                strategy = self._create_fallback_strategy()

            task.strategy = strategy

            # Execute the invalidation
            result = await strategy.invalidate(task.event)
            task.result = result
            task.completed_at = datetime.utcnow()

            # Fire event handlers
            await self._fire_event_handlers(task.event)

            # Update metrics
            self.metrics.update(task)

            # Record performance data
            if task.execution_time_ms:
                self.performance_history.append(
                    {
                        "task_id": task.id,
                        "execution_time_ms": task.execution_time_ms,
                        "keys_invalidated": result.keys_invalidated,
                        "success": result.success,
                        "trigger": task.event.trigger.value,
                        "priority": task.event.priority.value,
                        "timestamp": task.completed_at.isoformat(),
                    }
                )

            logger.debug(
                f"Task {task.id} completed: {result.keys_invalidated} keys invalidated "
                f"in {task.execution_time_ms:.2f}ms"
            )

        except Exception as e:
            logger.error(f"Task {task.id} execution failed: {e}")
            task.result = InvalidationResult(
                success=False,
                keys_invalidated=0,
                execution_time_ms=0,
                strategy_used="error",
                errors=[str(e)],
            )
            task.completed_at = datetime.utcnow()
            self.metrics.update(task)

    def _handle_completed_tasks(self) -> None:
        """Move completed tasks from active to completed"""
        completed_task_ids = [
            task_id for task_id, task in self.active_tasks.items() if task.is_completed
        ]

        for task_id in completed_task_ids:
            task = self.active_tasks.pop(task_id)
            self.completed_tasks.append(task)

    async def _handle_retries(self) -> None:
        """Handle task retries for failed tasks"""
        retry_tasks = [task for task in self.active_tasks.values() if task.should_retry]

        for task in retry_tasks:
            # Remove from active tasks
            self.active_tasks.pop(task.id, None)

            # Reset for retry
            task.retry_count += 1
            task.started_at = None
            task.completed_at = None
            task.result = None

            # Add back to queue with appropriate priority
            # Lower priority for retries to prevent blocking new tasks
            retry_priority = InvalidationPriority.LOW
            self.task_queues[retry_priority].append(task)

            logger.info(f"Retrying task {task.id} (attempt {task.retry_count})")

    async def _drain_remaining_tasks(self) -> None:
        """Process remaining tasks during shutdown"""
        logger.info("Draining remaining invalidation tasks...")

        # Wait for active tasks to complete
        timeout = 10.0  # 10 second timeout
        start_time = time.time()

        while self.active_tasks and (time.time() - start_time) < timeout:
            self._handle_completed_tasks()
            await asyncio.sleep(0.1)

        if self.active_tasks:
            logger.warning(
                f"Shutdown timeout: {len(self.active_tasks)} tasks still active"
            )

        # Process high and critical priority tasks from queues
        remaining_tasks = 0
        for priority in [InvalidationPriority.CRITICAL, InvalidationPriority.HIGH]:
            queue = self.task_queues[priority]
            while (
                queue and remaining_tasks < 100
            ):  # Limit to prevent infinite processing
                task = queue.popleft()
                await self._execute_task(task)
                remaining_tasks += 1

        if remaining_tasks > 0:
            logger.info(f"Processed {remaining_tasks} remaining high-priority tasks")

    def _refresh_rate_limit_tokens(self) -> None:
        """Refresh rate limit tokens based on time elapsed"""
        current_time = time.time()
        elapsed = current_time - self.last_token_refresh

        if elapsed >= self.rate_limit_window:
            # Add tokens based on elapsed time
            tokens_to_add = int(elapsed * self.rate_limit_per_second)
            self.rate_limit_tokens = min(
                self.rate_limit_per_second, self.rate_limit_tokens + tokens_to_add
            )
            self.last_token_refresh = current_time

    def _determine_priority(self, trigger: InvalidationTrigger) -> InvalidationPriority:
        """Determine invalidation priority based on trigger"""
        priority_map = {
            InvalidationTrigger.SECURITY_EVENT: InvalidationPriority.CRITICAL,
            InvalidationTrigger.USER_LOGIN: InvalidationPriority.CRITICAL,
            InvalidationTrigger.USER_LOGOUT: InvalidationPriority.CRITICAL,
            InvalidationTrigger.USER_ROLE_CHANGE: InvalidationPriority.HIGH,
            InvalidationTrigger.USER_PERMISSION_CHANGE: InvalidationPriority.HIGH,
            InvalidationTrigger.USER_CONTEXT_CHANGE: InvalidationPriority.HIGH,
            InvalidationTrigger.CLIENT_ACCESS_CHANGE: InvalidationPriority.MEDIUM,
            InvalidationTrigger.ENGAGEMENT_CHANGE: InvalidationPriority.MEDIUM,
            InvalidationTrigger.DATA_MODIFICATION: InvalidationPriority.MEDIUM,
            InvalidationTrigger.MANUAL_INVALIDATION: InvalidationPriority.MEDIUM,
            InvalidationTrigger.TTL_EXPIRATION: InvalidationPriority.LOW,
            InvalidationTrigger.BACKGROUND_REFRESH: InvalidationPriority.BACKGROUND,
        }

        return priority_map.get(trigger, InvalidationPriority.MEDIUM)

    def _create_fallback_strategy(self) -> BaseInvalidationStrategy:
        """Create fallback strategy when no specific strategy matches"""
        from app.services.caching.invalidation_strategies import (
            EventDrivenInvalidationStrategy,
        )

        return EventDrivenInvalidationStrategy(self.redis_cache)

    async def _fire_event_handlers(self, event: InvalidationEvent) -> None:
        """Fire registered event handlers"""
        handlers = self._event_handlers.get(event.trigger, [])

        for handler in handlers:
            try:
                if asyncio.iscoroutinefunction(handler):
                    await handler(event)
                else:
                    handler(event)
            except Exception as e:
                logger.error(f"Event handler failed for {event.trigger}: {e}")

    async def _wait_for_task_completion(self, task_id: str) -> InvalidationResult:
        """Wait for a specific task to complete"""
        timeout = 30.0  # 30 second timeout
        start_time = time.time()

        while (time.time() - start_time) < timeout:
            # Check if task is completed
            for task in self.completed_tasks:
                if task.id == task_id:
                    return task.result or InvalidationResult(
                        success=False,
                        keys_invalidated=0,
                        execution_time_ms=0,
                        strategy_used="timeout",
                        errors=["Task completed without result"],
                    )

            # Check if task is still active
            if task_id in self.active_tasks:
                await asyncio.sleep(0.1)
                continue

            # Task might be in queue still
            await asyncio.sleep(0.1)

        # Timeout reached
        return InvalidationResult(
            success=False,
            keys_invalidated=0,
            execution_time_ms=(time.time() - start_time) * 1000,
            strategy_used="timeout",
            errors=["Task completion timeout"],
        )


# Factory function for dependency injection
def create_cache_invalidation_manager(
    redis_cache: RedisCache,
    auth_cache_service: AuthCacheService,
    coherence_manager: Optional[CacheCoherenceManager] = None,
) -> CacheInvalidationManager:
    """
    Factory function to create CacheInvalidationManager with dependencies.
    """
    return CacheInvalidationManager(
        redis_cache=redis_cache,
        auth_cache_service=auth_cache_service,
        coherence_manager=coherence_manager,
    )


# Singleton instance for global access
_cache_invalidation_manager: Optional[CacheInvalidationManager] = None


def get_cache_invalidation_manager() -> Optional[CacheInvalidationManager]:
    """Get singleton CacheInvalidationManager instance"""
    return _cache_invalidation_manager


def set_cache_invalidation_manager(manager: CacheInvalidationManager) -> None:
    """Set singleton CacheInvalidationManager instance"""
    global _cache_invalidation_manager
    _cache_invalidation_manager = manager


# Export all classes
__all__ = [
    "InvalidationTask",
    "InvalidationMetrics",
    "CacheInvalidationManager",
    "create_cache_invalidation_manager",
    "get_cache_invalidation_manager",
    "set_cache_invalidation_manager",
]
