"""
Event-Driven Cache Invalidator for Auth Performance Optimization

This module implements an event-driven cache invalidation system that:
- Listens for database change events via SQLAlchemy ORM hooks
- Processes user action events (login, logout, context changes, etc.)
- Handles bulk operations with efficient batching
- Supports real-time cache updates for critical data changes
- Integrates with the CacheInvalidationManager for coordinated invalidation

The invalidator uses SQLAlchemy event listeners and custom event dispatching
to ensure cache consistency across the auth performance system.

ðŸ”’ Security: Multi-tenant isolation and secure event processing
âš¡ Performance: Optimized event processing with minimal database overhead
ðŸŽ¯ Coherence: Real-time cache updates with dependency tracking
ðŸ“Š Analytics: Comprehensive event tracking and metrics collection

Generated by CC (Claude Code)
"""

import asyncio
import json
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Set, Type

from sqlalchemy import event, inspect

from app.core.config import settings
from app.core.logging import get_logger
from app.models.base import Base
from app.services.caching.cache_invalidation_manager import CacheInvalidationManager
from app.services.caching.invalidation_strategies import (
    InvalidationEvent,
    InvalidationPriority,
    InvalidationTrigger,
)

logger = get_logger(__name__)


class EventSource(Enum):
    """Sources of cache invalidation events"""

    DATABASE_ORM = "database_orm"  # SQLAlchemy ORM events
    USER_ACTION = "user_action"  # User interface actions
    API_ENDPOINT = "api_endpoint"  # API endpoint calls
    BACKGROUND_TASK = "background_task"  # Background/scheduled tasks
    SYSTEM_EVENT = "system_event"  # System-level events
    WEBHOOK = "webhook"  # External webhook events


@dataclass
class CacheInvalidationEvent:
    """Represents a cache invalidation event from various sources"""

    source: EventSource
    trigger: InvalidationTrigger
    entity_type: str
    entity_id: str
    client_account_id: str
    user_id: Optional[str] = None
    engagement_id: Optional[str] = None
    session_id: Optional[str] = None
    endpoint: Optional[str] = None
    operation: str = "unknown"  # CREATE, UPDATE, DELETE, etc.
    old_values: Optional[Dict[str, Any]] = None
    new_values: Optional[Dict[str, Any]] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.utcnow)
    correlation_id: Optional[str] = None

    def to_invalidation_event(self) -> InvalidationEvent:
        """Convert to InvalidationEvent for processing"""
        return InvalidationEvent(
            trigger=self.trigger,
            priority=self._determine_priority(),
            entity_type=self.entity_type,
            entity_id=self.entity_id,
            client_account_id=self.client_account_id,
            user_id=self.user_id,
            engagement_id=self.engagement_id,
            metadata=self._build_metadata(),
            timestamp=self.timestamp,
            correlation_id=self.correlation_id,
        )

    def _determine_priority(self) -> InvalidationPriority:
        """Determine priority based on event characteristics"""
        # Security-related events get highest priority
        if self.trigger in [
            InvalidationTrigger.SECURITY_EVENT,
            InvalidationTrigger.USER_LOGIN,
            InvalidationTrigger.USER_LOGOUT,
        ]:
            return InvalidationPriority.CRITICAL

        # User context and role changes are high priority
        if self.trigger in [
            InvalidationTrigger.USER_CONTEXT_CHANGE,
            InvalidationTrigger.USER_ROLE_CHANGE,
            InvalidationTrigger.USER_PERMISSION_CHANGE,
        ]:
            return InvalidationPriority.HIGH

        # Data modifications and access changes are medium priority
        if self.trigger in [
            InvalidationTrigger.DATA_MODIFICATION,
            InvalidationTrigger.CLIENT_ACCESS_CHANGE,
            InvalidationTrigger.ENGAGEMENT_CHANGE,
        ]:
            return InvalidationPriority.MEDIUM

        return InvalidationPriority.LOW

    def _build_metadata(self) -> Dict[str, Any]:
        """Build metadata dictionary for the invalidation event"""
        metadata = self.metadata.copy()
        metadata.update(
            {
                "source": self.source.value,
                "operation": self.operation,
                "endpoint": self.endpoint,
                "session_id": self.session_id,
            }
        )

        if self.old_values:
            metadata["old_values"] = self.old_values
        if self.new_values:
            metadata["new_values"] = self.new_values

        return metadata


@dataclass
class EventProcessingStats:
    """Statistics for event processing performance"""

    total_events: int = 0
    events_by_source: Dict[EventSource, int] = field(
        default_factory=lambda: defaultdict(int)
    )
    events_by_trigger: Dict[InvalidationTrigger, int] = field(
        default_factory=lambda: defaultdict(int)
    )
    events_by_entity_type: Dict[str, int] = field(
        default_factory=lambda: defaultdict(int)
    )
    total_processing_time_ms: float = 0
    average_processing_time_ms: float = 0
    failed_events: int = 0
    last_reset: datetime = field(default_factory=datetime.utcnow)

    def update(
        self, event: CacheInvalidationEvent, processing_time_ms: float, success: bool
    ) -> None:
        """Update statistics with processed event"""
        self.total_events += 1
        self.events_by_source[event.source] += 1
        self.events_by_trigger[event.trigger] += 1
        self.events_by_entity_type[event.entity_type] += 1
        self.total_processing_time_ms += processing_time_ms

        if self.total_events > 0:
            self.average_processing_time_ms = (
                self.total_processing_time_ms / self.total_events
            )

        if not success:
            self.failed_events += 1


class EntityChangeDetector:
    """Detects meaningful changes in entity attributes for cache invalidation"""

    # Define which attributes trigger cache invalidation for each entity type
    CACHE_SENSITIVE_ATTRIBUTES = {
        "User": {
            "email",
            "is_active",
            "role",
            "permissions",
            "client_associations",
            "default_client_id",
            "default_engagement_id",
            "last_login",
        },
        "ClientAccount": {
            "name",
            "is_active",
            "settings",
            "subscription_status",
        },
        "Engagement": {
            "name",
            "status",
            "is_active",
            "client_account_id",
            "settings",
        },
        "UserClientAssociation": {
            "user_id",
            "client_account_id",
            "role",
            "permissions",
            "is_active",
        },
        "Flow": {
            "status",
            "phase",
            "progress",
            "engagement_id",
            "is_active",
        },
        "Import": {
            "status",
            "validation_status",
            "field_mappings",
            "engagement_id",
        },
    }

    @classmethod
    def has_cache_relevant_changes(
        cls, entity: Any, old_values: Dict[str, Any], new_values: Dict[str, Any]
    ) -> bool:
        """Check if entity changes are relevant for cache invalidation"""
        entity_type = entity.__class__.__name__
        sensitive_attrs = cls.CACHE_SENSITIVE_ATTRIBUTES.get(entity_type, set())

        if not sensitive_attrs:
            # If no specific attributes defined, consider all changes relevant
            return True

        # Check if any sensitive attribute changed
        for attr in sensitive_attrs:
            if old_values.get(attr) != new_values.get(attr):
                return True

        return False

    @classmethod
    def get_changed_attributes(
        cls, old_values: Dict[str, Any], new_values: Dict[str, Any]
    ) -> Set[str]:
        """Get set of attributes that changed"""
        changed = set()
        all_keys = set(old_values.keys()) | set(new_values.keys())

        for key in all_keys:
            if old_values.get(key) != new_values.get(key):
                changed.add(key)

        return changed


class EventDrivenInvalidator:
    """
    Event-driven cache invalidator that listens for various events and
    triggers appropriate cache invalidation strategies.

    Features:
    - SQLAlchemy ORM event listeners for database changes
    - User action event processing
    - Bulk operation handling with batching
    - Real-time event processing with minimal latency
    - Comprehensive event tracking and metrics
    """

    def __init__(self, invalidation_manager: CacheInvalidationManager):
        self.invalidation_manager = invalidation_manager
        self.stats = EventProcessingStats()

        # Event queuing and batching
        self.event_queue: deque = deque()
        self.batch_size = settings.get("CACHE_INVALIDATION_BATCH_SIZE", 50)
        self.batch_timeout_ms = settings.get("CACHE_INVALIDATION_BATCH_TIMEOUT_MS", 100)
        self.last_batch_time = time.time()

        # Event filtering and deduplication
        self.event_filters: List[Callable[[CacheInvalidationEvent], bool]] = []
        self.recent_events: deque = deque(
            maxlen=1000
        )  # Keep last 1000 events for deduplication
        self.deduplication_window_ms = 5000  # 5 second deduplication window

        # SQLAlchemy event listeners
        self._orm_listeners_registered = False
        self._entity_change_detector = EntityChangeDetector()

        # Background processing
        self._processing_task: Optional[asyncio.Task] = None
        self._shutdown_event = asyncio.Event()

        logger.info("EventDrivenInvalidator initialized")

    def start(self) -> None:
        """Start the event-driven invalidator"""
        if not self._orm_listeners_registered:
            self._register_orm_listeners()

        if self._processing_task is None:
            self._shutdown_event.clear()
            self._processing_task = asyncio.create_task(self._background_processing())

        logger.info("EventDrivenInvalidator started")

    async def stop(self) -> None:
        """Stop the event-driven invalidator"""
        logger.info("Stopping EventDrivenInvalidator...")

        if self._processing_task:
            self._shutdown_event.set()
            try:
                await asyncio.wait_for(self._processing_task, timeout=10.0)
            except asyncio.TimeoutError:
                logger.warning("EventDrivenInvalidator shutdown timeout")
                self._processing_task.cancel()
            self._processing_task = None

        self._unregister_orm_listeners()
        logger.info("EventDrivenInvalidator stopped")

    def add_event_filter(
        self, filter_func: Callable[[CacheInvalidationEvent], bool]
    ) -> None:
        """Add event filter function"""
        self.event_filters.append(filter_func)

    def emit_user_login_event(
        self,
        user_id: str,
        client_account_id: str,
        session_id: Optional[str] = None,
        endpoint: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Emit user login event"""
        event = CacheInvalidationEvent(
            source=EventSource.USER_ACTION,
            trigger=InvalidationTrigger.USER_LOGIN,
            entity_type="user",
            entity_id=user_id,
            client_account_id=client_account_id,
            user_id=user_id,
            session_id=session_id,
            endpoint=endpoint,
            operation="LOGIN",
            metadata=metadata or {},
        )
        self._queue_event(event)

    def emit_user_logout_event(
        self,
        user_id: str,
        client_account_id: str,
        session_id: Optional[str] = None,
        endpoint: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Emit user logout event"""
        event = CacheInvalidationEvent(
            source=EventSource.USER_ACTION,
            trigger=InvalidationTrigger.USER_LOGOUT,
            entity_type="user",
            entity_id=user_id,
            client_account_id=client_account_id,
            user_id=user_id,
            session_id=session_id,
            endpoint=endpoint,
            operation="LOGOUT",
            metadata=metadata or {},
        )
        self._queue_event(event)

    def emit_user_context_change_event(
        self,
        user_id: str,
        client_account_id: str,
        old_context: Optional[Dict[str, Any]] = None,
        new_context: Optional[Dict[str, Any]] = None,
        session_id: Optional[str] = None,
        endpoint: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Emit user context change event"""
        event = CacheInvalidationEvent(
            source=EventSource.USER_ACTION,
            trigger=InvalidationTrigger.USER_CONTEXT_CHANGE,
            entity_type="user",
            entity_id=user_id,
            client_account_id=client_account_id,
            user_id=user_id,
            session_id=session_id,
            endpoint=endpoint,
            operation="CONTEXT_CHANGE",
            old_values=old_context,
            new_values=new_context,
            metadata=metadata or {},
        )
        self._queue_event(event)

    def emit_user_role_change_event(
        self,
        user_id: str,
        client_account_id: str,
        old_role: Optional[str] = None,
        new_role: Optional[str] = None,
        session_id: Optional[str] = None,
        endpoint: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Emit user role change event"""
        event = CacheInvalidationEvent(
            source=EventSource.USER_ACTION,
            trigger=InvalidationTrigger.USER_ROLE_CHANGE,
            entity_type="user",
            entity_id=user_id,
            client_account_id=client_account_id,
            user_id=user_id,
            session_id=session_id,
            endpoint=endpoint,
            operation="ROLE_CHANGE",
            old_values={"role": old_role} if old_role else None,
            new_values={"role": new_role} if new_role else None,
            metadata=metadata or {},
        )
        self._queue_event(event)

    def emit_security_event(
        self,
        event_type: str,
        user_id: str,
        client_account_id: str,
        severity: str = "high",
        session_id: Optional[str] = None,
        endpoint: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Emit security event"""
        event_metadata = metadata or {}
        event_metadata.update(
            {
                "event_type": event_type,
                "severity": severity,
            }
        )

        event = CacheInvalidationEvent(
            source=EventSource.SYSTEM_EVENT,
            trigger=InvalidationTrigger.SECURITY_EVENT,
            entity_type="user",
            entity_id=user_id,
            client_account_id=client_account_id,
            user_id=user_id,
            session_id=session_id,
            endpoint=endpoint,
            operation="SECURITY_EVENT",
            metadata=event_metadata,
        )
        self._queue_event(event)

    def emit_bulk_data_change_event(
        self,
        entity_type: str,
        entity_ids: List[str],
        client_account_id: str,
        operation: str = "BULK_UPDATE",
        user_id: Optional[str] = None,
        endpoint: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None,
    ) -> None:
        """Emit bulk data change event"""
        event_metadata = metadata or {}
        event_metadata.update(
            {
                "bulk_operation": True,
                "entity_count": len(entity_ids),
                "all_entity_ids": entity_ids,
            }
        )

        # Create events for each entity but mark them as part of bulk operation
        for entity_id in entity_ids:
            event = CacheInvalidationEvent(
                source=EventSource.API_ENDPOINT,
                trigger=InvalidationTrigger.DATA_MODIFICATION,
                entity_type=entity_type,
                entity_id=entity_id,
                client_account_id=client_account_id,
                user_id=user_id,
                endpoint=endpoint,
                operation=operation,
                metadata=event_metadata,
            )
            self._queue_event(event)

    def get_stats(self) -> Dict[str, Any]:
        """Get event processing statistics"""
        return {
            "total_events": self.stats.total_events,
            "failed_events": self.stats.failed_events,
            "success_rate": (
                (self.stats.total_events - self.stats.failed_events)
                / max(self.stats.total_events, 1)
                * 100
            ),
            "average_processing_time_ms": self.stats.average_processing_time_ms,
            "events_by_source": {
                source.value: count
                for source, count in self.stats.events_by_source.items()
            },
            "events_by_trigger": {
                trigger.value: count
                for trigger, count in self.stats.events_by_trigger.items()
            },
            "events_by_entity_type": dict(self.stats.events_by_entity_type),
            "queue_size": len(self.event_queue),
            "last_reset": self.stats.last_reset.isoformat(),
        }

    def reset_stats(self) -> None:
        """Reset event processing statistics"""
        self.stats = EventProcessingStats()
        logger.info("Event processing statistics reset")

    # Private methods

    def _queue_event(self, event: CacheInvalidationEvent) -> None:
        """Queue event for processing with deduplication"""
        # Check if this is a duplicate recent event
        if self._is_duplicate_event(event):
            logger.debug(
                f"Skipping duplicate event: {event.entity_type}:{event.entity_id}"
            )
            return

        # Apply filters
        for filter_func in self.event_filters:
            try:
                if not filter_func(event):
                    logger.debug(
                        f"Event filtered out: {event.entity_type}:{event.entity_id}"
                    )
                    return
            except Exception as e:
                logger.error(f"Event filter error: {e}")

        # Add to queue
        self.event_queue.append(event)
        self.recent_events.append(event)

        logger.debug(
            f"Queued event: {event.trigger.value} for {event.entity_type}:{event.entity_id}"
        )

    def _is_duplicate_event(self, event: CacheInvalidationEvent) -> bool:
        """Check if event is a duplicate of a recent event"""
        cutoff_time = event.timestamp - timedelta(
            milliseconds=self.deduplication_window_ms
        )

        for recent_event in reversed(self.recent_events):  # Check most recent first
            if recent_event.timestamp < cutoff_time:
                break  # No need to check older events

            if (
                recent_event.trigger == event.trigger
                and recent_event.entity_type == event.entity_type
                and recent_event.entity_id == event.entity_id
                and recent_event.client_account_id == event.client_account_id
                and recent_event.operation == event.operation
            ):
                return True

        return False

    async def _background_processing(self) -> None:
        """Background task to process queued events"""
        logger.info("Starting event processing background task")

        while not self._shutdown_event.is_set():
            try:
                # Process events in batches
                await self._process_event_batch()

                # Small delay to prevent tight loop
                await asyncio.sleep(0.01)  # 10ms

            except Exception as e:
                logger.error(f"Error in event processing: {e}")
                await asyncio.sleep(0.1)  # Longer delay on error

        # Process remaining events before shutdown
        await self._drain_remaining_events()
        logger.info("Event processing background task stopped")

    async def _process_event_batch(self) -> None:
        """Process a batch of events"""
        if not self.event_queue:
            return

        # Check if we should process a batch
        current_time = time.time()
        should_process_batch = (
            len(self.event_queue) >= self.batch_size
            or (current_time - self.last_batch_time) * 1000 >= self.batch_timeout_ms
        )

        if not should_process_batch:
            return

        # Collect batch
        batch = []
        batch_size = min(self.batch_size, len(self.event_queue))

        for _ in range(batch_size):
            if self.event_queue:
                batch.append(self.event_queue.popleft())

        if not batch:
            return

        # Process batch
        start_time = time.time()

        for cache_event in batch:
            await self._process_single_event(cache_event)

        processing_time = (time.time() - start_time) * 1000
        self.last_batch_time = current_time

        logger.debug(
            f"Processed batch of {len(batch)} events in {processing_time:.2f}ms"
        )

    async def _process_single_event(self, event: CacheInvalidationEvent) -> None:
        """Process a single cache invalidation event"""
        start_time = time.time()
        success = True

        try:
            # Convert to InvalidationEvent and submit to manager
            invalidation_event = event.to_invalidation_event()

            # Submit to invalidation manager (non-blocking)
            task_id = await self.invalidation_manager.invalidate(
                trigger=invalidation_event.trigger,
                entity_type=invalidation_event.entity_type,
                entity_id=invalidation_event.entity_id,
                client_account_id=invalidation_event.client_account_id,
                user_id=invalidation_event.user_id,
                engagement_id=invalidation_event.engagement_id,
                priority=invalidation_event.priority,
                metadata=invalidation_event.metadata,
                wait_for_completion=False,  # Non-blocking
            )

            logger.debug(
                f"Submitted invalidation task {task_id} for event {event.entity_type}:{event.entity_id}"
            )

        except Exception as e:
            logger.error(
                f"Failed to process event {event.entity_type}:{event.entity_id}: {e}"
            )
            success = False

        # Update statistics
        processing_time_ms = (time.time() - start_time) * 1000
        self.stats.update(event, processing_time_ms, success)

    async def _drain_remaining_events(self) -> None:
        """Process remaining events during shutdown"""
        logger.info(f"Draining {len(self.event_queue)} remaining events...")

        while self.event_queue:
            event = self.event_queue.popleft()
            await self._process_single_event(event)

        logger.info("Finished draining remaining events")

    def _register_orm_listeners(self) -> None:
        """Register SQLAlchemy ORM event listeners"""
        if self._orm_listeners_registered:
            return

        # Register listeners for all model classes
        for model_class in self._get_model_classes():
            event.listen(model_class, "after_insert", self._on_orm_insert)
            event.listen(model_class, "after_update", self._on_orm_update)
            event.listen(model_class, "after_delete", self._on_orm_delete)

        self._orm_listeners_registered = True
        logger.info("SQLAlchemy ORM event listeners registered")

    def _unregister_orm_listeners(self) -> None:
        """Unregister SQLAlchemy ORM event listeners"""
        if not self._orm_listeners_registered:
            return

        # Unregister listeners for all model classes
        for model_class in self._get_model_classes():
            event.remove(model_class, "after_insert", self._on_orm_insert)
            event.remove(model_class, "after_update", self._on_orm_update)
            event.remove(model_class, "after_delete", self._on_orm_delete)

        self._orm_listeners_registered = False
        logger.info("SQLAlchemy ORM event listeners unregistered")

    def _get_model_classes(self) -> List[Type]:
        """Get all SQLAlchemy model classes"""
        # Return all classes that inherit from Base
        model_classes = []
        for cls in Base.__subclasses__():
            if hasattr(cls, "__tablename__"):
                model_classes.append(cls)
        return model_classes

    def _on_orm_insert(self, mapper, connection, target) -> None:
        """Handle ORM insert events"""
        try:
            entity_info = self._extract_entity_info(target)
            if not entity_info:
                return

            event = CacheInvalidationEvent(
                source=EventSource.DATABASE_ORM,
                trigger=InvalidationTrigger.DATA_MODIFICATION,
                entity_type=entity_info["entity_type"],
                entity_id=entity_info["entity_id"],
                client_account_id=entity_info["client_account_id"],
                user_id=entity_info.get("user_id"),
                engagement_id=entity_info.get("engagement_id"),
                operation="INSERT",
                new_values=self._extract_entity_values(target),
            )

            self._queue_event(event)

        except Exception as e:
            logger.error(f"Error handling ORM insert: {e}")

    def _on_orm_update(self, mapper, connection, target) -> None:
        """Handle ORM update events"""
        try:
            entity_info = self._extract_entity_info(target)
            if not entity_info:
                return

            # Get old and new values
            old_values = {}
            new_values = self._extract_entity_values(target)

            # Get changed attributes from SQLAlchemy inspection
            state = inspect(target)
            if state.committed_state:
                for attr in state.committed_state:
                    if attr in state.committed_state:
                        old_values[attr] = state.committed_state[attr]

            # Check if changes are cache-relevant
            if not self._entity_change_detector.has_cache_relevant_changes(
                target, old_values, new_values
            ):
                return

            event = CacheInvalidationEvent(
                source=EventSource.DATABASE_ORM,
                trigger=InvalidationTrigger.DATA_MODIFICATION,
                entity_type=entity_info["entity_type"],
                entity_id=entity_info["entity_id"],
                client_account_id=entity_info["client_account_id"],
                user_id=entity_info.get("user_id"),
                engagement_id=entity_info.get("engagement_id"),
                operation="UPDATE",
                old_values=old_values,
                new_values=new_values,
            )

            self._queue_event(event)

        except Exception as e:
            logger.error(f"Error handling ORM update: {e}")

    def _on_orm_delete(self, mapper, connection, target) -> None:
        """Handle ORM delete events"""
        try:
            entity_info = self._extract_entity_info(target)
            if not entity_info:
                return

            event = CacheInvalidationEvent(
                source=EventSource.DATABASE_ORM,
                trigger=InvalidationTrigger.DATA_MODIFICATION,
                entity_type=entity_info["entity_type"],
                entity_id=entity_info["entity_id"],
                client_account_id=entity_info["client_account_id"],
                user_id=entity_info.get("user_id"),
                engagement_id=entity_info.get("engagement_id"),
                operation="DELETE",
                old_values=self._extract_entity_values(target),
            )

            self._queue_event(event)

        except Exception as e:
            logger.error(f"Error handling ORM delete: {e}")

    def _extract_entity_info(self, entity: Any) -> Optional[Dict[str, Any]]:
        """Extract entity information for cache invalidation"""
        entity_type = entity.__class__.__name__.lower()

        # Extract entity ID
        entity_id = None
        if hasattr(entity, "id"):
            entity_id = str(entity.id)
        elif hasattr(entity, "uuid"):
            entity_id = str(entity.uuid)
        else:
            return None

        # Extract client account ID for multi-tenant isolation
        client_account_id = None
        if hasattr(entity, "client_account_id"):
            client_account_id = str(entity.client_account_id)
        elif hasattr(entity, "client_id"):
            client_account_id = str(entity.client_id)
        else:
            # Some entities might not have direct client association
            # For now, we'll skip them, but could be enhanced to look up relationships
            return None

        result = {
            "entity_type": entity_type,
            "entity_id": entity_id,
            "client_account_id": client_account_id,
        }

        # Extract optional fields
        if hasattr(entity, "user_id"):
            result["user_id"] = str(entity.user_id)
        if hasattr(entity, "engagement_id"):
            result["engagement_id"] = str(entity.engagement_id)

        return result

    def _extract_entity_values(self, entity: Any) -> Dict[str, Any]:
        """Extract entity attribute values for change tracking"""
        values = {}

        # Get all column attributes
        if hasattr(entity, "__table__"):
            for column in entity.__table__.columns:
                attr_name = column.name
                if hasattr(entity, attr_name):
                    attr_value = getattr(entity, attr_name)
                    # Convert to serializable format
                    if attr_value is not None:
                        try:
                            json.dumps(attr_value, default=str)  # Test serializability
                            values[attr_name] = attr_value
                        except (TypeError, ValueError):
                            values[attr_name] = str(attr_value)

        return values


# Factory function for dependency injection
def create_event_driven_invalidator(
    invalidation_manager: CacheInvalidationManager,
) -> EventDrivenInvalidator:
    """Create EventDrivenInvalidator with dependencies"""
    return EventDrivenInvalidator(invalidation_manager)


# Singleton instance for global access
_event_driven_invalidator: Optional[EventDrivenInvalidator] = None


def get_event_driven_invalidator() -> Optional[EventDrivenInvalidator]:
    """Get singleton EventDrivenInvalidator instance"""
    return _event_driven_invalidator


def set_event_driven_invalidator(invalidator: EventDrivenInvalidator) -> None:
    """Set singleton EventDrivenInvalidator instance"""
    global _event_driven_invalidator
    _event_driven_invalidator = invalidator


# Export all classes
__all__ = [
    "EventSource",
    "CacheInvalidationEvent",
    "EventProcessingStats",
    "EntityChangeDetector",
    "EventDrivenInvalidator",
    "create_event_driven_invalidator",
    "get_event_driven_invalidator",
    "set_event_driven_invalidator",
]
