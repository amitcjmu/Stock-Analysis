"""
Main field mapping service implementation.

This module contains the FieldMappingService class which orchestrates
all field mapping operations and maintains session management.
"""

from datetime import datetime
from typing import Any, Dict, List, Optional, Set
from uuid import UUID

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, and_, func
from sqlalchemy.exc import IntegrityError

from app.core.context import RequestContext
from app.services.base_service import ServiceBase
from app.models.data_import.mapping import ImportFieldMapping as FieldMapping

from .base import (
    MappingAnalysis,
    MappingRule,
    BASE_MAPPINGS,
    REQUIRED_FIELDS,
    logger,
)
from .mappers import FieldMappingAnalyzer
from .validators import FieldMappingValidator


class FieldMappingService(ServiceBase):
    """
    Service for managing field mappings in the discovery flow.

    This service:
    - Manages field mapping rules and transformations
    - Learns from user feedback and AI analysis
    - Provides mapping suggestions and validation
    - Maintains multi-tenant context for mappings
    - Never commits or closes the database session
    """

    # Base mappings for common field variations
    BASE_MAPPINGS = BASE_MAPPINGS

    # Required fields for different asset types
    REQUIRED_FIELDS = REQUIRED_FIELDS

    def __init__(self, session: AsyncSession, context: RequestContext):
        """
        Initialize FieldMappingService.

        Args:
            session: Database session from orchestrator
            context: Request context with tenant information
        """
        super().__init__(session, context)

        # Initialize mapping caches
        self._learned_mappings_cache: Optional[Dict[str, List[MappingRule]]] = None
        self._negative_mappings_cache: Set[tuple] = set()

        # Initialize analyzer
        self._analyzer = FieldMappingAnalyzer(
            learned_mappings_cache=self._learned_mappings_cache,
            negative_mappings_cache=self._negative_mappings_cache
        )

        self.logger.debug(
            f"Initialized FieldMappingService for client {context.client_account_id}"
        )

    async def health_check(self) -> Dict[str, Any]:
        """
        Perform health check for FieldMappingService.

        Returns:
            Health status and metrics
        """
        try:
            # Count mappings in database
            query = select(func.count(FieldMapping.id)).where(
                FieldMapping.client_account_id == self.context.client_account_id
            )
            result = await self.session.execute(query)
            mapping_count = result.scalar() or 0

            return {
                "status": "healthy",
                "service": "FieldMappingService",
                "client_account_id": self.context.client_account_id,
                "engagement_id": self.context.engagement_id,
                "mapping_count": mapping_count,
                "base_mappings": len(self.BASE_MAPPINGS),
                "cached_learned_mappings": (
                    len(self._learned_mappings_cache)
                    if self._learned_mappings_cache
                    else 0
                ),
            }
        except Exception as e:
            await self.record_failure(
                operation="health_check",
                error=e,
                context_data={"service": "FieldMappingService"},
            )
            return {
                "status": "unhealthy",
                "service": "FieldMappingService",
                "error": str(e),
            }

    async def analyze_columns(
        self,
        columns: List[str],
        data_import_id: Optional[UUID] = None,
        master_flow_id: Optional[UUID] = None,
        asset_type: str = "server",
        sample_data: Optional[List[List[Any]]] = None,
    ) -> MappingAnalysis:
        """
        Analyze columns and provide mapping insights.

        Args:
            columns: List of column names to analyze
            data_import_id: Optional data import ID for context
            master_flow_id: Optional master flow ID for context
            asset_type: Type of asset being analyzed
            sample_data: Optional sample data for content analysis

        Returns:
            MappingAnalysis with mapping suggestions and confidence scores
        """
        try:
            # Load learned mappings if not cached
            if self._learned_mappings_cache is None:
                await self._load_learned_mappings(data_import_id, master_flow_id)

            # Update analyzer with current cache
            self._analyzer.learned_mappings_cache = self._learned_mappings_cache
            self._analyzer.negative_mappings_cache = self._negative_mappings_cache

            return await self._analyzer.analyze_columns(columns, asset_type, sample_data)

        except Exception as e:
            await self.record_failure(
                operation="analyze_columns",
                error=e,
                context_data={"column_count": len(columns), "asset_type": asset_type},
            )
            # Return empty analysis on failure
            return MappingAnalysis(
                mapped_fields={},
                unmapped_fields=columns,
                suggested_mappings={},
                confidence_scores={col: 0.0 for col in columns},
                missing_required_fields=self.REQUIRED_FIELDS.get(asset_type, []),
                overall_confidence=0.0,
            )

    async def learn_field_mapping(
        self,
        source_field: str,
        target_field: str,
        data_import_id: UUID,
        master_flow_id: Optional[UUID] = None,
        confidence: float = 0.9,
        source: str = "user",
        context: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Learn a new field mapping.

        Args:
            source_field: Source field name
            target_field: Target canonical field name
            data_import_id: Required data import ID
            master_flow_id: Optional master flow ID
            confidence: Confidence score (0-1)
            source: Source of the mapping (user, ai, system)
            context: Optional context information

        Returns:
            Result of learning operation
        """
        try:
            normalized_source = self._normalize_field_name(source_field)
            normalized_target = self._normalize_field_name(target_field)

            # Check if this is a negative mapping we've seen before
            if (normalized_source, normalized_target) in self._negative_mappings_cache:
                return {
                    "success": False,
                    "message": f"Mapping {source_field} -> {target_field} was previously rejected",
                }

            # Create or update mapping in database
            existing_mapping = await self._get_existing_mapping(
                normalized_source, normalized_target, data_import_id
            )

            if existing_mapping:
                # Update confidence if this is higher
                if confidence > existing_mapping.confidence_score:
                    existing_mapping.confidence_score = confidence
                    existing_mapping.updated_at = datetime.utcnow()
                    existing_mapping.suggested_by = source
                    if context:
                        existing_mapping.transformation_rules = (
                            existing_mapping.transformation_rules or {}
                        )
                        existing_mapping.transformation_rules["context"] = context

                    # Flush to get ID without committing
                    await self.flush_for_id()

                    self.logger.info(
                        f"Updated field mapping: {source_field} -> {target_field} "
                        f"(confidence: {confidence})"
                    )

                    # Update cache
                    self._analyzer.update_cache(
                        normalized_source, normalized_target, confidence, source
                    )

                    return {
                        "success": True,
                        "action": "updated",
                        "mapping_id": str(existing_mapping.id),
                        "confidence": confidence,
                    }
                else:
                    return {
                        "success": True,
                        "action": "exists",
                        "message": "Mapping already exists with equal or higher confidence",
                        "existing_confidence": existing_mapping.confidence_score,
                    }
            else:
                # Create new mapping
                new_mapping = FieldMapping(
                    data_import_id=data_import_id,  # Required field
                    master_flow_id=master_flow_id,  # Optional field
                    client_account_id=self.context.client_account_id,
                    source_field=normalized_source,
                    target_field=normalized_target,
                    confidence_score=confidence,
                    match_type="learned",  # Use match_type instead of mapping_type
                    suggested_by=source,  # Use suggested_by instead of source
                    transformation_rules=(
                        {"context": context} if context else {}
                    ),  # Use transformation_rules instead of metadata
                    status="approved",  # Set status as approved since it's being learned
                    approved_by=(
                        str(self.context.user_id) if self.context.user_id else None
                    ),
                    approved_at=datetime.utcnow() if source == "user" else None,
                )

                self.session.add(new_mapping)
                await self.flush_for_id()

                self.logger.info(
                    f"Learned new field mapping: {source_field} -> {target_field} "
                    f"(confidence: {confidence})"
                )

                # Update cache
                self._analyzer.update_cache(
                    normalized_source, normalized_target, confidence, source
                )

                return {
                    "success": True,
                    "action": "created",
                    "mapping_id": str(new_mapping.id),
                    "source_field": source_field,
                    "target_field": target_field,
                    "confidence": confidence,
                }

        except IntegrityError as e:
            # Handle unique constraint violation
            self.logger.warning(f"Mapping already exists: {e}")
            return {
                "success": False,
                "error": "Mapping already exists",
                "message": str(e),
            }
        except Exception as e:
            await self.record_failure(
                operation="learn_field_mapping",
                error=e,
                context_data={
                    "source_field": source_field,
                    "target_field": target_field,
                },
            )
            return {"success": False, "error": str(e)}

    async def learn_negative_mapping(
        self,
        source_field: str,
        target_field: str,
        data_import_id: UUID,
        master_flow_id: Optional[UUID] = None,
        reason: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Learn that a field mapping should NOT be made.

        Args:
            source_field: Source field that should not map
            target_field: Target field to avoid
            data_import_id: Required data import ID
            master_flow_id: Optional master flow ID
            reason: Optional reason for rejection

        Returns:
            Result of negative learning
        """
        try:
            normalized_source = self._normalize_field_name(source_field)
            normalized_target = self._normalize_field_name(target_field)

            # Add to negative cache
            self._negative_mappings_cache.add((normalized_source, normalized_target))
            self._analyzer.add_negative_mapping(source_field, target_field)

            # Store in database as negative mapping using status field
            negative_mapping = FieldMapping(
                data_import_id=data_import_id,  # Required field
                master_flow_id=master_flow_id,  # Optional field
                client_account_id=self.context.client_account_id,
                source_field=normalized_source,
                target_field=normalized_target,
                confidence_score=0.0,  # Use 0 confidence for rejected
                match_type="manual",  # User rejected this
                suggested_by="user",  # User made this decision
                transformation_rules=(
                    {"rejection_reason": reason} if reason else {}
                ),  # Store rejection reason
                status="rejected",  # Set status as rejected
                approved_by=str(self.context.user_id) if self.context.user_id else None,
                approved_at=datetime.utcnow(),  # Rejection timestamp
            )

            self.session.add(negative_mapping)
            await self.flush_for_id()

            self.logger.info(
                f"Learned negative mapping: {source_field} should NOT map to {target_field}"
            )

            return {
                "success": True,
                "message": f"Learned to avoid mapping {source_field} to {target_field}",
                "mapping_id": str(negative_mapping.id),
            }

        except Exception as e:
            await self.record_failure(
                operation="learn_negative_mapping",
                error=e,
                context_data={
                    "source_field": source_field,
                    "target_field": target_field,
                },
            )
            return {"success": False, "error": str(e)}

    async def get_field_mappings(
        self,
        data_import_id: Optional[UUID] = None,
        master_flow_id: Optional[UUID] = None,
        asset_type: Optional[str] = None,
    ) -> Dict[str, List[str]]:
        """
        Get all field mappings for the current context.

        Args:
            data_import_id: Optional data import ID filter
            master_flow_id: Optional master flow ID filter
            asset_type: Optional asset type filter

        Returns:
            Dictionary of canonical fields to their variations
        """
        try:
            # Load learned mappings if not cached
            if self._learned_mappings_cache is None:
                await self._load_learned_mappings(data_import_id, master_flow_id)

            # Update analyzer cache
            self._analyzer.learned_mappings_cache = self._learned_mappings_cache

            return self._analyzer.get_field_mappings(asset_type)

        except Exception as e:
            await self.record_failure(
                operation="get_field_mappings",
                error=e,
                context_data={"asset_type": asset_type},
            )
            return dict(self.BASE_MAPPINGS)  # Fallback to base mappings

    async def validate_mapping(
        self,
        source_field: str,
        target_field: str,
        data_import_id: Optional[UUID] = None,
        master_flow_id: Optional[UUID] = None,
        sample_values: Optional[List[Any]] = None,
    ) -> Dict[str, Any]:
        """
        Validate a field mapping with optional content analysis.

        Args:
            source_field: Source field name
            target_field: Target field name
            data_import_id: Optional data import ID for context
            master_flow_id: Optional master flow ID for context
            sample_values: Optional sample values for validation

        Returns:
            Validation result with confidence and issues
        """
        try:
            normalized_source = self._normalize_field_name(source_field)
            normalized_target = self._normalize_field_name(target_field)

            # Check if this is a known negative mapping
            if (normalized_source, normalized_target) in self._negative_mappings_cache:
                return {
                    "valid": False,
                    "confidence": 0.0,
                    "issues": ["This mapping was previously rejected"],
                }

            # Check base mappings
            if normalized_target in self.BASE_MAPPINGS:
                variations = [
                    self._normalize_field_name(v)
                    for v in self.BASE_MAPPINGS[normalized_target]
                ]
                if normalized_source in variations:
                    return {
                        "valid": True,
                        "confidence": 1.0,
                        "source": "base_mapping",
                        "issues": [],
                    }

            # Check learned mappings
            existing = await self._get_existing_mapping(
                normalized_source, normalized_target, data_import_id
            )
            if existing and existing.confidence_score > 0:
                return {
                    "valid": True,
                    "confidence": existing.confidence_score,
                    "source": "learned_mapping",
                    "issues": [],
                }

            # Perform content validation if sample values provided
            if sample_values:
                content_issues = FieldMappingValidator.validate_content(target_field, sample_values)
                if content_issues:
                    return {"valid": False, "confidence": 0.5, "issues": content_issues}

            # No existing mapping found
            return {
                "valid": False,
                "confidence": 0.0,
                "issues": ["No existing mapping found"],
                "suggestion": "Consider learning this mapping if valid",
            }

        except Exception as e:
            await self.record_failure(
                operation="validate_mapping",
                error=e,
                context_data={
                    "source_field": source_field,
                    "target_field": target_field,
                },
            )
            return {"valid": False, "confidence": 0.0, "error": str(e)}

    # Private helper methods

    async def _load_learned_mappings(
        self,
        data_import_id: Optional[UUID] = None,
        master_flow_id: Optional[UUID] = None,
    ) -> None:
        """Load learned mappings from database into cache."""
        try:
            # Build query with proper filters
            conditions = [
                FieldMapping.client_account_id == self.context.client_account_id,
                FieldMapping.status == "approved",  # Only approved mappings
                FieldMapping.confidence_score > 0,  # Exclude rejected
            ]

            # Add data_import_id or master_flow_id filter if provided
            if data_import_id:
                conditions.append(FieldMapping.data_import_id == data_import_id)
            elif master_flow_id:
                conditions.append(FieldMapping.master_flow_id == master_flow_id)

            query = select(FieldMapping).where(and_(*conditions))

            result = await self.session.execute(query)
            mappings = result.scalars().all()

            # Build cache
            self._learned_mappings_cache = {}
            for mapping in mappings:
                target = mapping.target_field
                if target not in self._learned_mappings_cache:
                    self._learned_mappings_cache[target] = []

                rule = MappingRule(
                    source_field=mapping.source_field,
                    target_field=mapping.target_field,
                    confidence=mapping.confidence_score,
                    source=mapping.suggested_by
                    or "learned",  # Use suggested_by instead of source
                    context=(
                        mapping.transformation_rules.get("context")
                        if mapping.transformation_rules
                        else None
                    ),  # Use transformation_rules instead of metadata
                    created_at=mapping.created_at,
                )
                self._learned_mappings_cache[target].append(rule)

            # Load rejected mappings
            negative_conditions = [
                FieldMapping.client_account_id == self.context.client_account_id,
                FieldMapping.status == "rejected",  # Use status field
            ]

            if data_import_id:
                negative_conditions.append(
                    FieldMapping.data_import_id == data_import_id
                )
            elif master_flow_id:
                negative_conditions.append(
                    FieldMapping.master_flow_id == master_flow_id
                )

            negative_query = select(FieldMapping).where(and_(*negative_conditions))

            negative_result = await self.session.execute(negative_query)
            negative_mappings = negative_result.scalars().all()

            for mapping in negative_mappings:
                self._negative_mappings_cache.add(
                    (mapping.source_field, mapping.target_field)
                )

            self.logger.debug(
                f"Loaded {len(mappings)} learned mappings and "
                f"{len(self._negative_mappings_cache)} negative mappings"
            )

        except Exception as e:
            self.logger.error(f"Failed to load learned mappings: {e}")
            self._learned_mappings_cache = {}

    async def _get_existing_mapping(
        self,
        source_field: str,
        target_field: str,
        data_import_id: Optional[UUID] = None,
    ) -> Optional[FieldMapping]:
        """Get existing mapping from database."""
        conditions = [
            FieldMapping.client_account_id == self.context.client_account_id,
            FieldMapping.source_field == source_field,
            FieldMapping.target_field == target_field,
            FieldMapping.status.in_(["approved", "suggested"]),
        ]

        # Add data_import_id filter if provided
        if data_import_id:
            conditions.append(FieldMapping.data_import_id == data_import_id)

        query = select(FieldMapping).where(and_(*conditions))

        result = await self.session.execute(query)
        return result.scalar_one_or_none()

    def _normalize_field_name(self, field_name: str) -> str:
        """Normalize field name for comparison."""
        return field_name.lower().strip().replace(" ", "_").replace("-", "_")
