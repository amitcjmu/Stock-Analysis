"""
Business logic for field mapping operations.

Generated by CC (Claude Code)
"""

import logging
from datetime import datetime
from typing import Any, Dict, List, Optional, Set
from uuid import UUID

from sqlalchemy.exc import IntegrityError

from app.core.context import RequestContext
from app.services.base_service import ServiceBase

from .base import MappingAnalysis, MappingRule
from .mappers import FieldMappingAnalyzer
from .repository import FieldMappingRepository

logger = logging.getLogger(__name__)


class FieldMappingBusinessLogic:
    """Handles core business logic for field mapping operations."""

    def __init__(
        self,
        repository: FieldMappingRepository,
        analyzer: FieldMappingAnalyzer,
        context: RequestContext,
        service_base: ServiceBase,
    ):
        """
        Initialize business logic.

        Args:
            repository: Database repository
            analyzer: Field mapping analyzer
            context: Request context
            service_base: Base service for operations like flush_for_id, record_failure
        """
        self.repository = repository
        self.analyzer = analyzer
        self.context = context
        self.service_base = service_base
        self.logger = logger

    def _normalize_field_name(self, field_name: str) -> str:
        """Normalize field name for comparison."""
        return field_name.lower().strip().replace(" ", "_").replace("-", "_")

    async def analyze_columns(
        self,
        columns: List[str],
        data_import_id: Optional[UUID] = None,
        master_flow_id: Optional[UUID] = None,
        asset_type: str = "server",
        sample_data: Optional[List[List[Any]]] = None,
    ) -> MappingAnalysis:
        """
        Analyze columns and provide mapping insights.

        Args:
            columns: List of column names to analyze
            data_import_id: Optional data import ID for context
            master_flow_id: Optional master flow ID for context
            asset_type: Type of asset being analyzed
            sample_data: Optional sample data for context

        Returns:
            Mapping analysis with suggestions
        """
        try:
            # Load learned mappings if not cached
            if self.analyzer.learned_mappings_cache is None:
                learned_cache, negative_cache = (
                    await self.repository.load_learned_mappings(
                        data_import_id, master_flow_id
                    )
                )
                self.analyzer.learned_mappings_cache = learned_cache
                self.analyzer.negative_mappings_cache = negative_cache

            # Analyze columns
            return self.analyzer.analyze_columns(
                columns=columns,
                asset_type=asset_type,
                sample_data=sample_data,
            )

        except Exception as e:
            await self.service_base.record_failure(
                operation="analyze_columns",
                error=e,
                context_data={
                    "columns_count": len(columns),
                    "asset_type": asset_type,
                },
            )
            # Return empty analysis on error
            return MappingAnalysis(
                total_columns=len(columns),
                mapped_columns=0,
                unmapped_columns=len(columns),
                confidence_score=0.0,
                suggested_mappings=[],
                unmapped_fields=columns,
                validation_issues=[f"Analysis failed: {str(e)}"],
            )

    async def learn_field_mapping(
        self,
        source_field: str,
        target_field: str,
        data_import_id: UUID,
        master_flow_id: Optional[UUID] = None,
        confidence: float = 0.9,
        source: str = "user",
        context: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Learn a new field mapping.

        Args:
            source_field: Source field name
            target_field: Target canonical field name
            data_import_id: Required data import ID
            master_flow_id: Optional master flow ID
            confidence: Confidence score (0-1)
            source: Source of the mapping (user, ai, system)
            context: Optional context information

        Returns:
            Result of learning operation
        """
        try:
            normalized_source = self._normalize_field_name(source_field)
            normalized_target = self._normalize_field_name(target_field)

            # Check if this is a negative mapping we've seen before
            if (
                normalized_source,
                normalized_target,
            ) in self.analyzer.negative_mappings_cache:
                return {
                    "success": False,
                    "message": f"Mapping {source_field} -> {target_field} was previously rejected",
                }

            # Check for existing mapping
            existing_mapping = await self.repository.get_existing_mapping(
                normalized_source, normalized_target, data_import_id
            )

            if existing_mapping:
                # Update confidence if this is higher
                if confidence > existing_mapping.confidence_score:
                    await self.repository.update_mapping(
                        existing_mapping, confidence, source, context
                    )
                    await self.service_base.flush_for_id()

                    self.logger.info(
                        f"Updated field mapping: {source_field} -> {target_field} "
                        f"(confidence: {confidence})"
                    )

                    # Update cache
                    self.analyzer.update_cache(
                        normalized_source, normalized_target, confidence, source
                    )

                    return {
                        "success": True,
                        "action": "updated",
                        "mapping_id": str(existing_mapping.id),
                        "confidence": confidence,
                    }
                else:
                    return {
                        "success": True,
                        "action": "exists",
                        "message": "Mapping already exists with equal or higher confidence",
                        "existing_confidence": existing_mapping.confidence_score,
                    }
            else:
                # Create new mapping
                new_mapping = await self.repository.create_mapping(
                    normalized_source,
                    normalized_target,
                    data_import_id,
                    master_flow_id,
                    confidence,
                    source,
                    context,
                )

                await self.service_base.flush_for_id()

                self.logger.info(
                    f"Learned new field mapping: {source_field} -> {target_field} "
                    f"(confidence: {confidence})"
                )

                # Update cache
                self.analyzer.update_cache(
                    normalized_source, normalized_target, confidence, source
                )

                return {
                    "success": True,
                    "action": "created",
                    "mapping_id": str(new_mapping.id),
                    "source_field": source_field,
                    "target_field": target_field,
                    "confidence": confidence,
                }

        except IntegrityError as e:
            # Handle unique constraint violation
            self.logger.warning(f"Mapping already exists: {e}")
            return {
                "success": False,
                "error": "Mapping already exists",
                "message": str(e),
            }
        except Exception as e:
            await self.service_base.record_failure(
                operation="learn_field_mapping",
                error=e,
                context_data={
                    "source_field": source_field,
                    "target_field": target_field,
                },
            )
            return {"success": False, "error": str(e)}

    async def learn_negative_mapping(
        self,
        source_field: str,
        target_field: str,
        data_import_id: UUID,
        master_flow_id: Optional[UUID] = None,
        reason: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        Learn that a field mapping should NOT be made.

        Args:
            source_field: Source field that should not map
            target_field: Target field to avoid
            data_import_id: Required data import ID
            master_flow_id: Optional master flow ID
            reason: Optional reason for rejection

        Returns:
            Result of negative learning
        """
        try:
            normalized_source = self._normalize_field_name(source_field)
            normalized_target = self._normalize_field_name(target_field)

            # Add to negative cache
            self.analyzer.negative_mappings_cache.add(
                (normalized_source, normalized_target)
            )
            self.analyzer.add_negative_mapping(source_field, target_field)

            # Store in database as negative mapping
            negative_mapping = await self.repository.create_negative_mapping(
                normalized_source,
                normalized_target,
                data_import_id,
                master_flow_id,
                reason,
            )

            await self.service_base.flush_for_id()

            self.logger.info(
                f"Learned negative mapping: {source_field} should NOT map to {target_field}"
            )

            return {
                "success": True,
                "message": f"Learned to avoid mapping {source_field} to {target_field}",
                "mapping_id": str(negative_mapping.id),
            }

        except Exception as e:
            await self.service_base.record_failure(
                operation="learn_negative_mapping",
                error=e,
                context_data={
                    "source_field": source_field,
                    "target_field": target_field,
                },
            )
            return {"success": False, "error": str(e)}
