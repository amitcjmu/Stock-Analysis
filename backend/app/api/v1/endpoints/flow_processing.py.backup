"""
Flow Processing API Endpoints
Handles flow continuation and routing decisions using intelligent agents
"""

import logging
import asyncio
from datetime import datetime
from typing import Any, Dict
from collections import defaultdict

from fastapi import APIRouter, Depends
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.context import RequestContext, get_request_context
from app.core.database import get_db
from app.services.flow_orchestration.transition_utils import (
    is_simple_transition,
    needs_ai_analysis,
    get_fast_path_response,
)
from app.services.agents.agent_service_layer.handlers.flow_handler import FlowHandler

# Import modularized components
from .flow_processing_models import (
    FlowContinuationRequest,
    FlowContinuationResponse,
)
from .flow_processing_converters import (
    convert_fast_path_to_api_response,
    create_simple_transition_response,
    convert_to_api_response,
    create_fallback_response,
)

# Legacy validation functions available in flow_processing_legacy.py if needed

# Import TenantScopedAgentPool for optimized AI operations (ADR-015)
try:
    from app.services.persistent_agents.tenant_scoped_agent_pool import (
        TenantScopedAgentPool,
    )

    TENANT_AGENT_POOL_AVAILABLE = True
except ImportError:
    TENANT_AGENT_POOL_AVAILABLE = False

# Import the REAL single intelligent CrewAI agent
try:
    from app.services.agents.intelligent_flow_agent import (
        FlowIntelligenceResult,
        IntelligentFlowAgent,
    )

    INTELLIGENT_AGENT_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("âœ… REAL Single Intelligent CrewAI Agent imported successfully")
except ImportError as e:
    INTELLIGENT_AGENT_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.error(f"âŒ Failed to import intelligent agent: {e}")

    # Fallback classes
    class FlowIntelligenceResult:
        def __init__(self, **kwargs):
            from app.core.security.cache_encryption import secure_setattr

            for key, value in kwargs.items():
                secure_setattr(self, key, value)

    class IntelligentFlowAgent:
        async def analyze_flow_continuation(self, flow_id: str, **kwargs):
            return FlowIntelligenceResult(
                success=False,
                flow_id=flow_id,
                flow_type="discovery",
                current_phase="data_import",
                routing_decision="/discovery/overview",
                user_guidance="Intelligent agent not available",
                reasoning="Agent import failed",
                confidence=0.0,
            )


logger = logging.getLogger(__name__)

router = APIRouter()


# Observability Metrics (FIX for Issue #9 with async locking per Qodo review)
class FlowProcessingMetrics:
    """Track performance metrics for flow processing with thread-safe async operations"""

    def __init__(self):
        self.fast_path_count = 0
        self.ai_path_count = 0
        self.simple_logic_count = 0
        self.error_count = 0
        self.execution_times: Dict[str, list] = defaultdict(list)
        self._lock = asyncio.Lock()  # Add async lock for thread safety

    async def record_fast_path(self, execution_time: float):
        async with self._lock:
            self.fast_path_count += 1
            self.execution_times["fast_path"].append(execution_time)

    async def record_ai_path(self, execution_time: float):
        async with self._lock:
            self.ai_path_count += 1
            self.execution_times["ai_path"].append(execution_time)

    async def record_simple_logic(self, execution_time: float):
        async with self._lock:
            self.simple_logic_count += 1
            self.execution_times["simple_logic"].append(execution_time)

    async def record_error(self, execution_time: float):
        async with self._lock:
            self.error_count += 1
            self.execution_times["errors"].append(execution_time)

    async def get_p95(self, path_type: str) -> float:
        """Get P95 latency for a given path type"""
        async with self._lock:
            times = sorted(self.execution_times.get(path_type, []))
        if not times:
            return 0.0
        idx = int(len(times) * 0.95)
        return times[min(idx, len(times) - 1)]

    async def get_stats(self) -> Dict[str, Any]:
        """Get current performance statistics"""
        async with self._lock:
            total = self.fast_path_count + self.ai_path_count + self.simple_logic_count
            fast_path_count = self.fast_path_count
            simple_logic_count = self.simple_logic_count
            ai_path_count = self.ai_path_count
            error_count = self.error_count
            fast_path_times = list(self.execution_times["fast_path"])
            simple_logic_times = list(self.execution_times["simple_logic"])
            ai_path_times = list(self.execution_times["ai_path"])

        # Calculate p95 values outside the lock
        fast_p95 = await self.get_p95("fast_path")
        simple_p95 = await self.get_p95("simple_logic")
        ai_p95 = await self.get_p95("ai_path")

        return {
            "total_requests": total,
            "fast_path": {
                "count": fast_path_count,
                "percentage": (fast_path_count / total * 100) if total > 0 else 0,
                "p95_latency": fast_p95,
                "avg_latency": (
                    sum(fast_path_times) / len(fast_path_times)
                    if fast_path_times
                    else 0
                ),
            },
            "simple_logic": {
                "count": simple_logic_count,
                "percentage": ((simple_logic_count / total * 100) if total > 0 else 0),
                "p95_latency": simple_p95,
                "avg_latency": (
                    sum(simple_logic_times) / len(simple_logic_times)
                    if simple_logic_times
                    else 0
                ),
            },
            "ai_path": {
                "count": ai_path_count,
                "percentage": (ai_path_count / total * 100) if total > 0 else 0,
                "p95_latency": ai_p95,
                "avg_latency": (
                    sum(ai_path_times) / len(ai_path_times) if ai_path_times else 0
                ),
            },
            "errors": error_count,
        }


# Global metrics instance
flow_metrics = FlowProcessingMetrics()


# Models are now imported from flow_processing_models.py


def _validate_flow_phase(flow_data: dict, flow_type: str) -> dict:
    """Extract phase validation logic to reduce complexity."""
    phases_completed = flow_data.get("phases_completed", {})
    current_phase = flow_data.get("current_phase", "data_import")

    # Determine if current phase is actually complete based on flow type
    phase_valid = False
    if flow_type == "discovery" and isinstance(phases_completed, dict):
        # For discovery flows, check the phases_completed dictionary
        phase_valid = phases_completed.get(current_phase, False)
    elif flow_type == "collection":
        # For collection flows, check if questionnaires phase is complete
        # TODO: Map to actual collection phase completion once repository is ready
        phase_valid = (
            current_phase == "questionnaires" and flow_data.get("status") == "completed"
        )

    return {
        "phase_valid": phase_valid,  # Derived from actual flow data per ADR-012
        "issues": [],
        "error": None,
        "completion_status": "phase_complete" if phase_valid else "in_progress",
    }


async def _execute_data_cleansing_if_needed(
    flow_id: str,
    flow_type: str,
    current_phase: str,
    request: FlowContinuationRequest,
    db: AsyncSession,
    context: RequestContext,
    flow_handler: Any,
) -> tuple[dict, dict]:
    """Execute data cleansing phase if required."""
    flow_data = {}
    validation_data = {}

    # Execute data cleansing when transitioning TO data_cleansing phase
    # (not just when already in it)
    should_execute_cleansing = flow_type == "discovery" and (
        current_phase == "data_cleansing"
        or (
            current_phase == "field_mapping"
            and flow_data.get("phases_completed", {}).get("field_mapping", False)
        )
    )

    if should_execute_cleansing:
        logger.info(
            f"ðŸ§¹ Enforcing data_cleansing execution (unconditional) for {flow_id}"
        )
        try:
            from app.services.master_flow_orchestrator import MasterFlowOrchestrator

            orchestrator = MasterFlowOrchestrator(db, context)
            exec_result = await orchestrator.execute_phase(
                flow_id=flow_id,
                phase_name="data_cleansing",
                phase_input=request.user_context or {},
            )

            logger.info(f"ðŸ§¹ Data cleansing exec_result: {exec_result}")
            if exec_result.get("status") not in ("success", "completed"):
                logger.warning(
                    f"âŒ Data cleansing execution failed for {flow_id}: {exec_result}"
                )
                raise Exception("Data cleansing failed; cannot advance")

            # Refresh flow status after cleansing
            flow_status_result = await flow_handler.get_flow_status(flow_id)
            flow_data = flow_status_result.get("flow", {})
            phases_completed = flow_data.get("phases_completed", {})
            phase_valid = phases_completed.get("data_cleansing", False)
            logger.info(
                f"âœ… Data cleansing executed; completion flag={phase_valid} for {flow_id}"
            )
            validation_data = _validate_flow_phase(flow_data, flow_type)
        except Exception as e:
            logger.error(f"âŒ Failed to execute data_cleansing before advance: {e}")
            raise Exception(f"Data cleansing execution error: {str(e)}")

    return flow_data, validation_data


async def _handle_fast_path_processing(
    flow_id: str,
    flow_data: dict,
    validation_data: dict,
    current_phase: str,
    db: AsyncSession,
    context: RequestContext,
    start_time: float,
) -> Any:
    """Handle fast path processing logic."""
    import time

    logger.info(f"âš¡ FAST PATH: Simple transition detected for {flow_id}")

    # Get fast path response without AI (< 1 second)
    fast_response = get_fast_path_response(flow_data, validation_data)

    if fast_response:
        # Update current_phase in database if transitioning to next phase
        await _update_phase_if_needed(
            flow_id, fast_response.get("next_phase"), current_phase, db, context
        )

        execution_time = time.time() - start_time
        logger.info(f"âœ… FAST PATH COMPLETE: {flow_id} in {execution_time:.3f}s")

        # Record metrics for observability
        await flow_metrics.record_fast_path(execution_time)

        # Convert to proper API response format
        return convert_fast_path_to_api_response(
            fast_response, flow_data, execution_time
        )
    return None


async def _handle_simple_logic_processing(
    flow_id: str,
    flow_data: dict,
    flow_type: str,
    current_phase: str,
    db: AsyncSession,
    context: RequestContext,
    start_time: float,
) -> Any:
    """Handle simple logic processing without AI."""
    import time

    logger.info(f"âš¡ SIMPLE LOGIC: No AI needed for {flow_id}")

    # Import and use _get_next_phase_simple to determine next phase
    from app.services.flow_orchestration.transition_utils import (
        _get_next_phase_simple,
    )

    # Determine next phase using simple logic
    next_phase = _get_next_phase_simple(flow_type, current_phase)

    # CRITICAL FIX: Execute asset_inventory phase when transitioning TO or FROM it
    # This ensures assets are actually created, not just phase advancement
    # Handle both cases: moving TO asset_inventory OR already IN asset_inventory
    if flow_type == "discovery" and (
        next_phase == "asset_inventory"
        or current_phase == "asset_inventory"
        or (
            current_phase == "data_cleansing"
            and next_phase in ["asset_creation", "dependency_analysis"]
        )
    ):
        # Check if we need to execute asset_inventory
        execute_asset_phase = False
        asset_phase_name = "asset_inventory"

        if current_phase == "asset_inventory":
            # We're IN asset_inventory, check if it's completed
            asset_inventory_completed = flow_data.get("phases_completed", {}).get(
                "asset_inventory", False
            )
            if not asset_inventory_completed:
                logger.info(
                    f"ðŸ­ Currently in asset_inventory (not completed), executing before advancing to {next_phase}"
                )
                execute_asset_phase = True
            else:
                logger.info("âœ… Asset inventory already completed, skipping execution")
        elif next_phase == "asset_inventory":
            # Moving TO asset_inventory
            logger.info(f"ðŸ­ Moving to asset_inventory phase for {flow_id}")
            execute_asset_phase = True
        elif current_phase == "data_cleansing":
            # Just completed data_cleansing, ensure asset_inventory runs
            logger.info(
                f"ðŸ­ Post-data_cleansing: ensuring asset_inventory executes for {flow_id}"
            )
            execute_asset_phase = True
            # Override next phase to ensure proper sequence
            if next_phase == "dependency_analysis":
                # This means asset_inventory was skipped, force it
                asset_phase_name = "asset_inventory"

        if execute_asset_phase:
            try:
                from app.services.master_flow_orchestrator import MasterFlowOrchestrator
                from app.repositories.discovery_flow_repository import (
                    DiscoveryFlowRepository,
                )

                # CRITICAL: Get discovery flow first to access both data_import_id and master_flow_id
                flow_repo = DiscoveryFlowRepository(
                    db, context.client_account_id, context.engagement_id
                )
                discovery_flow = await flow_repo.get_flow(flow_id)

                # Get data_import_id from flow_data or discovery flow
                data_import_id = flow_data.get("data_import_id")

                if not data_import_id and discovery_flow:
                    data_import_id = discovery_flow.data_import_id
                    if data_import_id:
                        logger.info(
                            f"ðŸ“¦ Found data_import_id from discovery flow: {data_import_id}"
                        )

                if not data_import_id:
                    logger.warning(
                        f"âš ï¸ No data_import_id found for flow {flow_id}, cannot execute asset_inventory"
                    )
                    # Skip asset inventory if no data import
                else:
                    # CRITICAL FIX: Get the actual master_flow_id from discovery flow
                    # The flow_id here is the discovery flow ID, but we need the master flow ID
                    master_flow_id = (
                        discovery_flow.master_flow_id
                        if discovery_flow and discovery_flow.master_flow_id
                        else flow_id
                    )

                    logger.info(
                        f"ðŸ­ Executing asset_inventory with data_import_id: {data_import_id}"
                    )
                    logger.info(
                        f"ðŸ“‹ Using master_flow_id: {master_flow_id} (discovery_flow_id: {flow_id})"
                    )

                    orchestrator = MasterFlowOrchestrator(db, context)
                    exec_result = await orchestrator.execute_phase(
                        flow_id=str(
                            master_flow_id
                        ),  # Use master_flow_id for orchestrator
                        phase_name=asset_phase_name,
                        phase_input={
                            "data_import_id": str(
                                data_import_id
                            ),  # CRITICAL: Include data_import_id
                            "flow_id": str(
                                master_flow_id
                            ),  # Use master_flow_id in phase_input
                            "master_flow_id": str(
                                master_flow_id
                            ),  # Consistent master_flow_id
                            "discovery_flow_id": flow_id,  # Include discovery_flow_id for asset association
                            "client_account_id": str(context.client_account_id),
                            "engagement_id": str(context.engagement_id),
                        },
                    )

                    logger.info(f"âœ… Asset inventory executed: {exec_result}")
                    if exec_result.get("status") not in ("success", "completed"):
                        logger.warning(
                            f"âš ï¸ Asset inventory execution had issues: {exec_result}"
                        )
                    else:
                        # Mark asset_inventory as completed in discovery flow
                        from app.repositories.discovery_flow_repository.commands.flow_phase_management import (
                            FlowPhaseManagementCommands,
                        )

                        phase_mgmt = FlowPhaseManagementCommands(
                            db, context.client_account_id, context.engagement_id
                        )
                        await phase_mgmt.update_phase_completion(
                            flow_id=flow_id,  # Use discovery flow ID
                            phase="asset_inventory",
                            completed=True,
                            data=exec_result,
                            agent_insights=exec_result.get("agent_insights"),
                        )
                        logger.info(
                            f"âœ… Marked asset_inventory_completed=true for flow {flow_id}"
                        )
            except Exception as e:
                logger.error(f"âŒ Failed to execute asset_inventory phase: {e}")
                # Continue anyway - don't block flow progression

    # Update current_phase in database if transitioning to next phase
    await _update_phase_if_needed(flow_id, next_phase, current_phase, db, context)

    # Use simple logic without AI but with proper response format
    simple_response = create_simple_transition_response(flow_data)
    execution_time = time.time() - start_time

    # Record metrics for observability
    await flow_metrics.record_simple_logic(execution_time)

    return convert_fast_path_to_api_response(simple_response, flow_data, execution_time)


async def _update_phase_if_needed(
    flow_id: str,
    next_phase: str,
    current_phase: str,
    db: AsyncSession,
    context: RequestContext,
) -> None:
    """Update phase in database if transitioning to next phase.

    CRITICAL: Updates BOTH discovery_flows and master flow tables to ensure consistency.
    """
    from app.repositories.discovery_flow_repository.commands.flow_phase_management import (
        FlowPhaseManagementCommands,
    )
    from app.repositories.discovery_flow_repository import DiscoveryFlowRepository
    from app.repositories.crewai_flow_state_extensions_repository import (
        CrewAIFlowStateExtensionsRepository,
    )

    if next_phase and next_phase != current_phase:
        # Update discovery_flows table
        phase_mgmt = FlowPhaseManagementCommands(
            db, context.client_account_id, context.engagement_id
        )
        await phase_mgmt.update_phase_completion(
            flow_id=flow_id,
            phase=next_phase,
            completed=False,  # Don't mark complete, just update current_phase
            data=None,
            agent_insights=None,
        )

        # Also update master flow's persistence data to track current phase
        flow_repo = DiscoveryFlowRepository(
            db, context.client_account_id, context.engagement_id
        )
        discovery_flow = await flow_repo.get_flow(flow_id)
        if discovery_flow and discovery_flow.master_flow_id:
            master_repo = CrewAIFlowStateExtensionsRepository(
                db, context.client_account_id, context.engagement_id
            )
            # Update master flow persistence data to track phase transition
            master_flow = await master_repo.get_by_flow_id(
                str(discovery_flow.master_flow_id)
            )
            if master_flow and master_flow.flow_persistence_data:
                persistence_data = master_flow.flow_persistence_data
                persistence_data["current_phase"] = next_phase
                persistence_data["last_phase_transition"] = {
                    "from": current_phase,
                    "to": next_phase,
                    "timestamp": datetime.utcnow().isoformat(),
                }
                await master_repo.update_persistence_data(
                    flow_id=str(discovery_flow.master_flow_id),
                    persistence_data=persistence_data,
                )
                logger.info(
                    f"âœ… Updated master flow {discovery_flow.master_flow_id} phase metadata"
                )

        logger.info(
            f"âœ… Advanced current_phase from {current_phase} to {next_phase} in all tables"
        )


async def _handle_ai_processing(
    flow_id: str, context: RequestContext, start_time: float
) -> Any:
    """Handle AI-based processing logic."""
    import time

    logger.info(f"ðŸ§  AI ANALYSIS NEEDED for {flow_id}")

    if TENANT_AGENT_POOL_AVAILABLE:
        # Use persistent tenant-scoped agent for better performance
        try:
            # Get the actual agent from the pool (FIX for Issue #3 per ADR-015)
            agent = await TenantScopedAgentPool.get_agent(
                context=context,
                agent_type="IntelligentFlowAgent",
                force_recreate=False,
            )

            logger.info(f"ðŸ§  TENANT AGENT: Using persistent agent for {flow_id}")

            # Use the pooled agent's analyze_flow_continuation method
            if hasattr(agent, "analyze_flow_continuation"):
                result = await agent.analyze_flow_continuation(
                    flow_id=flow_id,
                    client_account_id=context.client_account_id,
                    engagement_id=context.engagement_id,
                    user_id=context.user_id,
                )
            else:
                # Fallback if agent doesn't have expected method
                logger.warning(
                    "Pooled agent lacks analyze_flow_continuation, using new instance"
                )
                result = await use_intelligent_agent(flow_id, context)

        except Exception as e:
            logger.warning(f"Tenant agent failed, using intelligent agent: {e}")
            result = await use_intelligent_agent(flow_id, context)
    else:
        # Fallback to single intelligent agent
        logger.info(f"ðŸ§  INTELLIGENT AGENT: TenantPool unavailable for {flow_id}")
        result = await use_intelligent_agent(flow_id, context)

    execution_time = time.time() - start_time
    logger.info(f"âœ… AI ANALYSIS COMPLETE: {flow_id} in {execution_time:.3f}s")

    # Record metrics for AI path
    await flow_metrics.record_ai_path(execution_time)

    return convert_to_api_response(result, execution_time)


@router.post("/continue/{flow_id}", response_model=FlowContinuationResponse)
async def continue_flow_processing(
    flow_id: str,
    request: FlowContinuationRequest,
    context: RequestContext = Depends(get_request_context),
    db: AsyncSession = Depends(get_db),
):
    """
    Optimized flow processing with fast path detection

    FAST PATH (< 1 second): Simple phase transitions without AI
    INTELLIGENT PATH (when needed): AI analysis for complex scenarios

    Performance goals:
    - Simple transitions: < 1 second response time
    - AI-required scenarios: Only when needed (field mapping, errors, etc.)
    - Uses TenantScopedAgentPool (ADR-015) for persistent agents
    """
    import time

    start_time = time.time()

    try:
        logger.info(f"ðŸš€ OPTIMIZED FLOW PROCESSING: Starting for {flow_id}")

        # Step 1: Get flow status using FlowHandler
        flow_handler = FlowHandler(context)
        flow_status_result = await flow_handler.get_flow_status(flow_id)

        if not flow_status_result.get("flow_exists", False):
            logger.warning(f"Flow not found: {flow_id}")
            return create_fallback_response(flow_id, "Flow not found")

        flow_data = flow_status_result.get("flow", {})
        current_phase = flow_data.get("current_phase", "data_import")
        flow_type = flow_data.get("flow_type", "discovery")

        # Step 2: Derive validation from actual flow completion data
        validation_data = _validate_flow_phase(flow_data, flow_type)

        # Step 2.5: Resume flow if it's paused (e.g., after field mapping approval)
        flow_status = flow_data.get("status", "")
        if flow_status == "paused":
            logger.info(f"ðŸ”„ Resuming paused flow {flow_id} for phase transition")
            # Update BOTH discovery_flows AND crewai_flow_state_extensions
            from app.repositories.discovery_flow_repository import (
                DiscoveryFlowRepository,
            )
            from app.repositories.crewai_flow_state_extensions_repository import (
                CrewAIFlowStateExtensionsRepository,
            )

            # Update discovery_flows table
            flow_repo = DiscoveryFlowRepository(
                db, context.client_account_id, context.engagement_id
            )
            await flow_repo.update_flow_status(flow_id, "active")

            # Also update master flow status to ensure consistency
            # Get the master_flow_id from discovery flow
            discovery_flow = await flow_repo.get_flow(flow_id)
            if discovery_flow and discovery_flow.master_flow_id:
                master_repo = CrewAIFlowStateExtensionsRepository(
                    db, context.client_account_id, context.engagement_id
                )
                await master_repo.update_flow_status(
                    flow_id=str(discovery_flow.master_flow_id),
                    flow_status="running",
                    metadata={"resumed_at": datetime.utcnow().isoformat()},
                )
                logger.info(
                    f"âœ… Updated master flow {discovery_flow.master_flow_id} to running"
                )

            flow_data["status"] = "active"  # Update local copy
            logger.info(f"âœ… Flow {flow_id} resumed from paused state in both tables")

        # Step 3: Execute data cleansing if needed
        updated_flow_data, updated_validation_data = (
            await _execute_data_cleansing_if_needed(
                flow_id, flow_type, current_phase, request, db, context, flow_handler
            )
        )

        # Use updated data if cleansing was executed
        if updated_flow_data:
            flow_data = updated_flow_data
            validation_data = updated_validation_data

        # Step 4: DISABLE fast path for discovery flows - always ensure proper execution
        # Fast path was causing phases to be skipped without actual execution
        disable_fast_path = flow_type == "discovery"
        if not disable_fast_path and is_simple_transition(flow_data, validation_data):
            result = await _handle_fast_path_processing(
                flow_id,
                flow_data,
                validation_data,
                current_phase,
                db,
                context,
                start_time,
            )
            if result:
                return result

        # Step 5: Check if AI analysis is actually needed
        requires_ai, ai_reason = needs_ai_analysis(flow_data, validation_data)

        if not requires_ai:
            return await _handle_simple_logic_processing(
                flow_id, flow_data, flow_type, current_phase, db, context, start_time
            )

        # Step 6: AI analysis is needed
        return await _handle_ai_processing(flow_id, context, start_time)

    except Exception as e:
        execution_time = time.time() - start_time
        logger.error(
            f"âŒ FLOW PROCESSING ERROR: {flow_id} failed after "
            f"{execution_time:.3f}s - {str(e)}"
        )

        # Record error metrics
        await flow_metrics.record_error(execution_time)

        return create_fallback_response(flow_id, f"Processing failed: {str(e)}")


async def use_intelligent_agent(flow_id: str, context: RequestContext) -> Any:
    """Use the intelligent agent for flow analysis"""
    if not INTELLIGENT_AGENT_AVAILABLE:
        raise Exception("Intelligent agent not available")

    # Create single intelligent agent
    intelligent_agent = IntelligentFlowAgent()

    # Analyze flow using single agent with multiple tools
    result = await intelligent_agent.analyze_flow_continuation(
        flow_id=flow_id,
        client_account_id=context.client_account_id,
        engagement_id=context.engagement_id,
        user_id=context.user_id,
    )

    return result


@router.get("/metrics")
async def get_flow_processing_metrics():
    """
    Get flow processing performance metrics (FIX for Issue #9)

    Returns statistics on fast-path vs AI-path usage and latencies
    """
    stats = await flow_metrics.get_stats()

    # Log current performance stats
    if stats["total_requests"] > 0:
        logger.info(
            f"ðŸ“Š PERFORMANCE METRICS: "
            f"Total: {stats['total_requests']}, "
            f"Fast Path: {stats['fast_path']['percentage']:.1f}%, "
            f"Simple Logic: {stats['simple_logic']['percentage']:.1f}%, "
            f"AI Path: {stats['ai_path']['percentage']:.1f}%, "
            f"Fast P95: {stats['fast_path']['p95_latency']:.3f}s, "
            f"AI P95: {stats['ai_path']['p95_latency']:.3f}s"
        )

    return stats


# Response conversion functions moved to flow_processing_converters.py

# Legacy validation functions moved to flow_processing_legacy.py
