"""
Background Workers for Gap Analysis Jobs

Handles asynchronous processing of comprehensive AI gap analysis with per-asset persistence.

CRITICAL: This uses comprehensive analysis (_run_tier_2_ai_analysis), NOT enhancement.
The AI analyzes the ENTIRE asset to find ALL gaps, not just enhance predetermined ones.
"""

import logging
from uuid import UUID

from .job_state_manager import update_job_state

logger = logging.getLogger(__name__)


async def cleanup_heuristic_gaps(
    collection_flow_id: UUID,
    analyzed_assets: list,
    ai_validated_gaps: dict,
    db,
):
    """Delete heuristic gaps that weren't validated by AI comprehensive analysis.

    The AI analysis is the AUTHORITATIVE source of gaps. Heuristic gaps are just
    a quick preview and should be removed if AI determined they aren't real gaps.

    Args:
        collection_flow_id: Collection flow UUID
        analyzed_assets: List of Asset objects that were analyzed
        ai_validated_gaps: Dict from AI analysis with validated gaps
        db: AsyncSession database session

    Returns:
        Number of heuristic gaps deleted
    """
    from sqlalchemy import select
    from app.models.collection_data_gap import CollectionDataGap

    logger.info(
        f"üîç Cleaning up heuristic gaps for {len(analyzed_assets)} analyzed assets "
        f"(AI gaps are authoritative and replace all heuristic gaps)"
    )

    # Delete ALL heuristic gaps for analyzed assets
    # AI gaps are authoritative and completely replace heuristic gaps
    deleted_count = 0
    for asset in analyzed_assets:
        # Find all heuristic gaps for this asset in this flow
        stmt = select(CollectionDataGap).where(
            CollectionDataGap.collection_flow_id == collection_flow_id,
            CollectionDataGap.asset_id == asset.id,
            CollectionDataGap.confidence_score.is_(None),  # Heuristic gaps only
        )
        result = await db.execute(stmt)
        heuristic_gaps = result.scalars().all()

        # Delete ALL heuristic gaps - AI analysis is authoritative
        for gap in heuristic_gaps:
            await db.delete(gap)
            deleted_count += 1
            logger.debug(
                f"üóëÔ∏è Deleting heuristic gap (AI analysis is authoritative): "
                f"Asset={asset.name}, Field={gap.field_name}"
            )

    await db.commit()

    if deleted_count > 0:
        logger.info(
            f"‚úÖ Cleaned up {deleted_count} heuristic gaps not validated by AI analysis"
        )

    return deleted_count


async def verify_ai_gaps_consistency(assets, db):
    """Verify that assets marked as AI-analyzed have gaps with confidence_score.

    CRITICAL FIX (Issue #1045 - Qodo Bot): Auto-fix inconsistencies by resetting status
    instead of just logging warnings.

    This ensures data consistency across all collection flows, not just the current one.

    Args:
        assets: List of Asset objects to verify
        db: AsyncSession database session
    """
    from sqlalchemy import select, func
    from app.models.collection_data_gap import CollectionDataGap

    fixed_count = 0
    for asset in assets:
        # Check if gaps with AI enhancement exist for this asset (across ALL flows)
        stmt = select(func.count()).where(
            CollectionDataGap.asset_id == asset.id,
            CollectionDataGap.confidence_score.is_not(None),
        )
        result = await db.execute(stmt)
        ai_gaps_count = result.scalar()

        if ai_gaps_count == 0:
            # AUTO-FIX: Reset status to 0 so asset can be re-analyzed
            logger.warning(
                f"‚ö†Ô∏è Consistency issue detected and FIXED: Asset {asset.id} marked as AI-analyzed "
                f"(status=2) but has no gaps with confidence_score - resetting to status=0"
            )
            asset.ai_gap_analysis_status = 0
            asset.ai_gap_analysis_timestamp = None
            fixed_count += 1
        else:
            logger.debug(
                f"‚úÖ Asset {asset.id} consistency verified - "
                f"{ai_gaps_count} AI-enhanced gaps found"
            )

    if fixed_count > 0:
        await db.commit()
        logger.info(
            f"üîß Auto-fixed {fixed_count} consistency issues by resetting status to 0"
        )


async def process_gap_enhancement_job(  # noqa: C901
    job_id: str,
    collection_flow_id: UUID,
    selected_asset_ids: list,
    client_account_id: str,
    engagement_id: str,
    force_refresh: bool = False,
):
    """Background worker for comprehensive AI gap analysis.

    Performs full asset analysis without requiring predetermined gaps.
    This enables comprehensive discovery of all data gaps.

    Skips assets with ai_gap_analysis_status = 2 unless force_refresh = True.
    Implements stale detection by comparing ai_gap_analysis_timestamp with asset.updated_at.

    Args:
        job_id: Unique job identifier
        collection_flow_id: Collection flow internal ID (used for all DB operations)
        selected_asset_ids: Asset IDs to analyze comprehensively
        client_account_id: Client account UUID (primitive, not mutable context)
        engagement_id: Engagement UUID (primitive, not mutable context)
        force_refresh: Force re-analysis even if status=2 (default: False)
    """
    from app.services.collection.gap_analysis.data_loader import load_assets
    from app.services.collection.gap_analysis.service import GapAnalysisService
    from datetime import datetime, timezone

    try:
        await update_job_state(collection_flow_id, {"status": "running"})

        # Get database session for background task
        from app.core.database import AsyncSessionLocal

        async with AsyncSessionLocal() as db:
            # Load assets (using primitive IDs)
            all_assets = await load_assets(
                selected_asset_ids,
                client_account_id,
                engagement_id,
                db,
            )

            if not all_assets:
                logger.warning("‚ö†Ô∏è No assets found for AI analysis")
                await update_job_state(
                    collection_flow_id,
                    {
                        "status": "completed",
                        "message": "No assets found",
                    },
                )
                return

            # Filter assets based on AI analysis status and staleness
            if force_refresh:
                # User forced re-analysis - analyze all assets
                assets_to_analyze = all_assets
                logger.info(
                    f"üîÑ Force refresh enabled - analyzing all {len(all_assets)} assets"
                )
            else:
                # Skip assets that already completed AI analysis (status = 2) AND aren't stale
                assets_to_analyze = []
                assets_skipped = 0
                assets_stale = 0

                for asset in all_assets:
                    # CRITICAL FIX (Issue #1045 - Qodo Bot): Skip assets being processed by other workers
                    if asset.ai_gap_analysis_status == 1:
                        assets_skipped += 1
                        logger.debug(
                            f"‚è≠Ô∏è Asset {asset.id} skipped - already being processed (status=1)"
                        )
                        continue

                    # Check if AI analysis completed
                    if asset.ai_gap_analysis_status != 2:
                        # Status = 0 (not started), add to analyze list
                        assets_to_analyze.append(asset)
                        continue

                    # Status = 2 (completed), check if analysis is stale
                    if (
                        asset.ai_gap_analysis_timestamp
                        and asset.updated_at
                        and asset.updated_at > asset.ai_gap_analysis_timestamp
                    ):
                        assets_to_analyze.append(asset)
                        assets_stale += 1
                        logger.debug(
                            f"üìä Asset {asset.id} marked for re-analysis - "
                            f"updated_at={asset.updated_at} > "
                            f"ai_analysis_timestamp={asset.ai_gap_analysis_timestamp}"
                        )
                    else:
                        assets_skipped += 1

                logger.info(
                    f"üìä AI analysis plan - "
                    f"Analyze: {len(assets_to_analyze)} "
                    f"(new: {len(assets_to_analyze) - assets_stale}, stale: {assets_stale}), "
                    f"Skipped (cached): {assets_skipped}"
                )

            if not assets_to_analyze:
                logger.info("‚úÖ All assets already have AI analysis - nothing to do")
                await update_job_state(
                    collection_flow_id,
                    {
                        "status": "completed",
                        "progress": 100,
                        "message": "All assets already analyzed",
                    },
                )
                return

            # Mark assets as in-progress (status = 1)
            for asset in assets_to_analyze:
                asset.ai_gap_analysis_status = 1
            await db.flush()

            # Initialize gap analysis service (using primitive IDs)
            gap_service = GapAnalysisService(
                client_account_id=client_account_id,
                engagement_id=engagement_id,
                collection_flow_id=str(collection_flow_id),
            )

            # Run tier_2 comprehensive AI analysis
            logger.info(
                f"ü§ñ Job {job_id}: Running COMPREHENSIVE AI analysis "
                f"on {len(assets_to_analyze)} assets"
            )

            # Correct - comprehensive analysis on assets only
            ai_result = await gap_service._run_tier_2_ai_analysis(
                assets=assets_to_analyze,
                collection_flow_id=str(collection_flow_id),
                db=db,
            )

            # CRITICAL: Delete heuristic gaps not validated by AI
            # AI analysis is authoritative - heuristic gaps are just preview
            deleted_count = await cleanup_heuristic_gaps(
                collection_flow_id=collection_flow_id,
                analyzed_assets=assets_to_analyze,
                ai_validated_gaps=ai_result,
                db=db,
            )
            logger.info(
                f"üßπ Cleaned up {deleted_count} heuristic gaps not validated by AI"
            )

            # Mark assets as completed (status = 2) with timestamp
            completion_time = datetime.now(timezone.utc)

            for asset in assets_to_analyze:
                asset.ai_gap_analysis_status = 2
                asset.ai_gap_analysis_timestamp = completion_time

            await db.commit()

            # Verify consistency: gaps with confidence_score should exist
            await verify_ai_gaps_consistency(assets_to_analyze, db)

            logger.info(
                f"‚úÖ AI analysis completed for {len(assets_to_analyze)} assets "
                f"at {completion_time.isoformat()}"
            )

            # Update final job state
            summary = ai_result.get("summary", {})
            await update_job_state(
                collection_flow_id,
                {
                    "status": "completed",
                    "enhanced_count": summary.get("total_gaps", 0),
                    "gaps_persisted": summary.get("gaps_persisted", 0),
                    "failed_count": summary.get("assets_failed", 0),
                },
            )

            # Auto-progress to next phase if gaps were persisted
            await _auto_progress_phase(
                job_id=job_id,
                collection_flow_id=collection_flow_id,
                gaps_persisted=summary.get("gaps_persisted", 0),
                client_account_id=client_account_id,
                engagement_id=engagement_id,
                db=db,
            )

            logger.info(f"‚úÖ Job {job_id}: Enhancement complete")

    except Exception as e:
        # Bug #892 Fix: Capture detailed error information for user feedback
        error_type = type(e).__name__
        error_message = str(e)

        logger.error(
            f"‚ùå Job {job_id} failed - {error_type}: {error_message}", exc_info=True
        )

        # CRITICAL FIX (Issue #1045 - Qodo Bot): Reset status ONLY for assets that were being analyzed
        # (status=1), not all assets in the job
        from app.core.database import AsyncSessionLocal

        try:
            async with AsyncSessionLocal() as db:
                from app.services.collection.gap_analysis.data_loader import load_assets

                # Reload assets to check which ones were in-progress
                all_job_assets = await load_assets(
                    selected_asset_ids,
                    client_account_id,
                    engagement_id,
                    db,
                )

                # Only reset assets that were actively being analyzed (status=1)
                reset_count = 0
                for asset in all_job_assets:
                    if asset.ai_gap_analysis_status == 1:
                        asset.ai_gap_analysis_status = 0
                        asset.ai_gap_analysis_timestamp = None
                        reset_count += 1

                await db.commit()

                logger.info(
                    f"üîÑ Reset AI analysis status for {reset_count}/{len(all_job_assets)} "
                    f"assets that were in-progress (status=1) to allow retry"
                )
        except Exception as reset_error:
            logger.error(
                f"‚ùå Failed to reset asset status: {reset_error}", exc_info=True
            )

        # Categorize error for better user messaging
        if "timeout" in error_message.lower() or "TimeoutError" in error_type:
            user_message = (
                "LLM API request timed out. Please try again with fewer assets or gaps."
            )
            error_category = "timeout"
        elif "connection" in error_message.lower() or "ConnectionError" in error_type:
            user_message = "Connection to AI service failed. Please check your network and try again."
            error_category = "connection"
        elif "no assets" in error_message.lower():
            user_message = "No assets found for analysis. Please ensure assets are properly selected."
            error_category = "no_assets"
        elif "authentication" in error_message.lower() or "401" in error_message:
            user_message = "AI service authentication failed. Please contact support."
            error_category = "auth"
        else:
            user_message = f"Enhancement failed: {error_message}"
            error_category = "unknown"

        await update_job_state(
            collection_flow_id,
            {
                "status": "failed",
                "error": error_message,
                "error_type": error_type,
                "error_category": error_category,
                "user_message": user_message,
            },
        )


async def _auto_progress_phase(
    job_id: str,
    collection_flow_id: UUID,
    gaps_persisted: int,
    client_account_id: str,
    engagement_id: str,
    db,
):
    """Auto-progress to questionnaire_generation phase if gaps were persisted.

    Args:
        job_id: Job identifier for logging
        collection_flow_id: Collection flow internal ID
        gaps_persisted: Number of gaps successfully persisted
        client_account_id: Client account UUID (primitive)
        engagement_id: Engagement UUID (primitive)
        db: Database session
    """
    if gaps_persisted <= 0:
        return

    try:
        # Get collection flow for progression
        from sqlalchemy import and_, select

        from app.models.collection_flow import CollectionFlow

        stmt = select(CollectionFlow).where(
            and_(
                CollectionFlow.id == collection_flow_id,
                CollectionFlow.client_account_id == UUID(client_account_id),
                CollectionFlow.engagement_id == UUID(engagement_id),
            )
        )
        result = await db.execute(stmt)
        collection_flow = result.scalar_one_or_none()

        if collection_flow and collection_flow.master_flow_id:
            logger.info(
                f"üöÄ Job {job_id}: Auto-progressing to questionnaire_generation"
            )

            # IMPORTANT: Create immutable context from primitives for progression service
            # Background jobs receive only primitive IDs (str), not mutable request context.
            # This RequestContext is constructed here as an immutable container of primitives
            # required by CollectionPhaseProgressionService and MFO utilities.
            # It is NOT the request context from the HTTP handler.
            from app.core.context import RequestContext
            from app.services.collection_phase_progression_service import (
                CollectionPhaseProgressionService,
            )

            temp_context = RequestContext(
                client_account_id=UUID(
                    client_account_id
                ),  # Converted from str primitive
                engagement_id=UUID(engagement_id),  # Converted from str primitive
                user_id=None,  # Background job has no user
            )

            progression_service = CollectionPhaseProgressionService(
                db_session=db, context=temp_context
            )
            progression_result = await progression_service.advance_to_next_phase(
                flow=collection_flow,
                target_phase="questionnaire_generation",
            )

            if progression_result["status"] == "success":
                logger.info(f"‚úÖ Job {job_id}: Advanced to questionnaire_generation")
                await update_job_state(
                    collection_flow_id,
                    {"phase_progression": "advanced_to_questionnaire_generation"},
                )
            else:
                error_msg = progression_result.get("error")
                logger.warning(f"‚ö†Ô∏è Job {job_id}: Phase progression failed: {error_msg}")
                await update_job_state(
                    collection_flow_id,
                    {"phase_progression": f"failed: {error_msg}"},
                )
    except Exception as progression_error:
        logger.error(
            f"‚ùå Job {job_id}: Phase progression error: {progression_error}",
            exc_info=True,
        )
        await update_job_state(
            collection_flow_id,
            {"phase_progression": f"error: {progression_error}"},
        )
