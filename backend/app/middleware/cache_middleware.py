"""
Cache Middleware - Main Orchestration

Provides transparent Redis caching for FastAPI with ETag support, multi-tenant
isolation, circuit breaker resilience, and request deduplication.

Modularized (originally 745 lines):
- cache_utils.py: Cache keys, ETag, filtering
- cache_responses.py: Response handling (CRITICAL: Bug fix #674)
- cache_middleware.py: Main orchestration

Generated by CC (Claude Code)
"""

import asyncio
import json
import time
from typing import Any, Dict, Optional

from fastapi import Request, Response, status
from fastapi.responses import JSONResponse, StreamingResponse
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.types import ASGIApp

from app.core.config import settings
from app.core.logging import get_logger
from app.middleware.cache_responses import (
    cache_response_data,
    create_cached_response,
)
from app.middleware.cache_utils import (
    generate_cache_key,
    generate_etag,
    get_cache_config,
    should_cache_request,
)
from app.services.caching.redis_cache import RedisCache, get_redis_cache
from app.utils.circuit_breaker import CircuitBreaker

logger = get_logger(__name__)


class CacheMiddleware(BaseHTTPMiddleware):
    """
    FastAPI middleware for transparent Redis caching with ETag support,
    multi-tenant isolation, circuit breaker, and request deduplication.
    Utilities in cache_utils.py, response handling in cache_responses.py.
    """

    def __init__(self, app: ASGIApp):
        super().__init__(app)
        self.redis: Optional[RedisCache] = None
        self.circuit_breaker = CircuitBreaker(
            name="cache_middleware",
            failure_threshold=5,
            recovery_timeout=60,
            expected_exception=(ConnectionError, TimeoutError, OSError),
        )
        self.pending_requests: Dict[str, Any] = {}  # Request deduplication
        self.stats = {
            "cache_hits": 0,
            "cache_misses": 0,
            "cache_errors": 0,
            "etag_matches": 0,
            "total_requests": 0,
        }

        # Initialize Redis connection
        self._initialize_redis()

    def _initialize_redis(self):
        """Initialize Redis connection with error handling."""
        try:
            self.redis = get_redis_cache()
            if self.redis and self.redis.enabled:
                logger.info("Cache middleware initialized with Redis support")
            else:
                logger.warning("Cache middleware running without Redis (fallback mode)")
        except Exception as e:
            logger.error(f"Failed to initialize Redis for cache middleware: {e}")
            self.redis = None

    async def dispatch(self, request: Request, call_next):
        """Main middleware dispatch logic."""
        start_time = time.time()
        self.stats["total_requests"] += 1

        # Skip caching for non-GET requests or excluded endpoints
        if not should_cache_request(request):
            return await call_next(request)

        # Get cache configuration for this endpoint
        cache_config = get_cache_config(request.url.path)
        if not cache_config:
            return await call_next(request)

        # Generate cache key with tenant isolation
        cache_key = generate_cache_key(request)
        if not cache_key:
            logger.debug(f"Could not generate cache key for {request.url.path}")
            return await call_next(request)

        # Check for ETag match (conditional request)
        etag_match_response = await self._check_etag_match(request, cache_key)
        if etag_match_response:
            self.stats["etag_matches"] += 1
            return etag_match_response

        # Try to get cached response
        cached_response = await self._get_cached_response(cache_key, cache_config)
        if cached_response:
            self.stats["cache_hits"] += 1
            return create_cached_response(cached_response, request)

        # Check if there's already a pending request for this cache key
        if cache_key in self.pending_requests:
            logger.debug(f"Request deduplication: waiting for {cache_key}")
            try:
                # Wait for the existing request to complete
                future = self.pending_requests[cache_key]
                response = await future

                # The response is already complete, just return it
                if response:
                    logger.debug(
                        f"Request deduplication: returning shared response for {cache_key}"
                    )
                    return response
            except Exception as e:
                logger.warning(f"Request deduplication failed: {e}")
                # Continue with normal request if deduplication fails

        # Cache miss - create a future for this request
        self.stats["cache_misses"] += 1
        future = asyncio.create_task(
            self._execute_and_cache_request(request, call_next, cache_key, cache_config)
        )

        # Store the future for deduplication
        self.pending_requests[cache_key] = future

        try:
            response = await future
        finally:
            # Clean up the pending request
            self.pending_requests.pop(cache_key, None)

        # Add performance headers
        elapsed_ms = (time.time() - start_time) * 1000
        response.headers["X-Cache-Time"] = f"{elapsed_ms:.2f}ms"
        response.headers["X-Cache"] = "MISS"

        return response

    async def _check_etag_match(
        self, request: Request, cache_key: str
    ) -> Optional[Response]:
        """Check ETag match, return 304 if appropriate."""
        if not self.redis or not self.redis.enabled:
            return None
        try:
            if_none_match = request.headers.get("If-None-Match")
            if not if_none_match:
                return None
            etag_key = f"{cache_key}:etag"
            cached_etag = await self.circuit_breaker.async_call(
                self.redis.get, etag_key
            )
            if cached_etag and cached_etag == if_none_match.strip('"'):
                response = Response(status_code=status.HTTP_304_NOT_MODIFIED)
                response.headers["ETag"] = f'"{cached_etag}"'
                response.headers["Cache-Control"] = "private, max-age=300"
                response.headers["X-Cache"] = "ETAG_MATCH"
                return response
        except Exception as e:
            logger.debug(f"ETag check failed: {e}")
        return None

    async def _get_cached_response(
        self, cache_key: str, cache_config: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """Get cached response from Redis."""
        if not self.redis or not self.redis.enabled:
            return None
        try:
            cached_data = await self.circuit_breaker.async_call(
                self.redis.get, cache_key
            )
            if cached_data:
                if isinstance(cached_data, dict) and "data" in cached_data:
                    return cached_data
                # Handle legacy cache format
                from datetime import datetime

                return {
                    "data": cached_data,
                    "cached_at": datetime.utcnow().isoformat(),
                    "ttl": cache_config.get("ttl", 300),
                }
        except Exception as e:
            self.stats["cache_errors"] += 1
            logger.debug(f"Cache get failed for key {cache_key}: {e}")
        return None

    async def _buffer_streaming_response(self, body_iterator) -> Optional[bytes]:
        """
        Buffer streaming response chunks into bytes.

        Args:
            body_iterator: Async iterator yielding response chunks

        Returns:
            Combined bytes from all chunks, or None if no chunks
        """
        chunks = []
        async for chunk in body_iterator:
            if isinstance(chunk, bytes):
                chunks.append(chunk)
            elif isinstance(chunk, str):
                chunks.append(chunk.encode("utf-8"))

        if chunks:
            return b"".join(chunks)
        return None

    async def _extract_json_response_body(
        self, original_response: Response
    ) -> Optional[Dict[str, Any]]:
        """
        Extract JSON body from different response types.

        Args:
            original_response: The response to extract body from

        Returns:
            Parsed JSON body or None if extraction fails
        """
        if isinstance(original_response, JSONResponse):
            # For JSONResponse, we can get content directly
            if hasattr(original_response, "body") and original_response.body:
                return json.loads(original_response.body.decode("utf-8"))
            elif hasattr(original_response, "content"):
                return original_response.content

        elif isinstance(original_response, StreamingResponse):
            # For StreamingResponse, we need to consume the stream and buffer it
            body_bytes = await self._buffer_streaming_response(
                original_response.body_iterator
            )
            if body_bytes:
                return json.loads(body_bytes.decode("utf-8"))

        else:
            # Handle regular Response - use utility function from cache_responses
            from app.middleware.cache_responses import extract_response_body

            return await extract_response_body(original_response)

        return None

    def _create_cached_json_response(
        self,
        response_body: Dict[str, Any],
        original_response: Response,
        cache_config: Dict[str, Any],
    ) -> JSONResponse:
        """
        Create new JSONResponse with cached data and proper headers.

        CRITICAL BUG FIX #674: Includes body pre-rendering logic to ensure
        Content-Length header is set correctly.

        Args:
            response_body: The JSON body to include in response
            original_response: Original response to copy headers from
            cache_config: Cache configuration with TTL

        Returns:
            JSONResponse with proper headers and pre-rendered body
        """
        # Create new response with cached data and proper headers
        # CRITICAL FIX (#674): Ensure Content-Length is calculated correctly
        # by creating response and rendering body immediately
        new_response = JSONResponse(content=response_body)

        # Pre-render the response body to ensure Content-Length is set correctly
        # This prevents "Response content shorter than Content-Length" errors
        # in BaseHTTPMiddleware when response is consumed by multiple middleware layers
        if not hasattr(new_response, "body") or not new_response.body:
            # Force body rendering by accessing it
            _ = new_response.body

        # Copy important headers from original response (but NOT Content-Length)
        for header_name in ["content-type", "content-encoding"]:
            if header_name in original_response.headers:
                new_response.headers[header_name] = original_response.headers[
                    header_name
                ]

        # Add cache-specific headers
        etag = generate_etag(response_body)
        ttl = cache_config.get("ttl", 300)
        new_response.headers["ETag"] = f'"{etag}"'
        new_response.headers["Cache-Control"] = f"private, max-age={ttl}"
        new_response.headers["Vary"] = "X-Client-Account-ID, X-Engagement-ID"

        return new_response

    async def _execute_and_cache_request(
        self, request: Request, call_next, cache_key: str, cache_config: Dict[str, Any]
    ) -> Response:
        """
        Execute the request, buffer the response, and cache it.

        CRITICAL BUG FIX #674: This method includes body pre-rendering logic
        to ensure Content-Length header is set correctly.
        """
        # Execute the original request
        original_response = await call_next(request)

        # Only process successful JSON responses for caching
        if original_response.status_code != 200 or not original_response.headers.get(
            "content-type", ""
        ).startswith("application/json"):
            return original_response

        response_body = None
        try:
            # Extract JSON body from response (handles all response types)
            response_body = await self._extract_json_response_body(original_response)

            # If we successfully extracted the body, cache it and create new response
            if response_body is not None:
                # Cache the response data using utility function
                await cache_response_data(
                    self.redis,
                    self.circuit_breaker,
                    cache_key,
                    response_body,
                    cache_config,
                    request,
                )

                # Create new response with cached data and proper headers
                # (includes bug fix #674 for Content-Length)
                return self._create_cached_json_response(
                    response_body, original_response, cache_config
                )

        except Exception as e:
            logger.debug(f"Failed to buffer and cache response: {e}")
            # If anything goes wrong, fall through to return a response

        # If caching failed or body was not extracted, but we have a buffered body,
        # create a response from it to avoid returning a consumed stream.
        if response_body is not None:
            logger.debug("Returning response from buffered body after cache failure.")
            return self._create_cached_json_response(
                response_body, original_response, cache_config
            )

        return original_response

    def get_stats(self) -> Dict[str, Any]:
        """Get middleware statistics."""
        total = max(self.stats["cache_hits"] + self.stats["cache_misses"], 1)
        hit_ratio = self.stats["cache_hits"] / total

        return {
            **self.stats,
            "hit_ratio": hit_ratio,
            "redis_enabled": self.redis and self.redis.enabled,
            "circuit_breaker_state": (
                self.circuit_breaker.get_state() if self.circuit_breaker else None
            ),
            "pending_requests": len(self.pending_requests),
        }


class CacheInstrumentationMiddleware(BaseHTTPMiddleware):
    """
    Additional middleware for cache performance monitoring and OpenTelemetry integration.
    Should be added after CacheMiddleware in the middleware stack.
    """

    def __init__(self, app: ASGIApp):
        super().__init__(app)
        self.enabled = (
            settings.CACHE_ANALYTICS_ENABLED
            if hasattr(settings, "CACHE_ANALYTICS_ENABLED")
            else True
        )

    async def dispatch(self, request: Request, call_next):
        """Add performance monitoring and tracing."""
        if not self.enabled:
            return await call_next(request)

        # Start OpenTelemetry span
        from opentelemetry import trace

        tracer = trace.get_tracer(__name__)

        with tracer.start_as_current_span("cache_middleware") as span:
            # Add request attributes
            span.set_attribute("http.method", request.method)
            span.set_attribute("http.url", str(request.url))
            span.set_attribute("cache.enabled", True)

            # Execute request
            start_time = time.time()
            response = await call_next(request)
            elapsed_ms = (time.time() - start_time) * 1000

            # Add response attributes
            span.set_attribute("http.status_code", response.status_code)
            span.set_attribute(
                "cache.hit",
                response.headers.get("X-Cache", "MISS") in ["HIT", "ETAG_MATCH"],
            )
            span.set_attribute("cache.response_time_ms", elapsed_ms)

            # Log performance metrics
            if elapsed_ms > 100:  # Log slow requests
                logger.warning(
                    f"Slow cache operation: {request.method} {request.url.path} "
                    f"took {elapsed_ms:.2f}ms"
                )

            return response


# Factory functions for dependency injection
def create_cache_middleware() -> CacheMiddleware:
    """Create cache middleware instance."""
    return CacheMiddleware


def create_cache_instrumentation_middleware() -> CacheInstrumentationMiddleware:
    """Create cache instrumentation middleware instance."""
    return CacheInstrumentationMiddleware


__all__ = [
    "CacheMiddleware",
    "CacheInstrumentationMiddleware",
    "create_cache_middleware",
    "create_cache_instrumentation_middleware",
]
