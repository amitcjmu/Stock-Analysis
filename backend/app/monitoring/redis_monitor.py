"""
Redis Monitoring and Observability
Generated by CC DevSecOps Engineer

Comprehensive monitoring for Redis operations including:
- OpenTelemetry tracing and metrics
- Prometheus metrics export
- Health checks and alerting
- Performance monitoring
- Security event tracking
"""

import asyncio

# import json  # unused import
# import logging  # unused import
import time
from collections import defaultdict, deque
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional

from opentelemetry import metrics, trace
from opentelemetry.trace import Status, StatusCode

# from opentelemetry.metrics import CallbackOptions, Observation  # unused imports
from prometheus_client import Counter, Gauge, Histogram, Info

# from app.core.config import settings  # unused import
from app.core.logging import get_logger

logger = get_logger(__name__)

# OpenTelemetry setup
tracer = trace.get_tracer(__name__)
meter = metrics.get_meter(__name__)

# Prometheus metrics
redis_operations_total = Counter(
    "redis_operations_total",
    "Total Redis operations",
    ["operation", "status", "cache_type", "tenant_id"],
)

redis_operation_duration = Histogram(
    "redis_operation_duration_seconds",
    "Redis operation duration",
    ["operation", "cache_type"],
    buckets=(0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0),
)

redis_connections_active = Gauge("redis_connections_active", "Active Redis connections")

redis_memory_usage = Gauge("redis_memory_usage_bytes", "Redis memory usage in bytes")

redis_cache_hit_rate = Gauge(
    "redis_cache_hit_rate", "Cache hit rate percentage", ["cache_type", "endpoint"]
)

redis_key_count = Gauge(
    "redis_key_count", "Total number of keys by pattern", ["pattern"]
)

redis_security_events = Counter(
    "redis_security_events_total",
    "Security events detected",
    ["event_type", "severity"],
)

redis_errors_total = Counter(
    "redis_errors_total", "Total Redis errors", ["error_type", "operation"]
)

# Info metrics
redis_info = Info("redis_info", "Redis instance information")


@dataclass
class RedisOperation:
    """Redis operation metadata for monitoring"""

    operation: str
    cache_type: str
    tenant_id: Optional[str]
    key: str
    start_time: float
    end_time: Optional[float] = None
    status: str = "pending"
    error: Optional[str] = None
    data_size: Optional[int] = None

    @property
    def duration(self) -> float:
        if self.end_time:
            return self.end_time - self.start_time
        return time.time() - self.start_time


@dataclass
class SecurityEvent:
    """Security event for Redis operations"""

    event_type: str
    severity: str
    operation: str
    key: str
    tenant_id: Optional[str]
    details: Dict[str, Any]
    timestamp: datetime


class RedisHealthMonitor:
    """Monitor Redis health and performance"""

    def __init__(self, redis_client=None):
        self.redis_client = redis_client
        self.health_history = deque(maxlen=100)
        self.performance_metrics = defaultdict(list)
        self.security_events = deque(maxlen=1000)

        # Health check intervals
        self.health_check_interval = 30  # seconds
        self.metrics_collection_interval = 60  # seconds

        # Performance thresholds
        self.latency_threshold_ms = 100
        self.memory_threshold_percent = 80
        self.connection_threshold = 80  # % of max connections

        # OpenTelemetry meters
        self._setup_otel_metrics()

    def _setup_otel_metrics(self):
        """Setup OpenTelemetry metrics"""
        # Operation counters
        self.operation_counter = meter.create_counter(
            name="redis_operations", description="Number of Redis operations", unit="1"
        )

        # Latency histogram
        self.latency_histogram = meter.create_histogram(
            name="redis_operation_latency",
            description="Redis operation latency",
            unit="ms",
        )

        # Memory usage gauge
        self.memory_gauge = meter.create_up_down_counter(
            name="redis_memory_usage", description="Redis memory usage", unit="bytes"
        )

        # Active connections gauge
        self.connections_gauge = meter.create_up_down_counter(
            name="redis_active_connections",
            description="Active Redis connections",
            unit="1",
        )

        # Security events counter
        self.security_counter = meter.create_counter(
            name="redis_security_events", description="Redis security events", unit="1"
        )

    async def monitor_operation(self, operation: RedisOperation) -> Dict[str, Any]:
        """Monitor a Redis operation with full observability"""
        span_name = f"redis.{operation.operation}"

        with tracer.start_as_current_span(span_name) as span:
            try:
                # Set span attributes
                span.set_attributes(
                    {
                        "redis.operation": operation.operation,
                        "redis.cache_type": operation.cache_type,
                        "redis.key": operation.key,
                        "redis.tenant_id": operation.tenant_id or "unknown",
                    }
                )

                # Validate operation security
                security_check = await self._validate_operation_security(operation)
                if not security_check["valid"]:
                    await self._record_security_event(
                        SecurityEvent(
                            event_type="invalid_operation",
                            severity="high",
                            operation=operation.operation,
                            key=operation.key,
                            tenant_id=operation.tenant_id,
                            details=security_check,
                            timestamp=datetime.utcnow(),
                        )
                    )

                    span.set_status(
                        Status(StatusCode.ERROR, "Security validation failed")
                    )
                    operation.status = "security_error"
                    operation.error = security_check["reason"]
                    return {"success": False, "error": security_check["reason"]}

                # Record operation start
                operation.end_time = time.time()
                duration_ms = operation.duration * 1000

                # Update metrics
                self._update_prometheus_metrics(operation, duration_ms)
                self._update_otel_metrics(operation, duration_ms)

                # Check performance thresholds
                if duration_ms > self.latency_threshold_ms:
                    logger.warning(
                        f"Slow Redis operation: {operation.operation} took {duration_ms:.2f}ms"
                    )
                    span.add_event("slow_operation", {"duration_ms": duration_ms})

                operation.status = "success"
                span.set_status(Status(StatusCode.OK))

                return {"success": True, "duration_ms": duration_ms}

            except Exception as e:
                operation.status = "error"
                operation.error = str(e)

                span.set_status(Status(StatusCode.ERROR, str(e)))
                span.record_exception(e)

                # Record error metrics
                redis_errors_total.labels(
                    error_type=type(e).__name__, operation=operation.operation
                ).inc()

                logger.error(f"Redis operation failed: {operation.operation} - {e}")
                return {"success": False, "error": str(e)}

    def _update_prometheus_metrics(self, operation: RedisOperation, duration_ms: float):
        """Update Prometheus metrics"""
        redis_operations_total.labels(
            operation=operation.operation,
            status=operation.status,
            cache_type=operation.cache_type,
            tenant_id=operation.tenant_id or "unknown",
        ).inc()

        redis_operation_duration.labels(
            operation=operation.operation, cache_type=operation.cache_type
        ).observe(
            duration_ms / 1000
        )  # Convert to seconds

    def _update_otel_metrics(self, operation: RedisOperation, duration_ms: float):
        """Update OpenTelemetry metrics"""
        attributes = {
            "operation": operation.operation,
            "cache_type": operation.cache_type,
            "tenant_id": operation.tenant_id or "unknown",
            "status": operation.status,
        }

        self.operation_counter.add(1, attributes)
        self.latency_histogram.record(duration_ms, attributes)

    async def _validate_operation_security(
        self, operation: RedisOperation
    ) -> Dict[str, Any]:
        """Validate Redis operation for security compliance"""
        validation = {"valid": True, "reason": None, "checks": []}

        # Check 1: Key format validation
        if not self._validate_key_format(operation.key):
            validation["valid"] = False
            validation["reason"] = f"Invalid key format: {operation.key}"
            validation["checks"].append("key_format_failed")

        # Check 2: Tenant isolation
        if operation.tenant_id and not self._validate_tenant_isolation(
            operation.key, operation.tenant_id
        ):
            validation["valid"] = False
            validation["reason"] = (
                f"Tenant isolation violation for key: {operation.key}"
            )
            validation["checks"].append("tenant_isolation_failed")

        # Check 3: Operation permissions
        if not self._validate_operation_permissions(
            operation.operation, operation.cache_type
        ):
            validation["valid"] = False
            validation["reason"] = (
                f"Operation {operation.operation} not allowed for cache type {operation.cache_type}"
            )
            validation["checks"].append("operation_permission_failed")

        # Check 4: Rate limiting (for write operations)
        if operation.operation in ["SET", "SETEX", "HSET", "SADD", "LPUSH", "RPUSH"]:
            if not await self._check_rate_limit(operation.tenant_id or "default"):
                validation["valid"] = False
                validation["reason"] = "Rate limit exceeded"
                validation["checks"].append("rate_limit_exceeded")

        return validation

    def _validate_key_format(self, key: str) -> bool:
        """Validate Redis key format"""
        # Must have version prefix
        if not key.startswith("v"):
            return False

        # No dangerous characters
        dangerous_chars = ["*", "?", "[", "]", "{", "}", "\n", "\r"]
        if any(char in key for char in dangerous_chars):
            return False

        # Reasonable length
        if len(key) > 250:  # Redis key limit is 512MB, but keep reasonable
            return False

        return True

    def _validate_tenant_isolation(self, key: str, tenant_id: str) -> bool:
        """Validate tenant isolation in cache keys"""
        # User-related keys must include tenant context
        user_indicators = ["user:", "client:", "tenant:", "account:"]
        if any(indicator in key.lower() for indicator in user_indicators):
            return f"tenant:{tenant_id}" in key or f"client:{tenant_id}" in key

        return True

    def _validate_operation_permissions(self, operation: str, cache_type: str) -> bool:
        """Validate operation permissions based on cache type"""
        # Define allowed operations per cache type
        permissions = {
            "flow": ["GET", "SET", "SETEX", "DEL", "EXISTS", "EXPIRE"],
            "agent": ["GET", "SET", "SETEX", "DEL", "HGET", "HSET", "HMGET", "HMSET"],
            "sse": ["GET", "SET", "DEL", "PUBLISH", "SUBSCRIBE"],
            "cache": ["GET", "SET", "SETEX", "DEL", "EXISTS", "EXPIRE", "TTL"],
            "session": ["GET", "SET", "SETEX", "DEL", "EXISTS"],
        }

        allowed_ops = permissions.get(cache_type, [])
        return operation.upper() in allowed_ops

    async def _check_rate_limit(self, tenant_id: str) -> bool:
        """Check rate limiting for tenant"""
        # Simple rate limiting - 1000 operations per minute per tenant
        # Rate limiting implementation
        # In production, this would use Redis to track rate limits
        # rate_limit_key = f"rate_limit:{tenant_id}"
        # current_time = int(time.time() / 60)  # Current minute
        # ... rate limiting logic ...
        return True  # For now, allow all requests

    async def _record_security_event(self, event: SecurityEvent):
        """Record a security event"""
        self.security_events.append(event)

        # Update metrics
        redis_security_events.labels(
            event_type=event.event_type, severity=event.severity
        ).inc()

        self.security_counter.add(
            1,
            {
                "event_type": event.event_type,
                "severity": event.severity,
                "operation": event.operation,
            },
        )

        # Log security event
        logger.warning(
            f"Redis security event: {event.event_type} "
            f"(severity: {event.severity}) - {event.details}"
        )

    async def get_redis_info(self) -> Dict[str, Any]:
        """Get comprehensive Redis information"""
        if not self.redis_client:
            return {"error": "Redis client not available"}

        try:
            # Get Redis INFO
            info = await self.redis_client.info()

            # Parse relevant metrics
            metrics = {
                "server": {
                    "redis_version": info.get("redis_version"),
                    "uptime_in_seconds": info.get("uptime_in_seconds"),
                    "process_id": info.get("process_id"),
                },
                "memory": {
                    "used_memory": info.get("used_memory", 0),
                    "used_memory_human": info.get("used_memory_human"),
                    "used_memory_peak": info.get("used_memory_peak", 0),
                    "used_memory_peak_human": info.get("used_memory_peak_human"),
                    "mem_fragmentation_ratio": info.get("mem_fragmentation_ratio", 0),
                    "maxmemory": info.get("maxmemory", 0),
                    "maxmemory_human": info.get("maxmemory_human"),
                },
                "clients": {
                    "connected_clients": info.get("connected_clients", 0),
                    "blocked_clients": info.get("blocked_clients", 0),
                    "max_clients": info.get("maxclients", 0),
                },
                "stats": {
                    "total_commands_processed": info.get("total_commands_processed", 0),
                    "instantaneous_ops_per_sec": info.get(
                        "instantaneous_ops_per_sec", 0
                    ),
                    "keyspace_hits": info.get("keyspace_hits", 0),
                    "keyspace_misses": info.get("keyspace_misses", 0),
                    "expired_keys": info.get("expired_keys", 0),
                    "evicted_keys": info.get("evicted_keys", 0),
                },
                "persistence": {
                    "rdb_last_save_time": info.get("rdb_last_save_time", 0),
                    "rdb_changes_since_last_save": info.get(
                        "rdb_changes_since_last_save", 0
                    ),
                    "aof_enabled": info.get("aof_enabled", False),
                    "aof_rewrite_in_progress": info.get(
                        "aof_rewrite_in_progress", False
                    ),
                },
            }

            # Calculate hit rate
            hits = metrics["stats"]["keyspace_hits"]
            misses = metrics["stats"]["keyspace_misses"]
            total_requests = hits + misses
            hit_rate = (hits / total_requests * 100) if total_requests > 0 else 0
            metrics["stats"]["hit_rate_percent"] = round(hit_rate, 2)

            # Update Prometheus metrics
            redis_memory_usage.set(metrics["memory"]["used_memory"])
            redis_connections_active.set(metrics["clients"]["connected_clients"])

            # Update Redis info
            redis_info.info(
                {
                    "version": metrics["server"]["redis_version"],
                    "uptime_seconds": str(metrics["server"]["uptime_in_seconds"]),
                    "max_memory": metrics["memory"]["maxmemory_human"] or "unlimited",
                }
            )

            return metrics

        except Exception as e:
            logger.error(f"Failed to get Redis info: {e}")
            return {"error": str(e)}

    async def run_health_check(self) -> Dict[str, Any]:
        """Run comprehensive Redis health check"""
        health_status = {
            "status": "unknown",
            "timestamp": datetime.utcnow().isoformat(),
            "checks": {},
            "metrics": {},
            "alerts": [],
        }

        try:
            # Basic connectivity check
            start_time = time.time()
            if self.redis_client:
                await self.redis_client.ping()
                ping_time = (time.time() - start_time) * 1000
                health_status["checks"]["connectivity"] = {
                    "status": "healthy",
                    "latency_ms": round(ping_time, 2),
                }
            else:
                health_status["checks"]["connectivity"] = {
                    "status": "unhealthy",
                    "error": "Redis client not available",
                }
                health_status["status"] = "unhealthy"
                return health_status

            # Get Redis metrics
            redis_info = await self.get_redis_info()
            if "error" not in redis_info:
                health_status["metrics"] = redis_info

                # Memory check
                memory_used = redis_info["memory"]["used_memory"]
                memory_max = redis_info["memory"]["maxmemory"]
                if memory_max > 0:
                    memory_percent = (memory_used / memory_max) * 100
                    if memory_percent > self.memory_threshold_percent:
                        health_status["alerts"].append(
                            {
                                "level": "warning",
                                "type": "memory",
                                "message": f"High memory usage: {memory_percent:.1f}%",
                            }
                        )

                # Connection check
                active_connections = redis_info["clients"]["connected_clients"]
                max_connections = redis_info["clients"]["max_clients"]
                if max_connections > 0:
                    connection_percent = (active_connections / max_connections) * 100
                    if connection_percent > self.connection_threshold:
                        health_status["alerts"].append(
                            {
                                "level": "warning",
                                "type": "connections",
                                "message": f"High connection usage: {connection_percent:.1f}%",
                            }
                        )

                # Performance check
                if ping_time > self.latency_threshold_ms:
                    health_status["alerts"].append(
                        {
                            "level": "warning",
                            "type": "latency",
                            "message": f"High latency: {ping_time:.2f}ms",
                        }
                    )

                health_status["checks"]["memory"] = {
                    "status": (
                        "healthy"
                        if not any(
                            alert["type"] == "memory"
                            for alert in health_status["alerts"]
                        )
                        else "warning"
                    ),
                    "used_memory_human": redis_info["memory"]["used_memory_human"],
                    "memory_usage_percent": (
                        round(memory_percent, 2) if memory_max > 0 else None
                    ),
                }

                health_status["checks"]["performance"] = {
                    "status": (
                        "healthy"
                        if ping_time <= self.latency_threshold_ms
                        else "warning"
                    ),
                    "hit_rate_percent": redis_info["stats"]["hit_rate_percent"],
                    "ops_per_sec": redis_info["stats"]["instantaneous_ops_per_sec"],
                }

            # Overall status
            if not health_status["alerts"]:
                health_status["status"] = "healthy"
            elif any(alert["level"] == "critical" for alert in health_status["alerts"]):
                health_status["status"] = "critical"
            else:
                health_status["status"] = "warning"

        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["checks"]["connectivity"] = {
                "status": "unhealthy",
                "error": str(e),
            }
            logger.error(f"Redis health check failed: {e}")

        # Store health check result
        self.health_history.append(health_status)

        return health_status

    async def get_security_events(self, limit: int = 100) -> List[Dict[str, Any]]:
        """Get recent security events"""
        events = list(self.security_events)[-limit:]
        return [
            {**asdict(event), "timestamp": event.timestamp.isoformat()}
            for event in events
        ]

    async def get_performance_summary(self) -> Dict[str, Any]:
        """Get performance summary statistics"""
        if not self.health_history:
            return {"error": "No health data available"}

        recent_checks = list(self.health_history)[-10:]  # Last 10 checks

        # Calculate averages
        latencies = [
            check["checks"]["connectivity"]["latency_ms"]
            for check in recent_checks
            if check["checks"].get("connectivity", {}).get("latency_ms")
        ]

        hit_rates = [
            check["checks"]["performance"]["hit_rate_percent"]
            for check in recent_checks
            if check["checks"].get("performance", {}).get("hit_rate_percent")
            is not None
        ]

        return {
            "avg_latency_ms": (
                round(sum(latencies) / len(latencies), 2) if latencies else None
            ),
            "avg_hit_rate_percent": (
                round(sum(hit_rates) / len(hit_rates), 2) if hit_rates else None
            ),
            "total_checks": len(self.health_history),
            "healthy_checks": len(
                [check for check in recent_checks if check["status"] == "healthy"]
            ),
            "warning_checks": len(
                [check for check in recent_checks if check["status"] == "warning"]
            ),
            "critical_checks": len(
                [check for check in recent_checks if check["status"] == "critical"]
            ),
            "uptime_percent": (
                round(
                    len(
                        [
                            check
                            for check in recent_checks
                            if check["status"] != "unhealthy"
                        ]
                    )
                    / len(recent_checks)
                    * 100,
                    2,
                )
                if recent_checks
                else 0
            ),
        }


# Global monitor instance
redis_monitor = RedisHealthMonitor()


async def setup_monitoring(redis_client):
    """Setup Redis monitoring with client"""
    # Set the redis client on the global monitor instance
    redis_monitor.redis_client = redis_client

    # Start background monitoring tasks
    asyncio.create_task(periodic_health_check())
    asyncio.create_task(periodic_metrics_collection())

    logger.info("Redis monitoring setup complete")


async def periodic_health_check():
    """Periodic health check task"""
    while True:
        try:
            await redis_monitor.run_health_check()
            await asyncio.sleep(redis_monitor.health_check_interval)
        except Exception as e:
            logger.error(f"Health check task error: {e}")
            await asyncio.sleep(redis_monitor.health_check_interval)


async def periodic_metrics_collection():
    """Periodic metrics collection task"""
    while True:
        try:
            await redis_monitor.get_redis_info()
            await asyncio.sleep(redis_monitor.metrics_collection_interval)
        except Exception as e:
            logger.error(f"Metrics collection task error: {e}")
            await asyncio.sleep(redis_monitor.metrics_collection_interval)
