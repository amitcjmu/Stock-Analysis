#!/usr/bin/env python3
"""
Backfill script for AgentDiscoveredPatterns embeddings
Generated by CC for DATABASE_ENHANCEMENT_PLAN.md - Phase 1 Gap 1

This script backfills existing AgentDiscoveredPatterns records with dummy vector
embeddings to support the new vector search capability. This is a critical
implementation task, not just risk mitigation.

Usage:
    python scripts/backfill_agent_patterns_embeddings.py [--dry-run] [--batch-size=100]
"""

import argparse
import asyncio
import logging
import random
import sys
from typing import List

# Add the parent directory to sys.path so we can import app modules
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from app.core.database import get_db_session  # noqa: E402
from sqlalchemy import text  # noqa: E402
from sqlalchemy.exc import SQLAlchemyError  # noqa: E402

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# OpenAI embedding dimension
EMBEDDING_DIMENSION = 1536


def generate_dummy_embedding() -> List[float]:
    """
    Generate a dummy embedding vector of 1536 dimensions.

    This creates a normalized random vector suitable for similarity testing.
    In production, these would be replaced with actual OpenAI embeddings.

    Returns:
        List of 1536 float values representing a normalized vector
    """
    # Generate random values between -1 and 1
    vector = [random.uniform(-1.0, 1.0) for _ in range(EMBEDDING_DIMENSION)]

    # Normalize the vector to unit length for better similarity behavior
    magnitude = sum(x * x for x in vector) ** 0.5
    if magnitude > 0:
        vector = [x / magnitude for x in vector]

    return vector


def classify_pattern_insight_type(
    pattern_type: str, pattern_name: str, pattern_description: str = None
) -> str:
    """
    Classify existing patterns into structured insight types based on their content.

    Args:
        pattern_type: The existing pattern_type field
        pattern_name: The pattern name
        pattern_description: Optional pattern description

    Returns:
        One of the valid insight_type enum values
    """
    # Convert to lowercase for matching
    content = f"{pattern_type} {pattern_name} {pattern_description or ''}".lower()

    # Classification logic based on content
    if any(
        word in content for word in ["field", "mapping", "map", "column", "attribute"]
    ):
        return "field_mapping_suggestion"
    elif any(
        word in content
        for word in ["risk", "danger", "threat", "vulnerability", "security"]
    ):
        return "risk_pattern"
    elif any(
        word in content
        for word in ["optimize", "optimization", "improve", "enhance", "efficiency"]
    ):
        return "optimization_opportunity"
    elif any(
        word in content
        for word in ["anomaly", "outlier", "unusual", "abnormal", "detect"]
    ):
        return "anomaly_detection"
    elif any(
        word in content for word in ["workflow", "process", "procedure", "step", "flow"]
    ):
        return "workflow_improvement"
    elif any(
        word in content
        for word in ["dependency", "depend", "relation", "link", "connect"]
    ):
        return "dependency_pattern"
    elif any(
        word in content
        for word in ["performance", "speed", "latency", "throughput", "timing"]
    ):
        return "performance_pattern"
    elif any(
        word in content for word in ["error", "fail", "exception", "bug", "issue"]
    ):
        return "error_pattern"
    else:
        # Default classification based on pattern_type
        return "optimization_opportunity"


async def backfill_embeddings(dry_run: bool = False, batch_size: int = 100) -> None:
    """
    Backfill embeddings for existing AgentDiscoveredPatterns records.

    Args:
        dry_run: If True, only show what would be updated without making changes
        batch_size: Number of records to process in each batch
    """
    logger.info(
        f"Starting backfill process (dry_run={dry_run}, batch_size={batch_size})"
    )

    async with get_db_session() as session:
        try:
            # Check if the embedding column exists
            result = await session.execute(
                text(
                    """
                SELECT column_name
                FROM information_schema.columns
                WHERE table_schema = 'migration'
                AND table_name = 'agent_discovered_patterns'
                AND column_name = 'embedding'
            """
                )
            )

            if not result.fetchone():
                logger.error(
                    "âŒ Embedding column not found. Please run migration 017 first."
                )
                return

            # Get count of patterns without embeddings
            count_result = await session.execute(
                text(
                    """
                SELECT COUNT(*)
                FROM migration.agent_discovered_patterns
                WHERE embedding IS NULL
            """
                )
            )
            total_count = count_result.scalar()

            logger.info(f"Found {total_count} patterns without embeddings")

            if total_count == 0:
                logger.info(
                    "âœ… All patterns already have embeddings. Nothing to backfill."
                )
                return

            if dry_run:
                logger.info(f"ðŸ” DRY RUN: Would backfill {total_count} patterns")

                # Show sample of patterns that would be updated
                sample_result = await session.execute(
                    text(
                        """
                    SELECT id, pattern_name, pattern_type, pattern_description
                    FROM migration.agent_discovered_patterns
                    WHERE embedding IS NULL
                    ORDER BY created_at DESC
                    LIMIT 5
                """
                    )
                )

                logger.info("Sample patterns that would be updated:")
                for row in sample_result.fetchall():
                    insight_type = classify_pattern_insight_type(
                        row.pattern_type, row.pattern_name, row.pattern_description
                    )
                    logger.info(
                        f"  - {row.pattern_name} ({row.pattern_type}) -> {insight_type}"
                    )

                return

            # Process in batches
            processed = 0
            while processed < total_count:
                # Get batch of patterns without embeddings
                batch_result = await session.execute(
                    text(
                        """
                    SELECT id, pattern_name, pattern_type, pattern_description
                    FROM migration.agent_discovered_patterns
                    WHERE embedding IS NULL
                    ORDER BY created_at
                    LIMIT :batch_size
                """
                    ),
                    {"batch_size": batch_size},
                )

                batch_rows = batch_result.fetchall()
                if not batch_rows:
                    break

                logger.info(
                    f"Processing batch {processed + 1}-{processed + len(batch_rows)} of {total_count}"
                )

                # Update each pattern in the batch
                for row in batch_rows:
                    try:
                        # Generate dummy embedding
                        embedding = generate_dummy_embedding()

                        # Classify insight type
                        insight_type = classify_pattern_insight_type(
                            row.pattern_type, row.pattern_name, row.pattern_description
                        )

                        # Update the record
                        await session.execute(
                            text(
                                """
                            UPDATE migration.agent_discovered_patterns
                            SET
                                embedding = :embedding,
                                insight_type = :insight_type,
                                updated_at = NOW()
                            WHERE id = :pattern_id
                        """
                            ),
                            {
                                "embedding": embedding,
                                "insight_type": insight_type,
                                "pattern_id": row.id,
                            },
                        )

                        processed += 1

                    except Exception as e:
                        logger.error(f"Failed to update pattern {row.id}: {e}")
                        continue

                # Commit batch
                await session.commit()
                logger.info(f"âœ… Updated {len(batch_rows)} patterns in this batch")

            logger.info(
                f"ðŸŽ‰ Backfill completed! Updated {processed} patterns with embeddings and insight types"
            )

            # Verify the update
            verify_result = await session.execute(
                text(
                    """
                SELECT COUNT(*)
                FROM migration.agent_discovered_patterns
                WHERE embedding IS NOT NULL AND insight_type IS NOT NULL
            """
                )
            )
            final_count = verify_result.scalar()

            logger.info(
                f"âœ… Verification: {final_count} patterns now have embeddings and insight types"
            )

        except SQLAlchemyError as e:
            logger.error(f"âŒ Database error during backfill: {e}")
            await session.rollback()
            raise
        except Exception as e:
            logger.error(f"âŒ Unexpected error during backfill: {e}")
            await session.rollback()
            raise


async def main():
    """Main entry point for the backfill script"""
    parser = argparse.ArgumentParser(
        description="Backfill AgentDiscoveredPatterns with dummy embeddings for vector search"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show what would be updated without making changes",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=100,
        help="Number of records to process in each batch (default: 100)",
    )

    args = parser.parse_args()

    try:
        await backfill_embeddings(dry_run=args.dry_run, batch_size=args.batch_size)
    except KeyboardInterrupt:
        logger.info("âŒ Backfill interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ Backfill failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
