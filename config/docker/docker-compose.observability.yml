# Grafana Observability Stack (Tier B)
# ADR-031: Enterprise Observability Architecture
# 5 Containers: Grafana + Loki + Tempo + Prometheus + Alloy
#
# Usage:
#   Local Dev:  docker-compose -f docker-compose.yml -f docker-compose.observability.yml up -d
#   Azure Dev:  docker-compose -f docker-compose.yml -f docker-compose.observability.yml up -d
#   Railway:    Same command (HTTP-only verified, no WebSockets)
#
# Access:
#   Grafana:    http://localhost:9999 (local) or https://aiforceasses.cloudsmarthcl.com:9999 (Azure)
#   Prometheus: http://localhost:9090 (metrics UI)
#
# Prerequisites:
#   1. Copy .env.observability.template to .env.observability
#   2. Generate strong password: openssl rand -base64 32
#   3. Fill in GRAFANA_ADMIN_PASSWORD

name: migration

services:
  # ============================================================================
  # GRAFANA - Web UI for dashboards and datasource configuration
  # ============================================================================
  grafana:
    image: grafana/grafana:11.3.0
    container_name: migration_grafana
    restart: unless-stopped
    ports:
      - "9999:3000"  # External port 9999 (per ADR-031)
    environment:
      # Admin credentials
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}

      # Server configuration
      - GF_SERVER_PROTOCOL=${GF_SERVER_PROTOCOL:-http}
      - GF_SERVER_DOMAIN=${GF_SERVER_DOMAIN:-localhost}
      - GF_SERVER_ROOT_URL=${GF_SERVER_ROOT_URL:-http://localhost:9999}
      - GF_SERVER_HTTP_PORT=3000

      # Security
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_USERS_ALLOW_ORG_CREATE=false
      - GF_AUTH_ANONYMOUS_ENABLED=false

      # GitHub OAuth (Railway prod only - disabled for Azure due to firewall)
      - GF_AUTH_GITHUB_ENABLED=${GF_AUTH_GITHUB_ENABLED:-false}
      - GF_AUTH_GITHUB_CLIENT_ID=${GF_AUTH_GITHUB_CLIENT_ID:-}
      - GF_AUTH_GITHUB_CLIENT_SECRET=${GF_AUTH_GITHUB_CLIENT_SECRET:-}
      - GF_AUTH_GITHUB_ALLOWED_ORGANIZATIONS=${GF_AUTH_GITHUB_ALLOWED_ORGANIZATIONS:-}

      # Azure AD OAuth (Phase 2 enhancement if available within firewall)
      - GF_AUTH_AZUREAD_ENABLED=${AZURE_AD_ENABLED:-false}
      - GF_AUTH_AZUREAD_CLIENT_ID=${AZURE_AD_CLIENT_ID:-}
      - GF_AUTH_AZUREAD_CLIENT_SECRET=${AZURE_AD_CLIENT_SECRET:-}
      - GF_AUTH_AZUREAD_AUTH_URL=${AZURE_AD_AUTH_URL:-}
      - GF_AUTH_AZUREAD_TOKEN_URL=${AZURE_AD_TOKEN_URL:-}

      # Disable telemetry
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false

      # Logging
      - GF_LOG_LEVEL=info
      - GF_LOG_MODE=console
    volumes:
      - grafana_data:/var/lib/grafana
      - ./observability/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./observability/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./observability/grafana/dashboards:/var/lib/grafana/dashboards:ro
      # SSL certificates (only if GF_SERVER_PROTOCOL=https)
      - ./observability/ssl:/etc/grafana/ssl:ro
    networks:
      - migration_network
    depends_on:
      - loki
      - tempo
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 40s

  # ============================================================================
  # LOKI - Log aggregation and storage
  # ============================================================================
  loki:
    image: grafana/loki:3.2.0
    container_name: migration_loki
    restart: unless-stopped
    # Run as root to fix volume permission issues on Azure VM
    user: "0"
    ports:
      - "3100:3100"  # Loki API
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./observability/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    networks:
      - migration_network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 20s

  # ============================================================================
  # TEMPO - Distributed tracing (replaces Jaeger per ADR-031)
  # ============================================================================
  tempo:
    image: grafana/tempo:2.6.0
    container_name: migration_tempo
    restart: unless-stopped
    # Run as root to fix volume permission issues on Azure VM
    user: "0"
    ports:
      - "3200:3200"   # Tempo API
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
    command: -config.file=/etc/tempo/tempo.yaml
    volumes:
      - ./observability/tempo-config.yaml:/etc/tempo/tempo.yaml:ro
      - tempo_data:/var/tempo
    networks:
      - migration_network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3200/ready || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 20s

  # ============================================================================
  # PROMETHEUS - Metrics time-series database
  # ============================================================================
  prometheus:
    image: prom/prometheus:v2.54.1
    container_name: migration_prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"  # Prometheus UI
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=${PROMETHEUS_RETENTION_DAYS:-14}d'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    volumes:
      - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    networks:
      - migration_network
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 20s

  # ============================================================================
  # ALLOY - Unified collector (replaces Promtail + OTel Collector)
  # ============================================================================
  alloy:
    image: grafana/alloy:v1.4.2
    container_name: migration_alloy
    restart: unless-stopped
    # Run as root to access Docker socket and fix volume permissions
    user: "0"
    ports:
      - "12345:12345"  # Alloy UI
      - "4317"         # OTLP gRPC (internal)
      - "4318"         # OTLP HTTP (internal)
    command:
      - run
      - --server.http.listen-addr=0.0.0.0:12345
      - --storage.path=/var/lib/alloy/data
      - /etc/alloy/config.alloy
    volumes:
      - ./observability/alloy-config.alloy:/etc/alloy/config.alloy:ro
      - alloy_data:/var/lib/alloy
      # Mount Docker socket for log collection
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - migration_network
    depends_on:
      - loki
      - tempo
      - prometheus
    environment:
      - LOKI_ENDPOINT=http://loki:3100
      - TEMPO_ENDPOINT=tempo:4317
      - PROMETHEUS_ENDPOINT=http://prometheus:9090
      - OBSERVABILITY_ENVIRONMENT=${OBSERVABILITY_ENVIRONMENT:-local}
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:12345/-/healthy || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 20s

# ============================================================================
# VOLUMES
# ============================================================================
volumes:
  grafana_data:
    driver: local
  loki_data:
    driver: local
  tempo_data:
    driver: local
  prometheus_data:
    driver: local
  alloy_data:
    driver: local

# ============================================================================
# NETWORKS
# ============================================================================
# Network is created by main docker-compose.yml (project name: migration)
# Docker Compose names it: migration_migration_network
# Reference it as external to connect observability stack
networks:
  migration_network:
    external: true
    name: migration_migration_network
